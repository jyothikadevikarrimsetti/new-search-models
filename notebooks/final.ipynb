{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "320482ae",
   "metadata": {},
   "source": [
    "# Keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56ea02f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ReadTimeout",
     "evalue": "(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: d47c8f26-aafb-49d8-a6bf-91d459e9005d)')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\91807\\Downloads\\search_models\\search_env\\lib\\site-packages\\urllib3\\connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\91807\\Downloads\\search_models\\search_env\\lib\\site-packages\\urllib3\\connection.py:516\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 516\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Python310\\lib\\http\\client.py:1374\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1374\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Python310\\lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n",
      "File \u001b[1;32mC:\\Python310\\lib\\http\\client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n",
      "File \u001b[1;32mC:\\Python310\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "File \u001b[1;32mC:\\Python310\\lib\\ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1271\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1272\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1273\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Python310\\lib\\ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mTimeoutError\u001b[0m: The read operation timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mReadTimeoutError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\91807\\Downloads\\search_models\\search_env\\lib\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\91807\\Downloads\\search_models\\search_env\\lib\\site-packages\\urllib3\\connectionpool.py:841\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    839\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 841\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    844\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32mc:\\Users\\91807\\Downloads\\search_models\\search_env\\lib\\site-packages\\urllib3\\util\\retry.py:474\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_method_retryable(method):\n\u001b[1;32m--> 474\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\91807\\Downloads\\search_models\\search_env\\lib\\site-packages\\urllib3\\util\\util.py:39\u001b[0m, in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\91807\\Downloads\\search_models\\search_env\\lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    788\u001b[0m     conn,\n\u001b[0;32m    789\u001b[0m     method,\n\u001b[0;32m    790\u001b[0m     url,\n\u001b[0;32m    791\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    792\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    793\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    794\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    795\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    796\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    797\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    798\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    800\u001b[0m )\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\91807\\Downloads\\search_models\\search_env\\lib\\site-packages\\urllib3\\connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 536\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_timeout\u001b[49m\u001b[43m(\u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    537\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\91807\\Downloads\\search_models\\search_env\\lib\\site-packages\\urllib3\\connectionpool.py:367\u001b[0m, in \u001b[0;36mHTTPConnectionPool._raise_timeout\u001b[1;34m(self, err, url, timeout_value)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(err, SocketTimeout):\n\u001b[1;32m--> 367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeoutError(\n\u001b[0;32m    368\u001b[0m         \u001b[38;5;28mself\u001b[39m, url, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead timed out. (read timeout=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    369\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;66;03m# See the above comment about EAGAIN in Python 3.\u001b[39;00m\n",
      "\u001b[1;31mReadTimeoutError\u001b[0m: HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mReadTimeout\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# 3. Tokenize\u001b[39;00m\n\u001b[0;32m     18\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistilbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 19\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpreprocess\u001b[39m(example):\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer(example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m], truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\91807\\Downloads\\search_models\\search_env\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:950\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    947\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    949\u001b[0m \u001b[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[1;32m--> 950\u001b[0m tokenizer_config \u001b[38;5;241m=\u001b[39m get_tokenizer_config(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tokenizer_config:\n\u001b[0;32m    952\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenizer_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\91807\\Downloads\\search_models\\search_env\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:782\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[1;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[0;32m    779\u001b[0m     token \u001b[38;5;241m=\u001b[39m use_auth_token\n\u001b[0;32m    781\u001b[0m commit_hash \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 782\u001b[0m resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    799\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\91807\\Downloads\\search_models\\search_env\\lib\\site-packages\\transformers\\utils\\hub.py:312\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcached_file\u001b[39m(\n\u001b[0;32m    255\u001b[0m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[0;32m    256\u001b[0m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    258\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    259\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;124;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 312\u001b[0m     file \u001b[38;5;241m=\u001b[39m cached_files(path_or_repo_id\u001b[38;5;241m=\u001b[39mpath_or_repo_id, filenames\u001b[38;5;241m=\u001b[39m[filename], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    313\u001b[0m     file \u001b[38;5;241m=\u001b[39m file[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "File \u001b[1;32mc:\\Users\\91807\\Downloads\\search_models\\search_env\\lib\\site-packages\\transformers\\utils\\hub.py:557\u001b[0m, in \u001b[0;36mcached_files\u001b[1;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;66;03m# Any other Exception type should now be re-raised, in order to provide helpful error messages and break the execution flow\u001b[39;00m\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;66;03m# (EntryNotFoundError will be treated outside this block and correctly re-raised if needed)\u001b[39;00m\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, EntryNotFoundError):\n\u001b[1;32m--> 557\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    559\u001b[0m resolved_files \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    560\u001b[0m     _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision) \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m full_filenames\n\u001b[0;32m    561\u001b[0m ]\n\u001b[0;32m    562\u001b[0m \u001b[38;5;66;03m# If there are any missing file and the flag is active, raise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\91807\\Downloads\\search_models\\search_env\\lib\\site-packages\\transformers\\utils\\hub.py:470\u001b[0m, in \u001b[0;36mcached_files\u001b[1;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[1;32m--> 470\u001b[0m         \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    471\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    472\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    484\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    485\u001b[0m         snapshot_download(\n\u001b[0;32m    486\u001b[0m             path_or_repo_id,\n\u001b[0;32m    487\u001b[0m             allow_patterns\u001b[38;5;241m=\u001b[39mfull_filenames,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    496\u001b[0m             local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    497\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\91807\\Downloads\\search_models\\search_env\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\91807\\Downloads\\search_models\\search_env\\lib\\site-packages\\huggingface_hub\\file_download.py:1008\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[0;32m    988\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[0;32m    989\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[0;32m    990\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1005\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   1006\u001b[0m     )\n\u001b[0;32m   1007\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1008\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[0;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[0;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[0;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[0;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1024\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\91807\\Downloads\\search_models\\search_env\\lib\\site-packages\\huggingface_hub\\file_download.py:1161\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[1;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[0;32m   1158\u001b[0m \u001b[38;5;66;03m# Local file doesn't exist or etag isn't a match => retrieve file from remote (or cache)\u001b[39;00m\n\u001b[0;32m   1160\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[1;32m-> 1161\u001b[0m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.incomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1164\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1170\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxet_file_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1172\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1173\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pointer_path):\n\u001b[0;32m   1174\u001b[0m         _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\91807\\Downloads\\search_models\\search_env\\lib\\site-packages\\huggingface_hub\\file_download.py:1725\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[1;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download, etag, xet_file_data)\u001b[0m\n\u001b[0;32m   1718\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m xet_file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1719\u001b[0m             logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m   1720\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXet Storage is enabled for this repo, but the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhf_xet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m package is not installed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1721\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFalling back to regular HTTP download. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1722\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1723\u001b[0m             )\n\u001b[1;32m-> 1725\u001b[0m         \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1726\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1727\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1728\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1729\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1730\u001b[0m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1731\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1732\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1734\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1735\u001b[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001b[1;32mc:\\Users\\91807\\Downloads\\search_models\\search_env\\lib\\site-packages\\huggingface_hub\\file_download.py:420\u001b[0m, in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    415\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    416\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe file is too large to be downloaded using the regular download method. Use `hf_transfer` or `hf_xet` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    417\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Try `pip install hf_transfer` or `pip install hf_xet`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    418\u001b[0m         )\n\u001b[1;32m--> 420\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstants\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHF_HUB_DOWNLOAD_TIMEOUT\u001b[49m\n\u001b[0;32m    422\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    424\u001b[0m hf_raise_for_status(r)\n\u001b[0;32m    425\u001b[0m content_length \u001b[38;5;241m=\u001b[39m _get_file_length_from_http_response(r)\n",
      "File \u001b[1;32mc:\\Users\\91807\\Downloads\\search_models\\search_env\\lib\\site-packages\\huggingface_hub\\file_download.py:309\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[0;32m    308\u001b[0m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[1;32m--> 309\u001b[0m response \u001b[38;5;241m=\u001b[39m http_backoff(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, retry_on_exceptions\u001b[38;5;241m=\u001b[39m(), retry_on_status_codes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m429\u001b[39m,))\n\u001b[0;32m    310\u001b[0m hf_raise_for_status(response)\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\91807\\Downloads\\search_models\\search_env\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:310\u001b[0m, in \u001b[0;36mhttp_backoff\u001b[1;34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[0m\n\u001b[0;32m    307\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mseek(io_obj_initial_pos)\n\u001b[0;32m    309\u001b[0m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m response \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m retry_on_status_codes:\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\91807\\Downloads\\search_models\\search_env\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\91807\\Downloads\\search_models\\search_env\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\91807\\Downloads\\search_models\\search_env\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:96\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[1;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSend: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_curlify(request)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     98\u001b[0m     request_id \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001b[1;32mc:\\Users\\91807\\Downloads\\search_models\\search_env\\lib\\site-packages\\requests\\adapters.py:713\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    712\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, ReadTimeoutError):\n\u001b[1;32m--> 713\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeout(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    714\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, _InvalidHeader):\n\u001b[0;32m    715\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidHeader(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[1;31mReadTimeout\u001b[0m: (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: d47c8f26-aafb-49d8-a6bf-91d459e9005d)')"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# 1. Prepare label mappings\n",
    "df = pd.DataFrame(expanded_data)\n",
    "df['label_id'] = df['label'].astype('category').cat.codes\n",
    "label2id = {label: i for i, label in enumerate(df['label'].astype('category').cat.categories)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "# 2. Convert to Hugging Face Dataset\n",
    "# Rename 'label_id' to 'labels' for Trainer compatibility\n",
    "df_for_hf = df.rename(columns={'label_id': 'labels'})\n",
    "# Remove the 'label' column (string) to avoid Trainer confusion\n",
    "df_for_hf = df_for_hf.drop(columns=['label'])\n",
    "dataset = Dataset.from_pandas(df_for_hf)\n",
    "\n",
    "# 3. Tokenize\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "def preprocess(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=64)\n",
    "dataset = dataset.map(preprocess, batched=True)\n",
    "\n",
    "# 4. Train/Test split\n",
    "split = dataset.train_test_split(test_size=0.2)\n",
    "train_ds, test_ds = split[\"train\"], split[\"test\"]\n",
    "\n",
    "# 5. Model and Trainer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, num_labels=len(label2id), id2label=id2label, label2id=label2id)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./intent_model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# 6. Train (this will take a few minutes on CPU, much faster on GPU)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693b1154",
   "metadata": {},
   "source": [
    "PI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d7fa15",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      4\u001b[0m correct_intents_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mNew folder (5)\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mnew-search-models\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcorrect_intents.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 5\u001b[0m chunk_dir \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(project_root, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunks\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m labeled_from_file \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(correct_intents_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "#  Build labeled training data from correct_intents.txt\n",
    "import re\n",
    "\n",
    "correct_intents_path = r'C:\\New folder (5)\\new-search-models\\correct_intents.txt'\n",
    "chunk_dir = os.path.join(project_root, '..', 'data', 'chunks')\n",
    "\n",
    "labeled_from_file = []\n",
    "with open(correct_intents_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        m = re.match(r'([^:]+):.*?\\'intent\\': \\'([^\\']+)\\'', line)\n",
    "        if m:\n",
    "            fname, intent = m.group(1).strip(), m.group(2).strip()\n",
    "            chunk_path = os.path.join(chunk_dir, fname)\n",
    "            if os.path.exists(chunk_path):\n",
    "                with open(chunk_path, 'r', encoding='utf-8') as cf:\n",
    "                    chunk_text = cf.read()\n",
    "                labeled_from_file.append({\"text\": chunk_text, \"label\": intent})\n",
    "            else:\n",
    "                print(f\" Chunk file not found: {chunk_path}\")\n",
    "        else:\n",
    "            print(f\" Could not parse line: {line.strip()}\")\n",
    "\n",
    "print(f\"Loaded {len(labeled_from_file)} labeled examples from correct_intents.txt.\")\n",
    "# Add these to your expanded_data list before retraining:\n",
    "# expanded_data.extend(labeled_from_file)\n",
    "\n",
    "import glob\n",
    "\n",
    "# 7. Inference: Predict intent for new text\n",
    "def predict_intent(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=64)\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    pred_id = logits.argmax(dim=1).item()\n",
    "    intent = id2label[pred_id]\n",
    "    confidence = logits.softmax(dim=1)[0, pred_id].item()\n",
    "    return intent, confidence\n",
    "# def predict_intent(text):\n",
    "#     inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=64)\n",
    "#     device = next(model.parameters()).device\n",
    "#     inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "#     outputs = model(**inputs)\n",
    "#     pred_id = outputs.logits.argmax(dim=1).item()\n",
    "#     return id2label[pred_id]\n",
    "chunk_dir = os.path.join(project_root, '..', 'data', 'chunks')\n",
    "chunk_files = glob.glob(os.path.join(chunk_dir, '*.txt'))\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# print(predict_intent(open(r'C:\\New folder (5)\\new-search-models\\data\\chunks\\MHC_CaseStatus_511605_chunk1.txt', 'r', encoding='utf-8').read()))\n",
    "for chunk_path in chunk_files:\n",
    "    with open(chunk_path, 'r', encoding='utf-8') as f:\n",
    "        chunk_text = f.read()\n",
    "    print(f\"{os.path.basename(chunk_path)}: {predict_intent(chunk_text)}\")\n",
    "# print(predict_intent(\"List the skills in this resume.\"))\n",
    "# print(predict_intent(\"Who is the presiding judge?\"))\n",
    "# print(predict_intent(\"I have a technical issue with the system.\"))\n",
    "\n",
    "#  Add labeled examples from correct_intents.txt to your training data\n",
    "if 'expanded_data' in globals() and 'labeled_from_file' in globals():\n",
    "    expanded_data.extend(labeled_from_file)\n",
    "    print(f\"expanded_data now has {len(expanded_data)} examples (including those from correct_intents.txt).\")\n",
    "else:\n",
    "    print(\" Make sure both expanded_data and labeled_from_file are defined before running this cell.\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Example labeled data for intent fine-tuning (expand with more real samples for best results)\n",
    "# data = [\n",
    "#     {\"text\": \"How do I file a claim?\", \"label\": \"claim_process\"},\n",
    "#     {\"text\": \"What is the process for submitting an insurance claim?\", \"label\": \"claim_process\"},\n",
    "#     {\"text\": \"What is the current status of the case?\", \"label\": \"case_status\"},\n",
    "#     {\"text\": \"Show me the progress of case number 511605.\", \"label\": \"case_status\"},\n",
    "#     {\"text\": \"Can I get a copy of the case order?\", \"label\": \"document_request\"},\n",
    "#     {\"text\": \"How do I request the judgment document?\", \"label\": \"document_request\"},\n",
    "#     {\"text\": \"What skills are listed in the resume?\", \"label\": \"resume_info\"},\n",
    "#     {\"text\": \"List the programming languages known by the applicant.\", \"label\": \"resume_info\"},\n",
    "#     {\"text\": \"Who is the presiding judge for this case?\", \"label\": \"court_details\"},\n",
    "#     {\"text\": \"Who are the parties involved in this case?\", \"label\": \"party_information\"},\n",
    "#     {\"text\": \"When was the last hearing held?\", \"label\": \"hearing_information\"},\n",
    "#     {\"text\": \"I have a technical issue with the system.\", \"label\": \"technical_support\"},\n",
    "#     {\"text\": \"Give me a summary of the file.\", \"label\": \"general_info\"},\n",
    "#     {\"text\": \"Tell me about this document.\", \"label\": \"general_info\"},\n",
    "# ]\n",
    "df = pd.DataFrame(expanded_data)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f79914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3ab1c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Expand Labeled Data for Better Intent Classification\n",
    "\n",
    "# To improve classifier performance, add more diverse and realistic examples for each intent. This helps the model generalize and reduces bias toward majority classes. Below is an expanded dataset template you can use and modify for your domain.\n",
    "# Template: Expanded labeled data for intent fine-tuning\n",
    "# Copy, edit, and expand this list with your real examples\n",
    "expanded_data = [\n",
    "    # claim_process\n",
    "    {\"text\": \"How do I file a claim?\", \"label\": \"claim_process\"},\n",
    "    {\"text\": \"What is the process for submitting an insurance claim?\", \"label\": \"claim_process\"},\n",
    "    {\"text\": \"I want to submit a new claim for my car accident.\", \"label\": \"claim_process\"},\n",
    "    {\"text\": \"Guide me through the claim submission steps.\", \"label\": \"claim_process\"},\n",
    "    {\"text\": \"Where do I upload my claim documents?\", \"label\": \"claim_process\"},\n",
    "    {\"text\": \"How long does it take to process a claim?\", \"label\": \"claim_process\"},\n",
    "    {\"text\": \"Can I check the status of my insurance claim?\", \"label\": \"claim_process\"},\n",
    "    {\"text\": \"What documents are needed to file a claim?\", \"label\": \"claim_process\"},\n",
    "    {\"text\": \"Is there a deadline for submitting claims?\", \"label\": \"claim_process\"},\n",
    "    {\"text\": \"Can I cancel a claim after submitting?\", \"label\": \"claim_process\"},\n",
    "    # case_status\n",
    "    {\"text\": \"What is the current status of the case?\", \"label\": \"case_status\"},\n",
    "    {\"text\": \"Show me the progress of case number 511605.\", \"label\": \"case_status\"},\n",
    "    {\"text\": \"Has a judgment been issued in my case?\", \"label\": \"case_status\"},\n",
    "    {\"text\": \"Is my case still pending?\", \"label\": \"case_status\"},\n",
    "    {\"text\": \"When is the next hearing for my case?\", \"label\": \"case_status\"},\n",
    "    {\"text\": \"What was the outcome of the last court session?\", \"label\": \"case_status\"},\n",
    "    {\"text\": \"Who is the presiding judge for this case?\", \"label\": \"case_status\"},\n",
    "    {\"text\": \"Has an appeal been filed?\", \"label\": \"case_status\"},\n",
    "    {\"text\": \"Is there an order available for my case?\", \"label\": \"case_status\"},\n",
    "    {\"text\": \"What is the next step in my case?\", \"label\": \"case_status\"},\n",
    "    # document_request\n",
    "    {\"text\": \"Can I get a copy of the case order?\", \"label\": \"document_request\"},\n",
    "    {\"text\": \"How do I request the judgment document?\", \"label\": \"document_request\"},\n",
    "    {\"text\": \"I need certified copies of my case documents.\", \"label\": \"document_request\"},\n",
    "    {\"text\": \"Where can I download the court forms?\", \"label\": \"document_request\"},\n",
    "    {\"text\": \"Request a copy of the final order.\", \"label\": \"document_request\"},\n",
    "    {\"text\": \"How do I obtain previous hearing transcripts?\", \"label\": \"document_request\"},\n",
    "    {\"text\": \"Can I get a digital copy of my case file?\", \"label\": \"document_request\"},\n",
    "    {\"text\": \"What is the fee for document requests?\", \"label\": \"document_request\"},\n",
    "    {\"text\": \"How long does it take to receive requested documents?\", \"label\": \"document_request\"},\n",
    "    {\"text\": \"Is there a limit to the number of documents I can request?\", \"label\": \"document_request\"},\n",
    "    # resume_info\n",
    "    {\"text\": \"What skills are listed in the resume?\", \"label\": \"resume_info\"},\n",
    "    {\"text\": \"List the programming languages known by the applicant.\", \"label\": \"resume_info\"},\n",
    "    {\"text\": \"Show me the candidate's work experience.\", \"label\": \"resume_info\"},\n",
    "    {\"text\": \"What certifications does the applicant have?\", \"label\": \"resume_info\"},\n",
    "    {\"text\": \"Summarize the professional experience section.\", \"label\": \"resume_info\"},\n",
    "    {\"text\": \"List the tools and technologies used by the candidate.\", \"label\": \"resume_info\"},\n",
    "    {\"text\": \"What is the educational background of the applicant?\", \"label\": \"resume_info\"},\n",
    "    {\"text\": \"What are the achievements or awards?\", \"label\": \"resume_info\"},\n",
    "    {\"text\": \"Show me the contact information in the resume.\", \"label\": \"resume_info\"},\n",
    "    {\"text\": \"What is the career objective or summary?\", \"label\": \"resume_info\"},\n",
    "    # technical_support\n",
    "    {\"text\": \"I have a technical issue with the system.\", \"label\": \"technical_support\"},\n",
    "    {\"text\": \"There is a problem with the website.\", \"label\": \"technical_support\"},\n",
    "    {\"text\": \"I can't log in to my account.\", \"label\": \"technical_support\"},\n",
    "    {\"text\": \"The upload button is not working.\", \"label\": \"technical_support\"},\n",
    "    {\"text\": \"How do I reset my password?\", \"label\": \"technical_support\"},\n",
    "    {\"text\": \"The page is loading very slowly.\", \"label\": \"technical_support\"},\n",
    "    {\"text\": \"I received an error message while submitting my form.\", \"label\": \"technical_support\"},\n",
    "    {\"text\": \"The system crashed during my session.\", \"label\": \"technical_support\"},\n",
    "    {\"text\": \"How do I contact technical support?\", \"label\": \"technical_support\"},\n",
    "    {\"text\": \"The app keeps freezing.\", \"label\": \"technical_support\"},\n",
    "    # general_info\n",
    "    {\"text\": \"Give me a summary of the file.\", \"label\": \"general_info\"},\n",
    "    {\"text\": \"Tell me about this document.\", \"label\": \"general_info\"},\n",
    "    {\"text\": \"What is the purpose of this document?\", \"label\": \"general_info\"},\n",
    "    {\"text\": \"Provide general information about the case.\", \"label\": \"general_info\"},\n",
    "    {\"text\": \"What are the office hours?\", \"label\": \"general_info\"},\n",
    "    {\"text\": \"How do I contact the support team?\", \"label\": \"general_info\"},\n",
    "    {\"text\": \"Where is the office located?\", \"label\": \"general_info\"},\n",
    "    {\"text\": \"What services are offered?\", \"label\": \"general_info\"},\n",
    "    {\"text\": \"How do I register for an account?\", \"label\": \"general_info\"},\n",
    "    {\"text\": \"What is the refund policy?\", \"label\": \"general_info\"},\n",
    "]\n",
    "\n",
    "# You can now use expanded_data instead of the old 'data' list for training your classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba16b2a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How do I file a claim?</td>\n",
       "      <td>claim_process</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the process for submitting an insuranc...</td>\n",
       "      <td>claim_process</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I want to submit a new claim for my car accident.</td>\n",
       "      <td>claim_process</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Guide me through the claim submission steps.</td>\n",
       "      <td>claim_process</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Where do I upload my claim documents?</td>\n",
       "      <td>claim_process</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How long does it take to process a claim?</td>\n",
       "      <td>claim_process</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Can I check the status of my insurance claim?</td>\n",
       "      <td>claim_process</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What documents are needed to file a claim?</td>\n",
       "      <td>claim_process</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Is there a deadline for submitting claims?</td>\n",
       "      <td>claim_process</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Can I cancel a claim after submitting?</td>\n",
       "      <td>claim_process</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What is the current status of the case?</td>\n",
       "      <td>case_status</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Show me the progress of case number 511605.</td>\n",
       "      <td>case_status</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Has a judgment been issued in my case?</td>\n",
       "      <td>case_status</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Is my case still pending?</td>\n",
       "      <td>case_status</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>When is the next hearing for my case?</td>\n",
       "      <td>case_status</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>What was the outcome of the last court session?</td>\n",
       "      <td>case_status</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Who is the presiding judge for this case?</td>\n",
       "      <td>case_status</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Has an appeal been filed?</td>\n",
       "      <td>case_status</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Is there an order available for my case?</td>\n",
       "      <td>case_status</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>What is the next step in my case?</td>\n",
       "      <td>case_status</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Can I get a copy of the case order?</td>\n",
       "      <td>document_request</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>How do I request the judgment document?</td>\n",
       "      <td>document_request</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>I need certified copies of my case documents.</td>\n",
       "      <td>document_request</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Where can I download the court forms?</td>\n",
       "      <td>document_request</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Request a copy of the final order.</td>\n",
       "      <td>document_request</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>How do I obtain previous hearing transcripts?</td>\n",
       "      <td>document_request</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Can I get a digital copy of my case file?</td>\n",
       "      <td>document_request</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>What is the fee for document requests?</td>\n",
       "      <td>document_request</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>How long does it take to receive requested doc...</td>\n",
       "      <td>document_request</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Is there a limit to the number of documents I ...</td>\n",
       "      <td>document_request</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>What skills are listed in the resume?</td>\n",
       "      <td>resume_info</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>List the programming languages known by the ap...</td>\n",
       "      <td>resume_info</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Show me the candidate's work experience.</td>\n",
       "      <td>resume_info</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>What certifications does the applicant have?</td>\n",
       "      <td>resume_info</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Summarize the professional experience section.</td>\n",
       "      <td>resume_info</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>List the tools and technologies used by the ca...</td>\n",
       "      <td>resume_info</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>What is the educational background of the appl...</td>\n",
       "      <td>resume_info</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>What are the achievements or awards?</td>\n",
       "      <td>resume_info</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Show me the contact information in the resume.</td>\n",
       "      <td>resume_info</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>What is the career objective or summary?</td>\n",
       "      <td>resume_info</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>I have a technical issue with the system.</td>\n",
       "      <td>technical_support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>There is a problem with the website.</td>\n",
       "      <td>technical_support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>I can't log in to my account.</td>\n",
       "      <td>technical_support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>The upload button is not working.</td>\n",
       "      <td>technical_support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>How do I reset my password?</td>\n",
       "      <td>technical_support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>The page is loading very slowly.</td>\n",
       "      <td>technical_support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>I received an error message while submitting m...</td>\n",
       "      <td>technical_support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>The system crashed during my session.</td>\n",
       "      <td>technical_support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>How do I contact technical support?</td>\n",
       "      <td>technical_support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>The app keeps freezing.</td>\n",
       "      <td>technical_support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Give me a summary of the file.</td>\n",
       "      <td>general_info</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Tell me about this document.</td>\n",
       "      <td>general_info</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>What is the purpose of this document?</td>\n",
       "      <td>general_info</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Provide general information about the case.</td>\n",
       "      <td>general_info</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>What are the office hours?</td>\n",
       "      <td>general_info</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>How do I contact the support team?</td>\n",
       "      <td>general_info</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Where is the office located?</td>\n",
       "      <td>general_info</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>What services are offered?</td>\n",
       "      <td>general_info</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>How do I register for an account?</td>\n",
       "      <td>general_info</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>What is the refund policy?</td>\n",
       "      <td>general_info</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text              label\n",
       "0                              How do I file a claim?      claim_process\n",
       "1   What is the process for submitting an insuranc...      claim_process\n",
       "2   I want to submit a new claim for my car accident.      claim_process\n",
       "3        Guide me through the claim submission steps.      claim_process\n",
       "4               Where do I upload my claim documents?      claim_process\n",
       "5           How long does it take to process a claim?      claim_process\n",
       "6       Can I check the status of my insurance claim?      claim_process\n",
       "7          What documents are needed to file a claim?      claim_process\n",
       "8          Is there a deadline for submitting claims?      claim_process\n",
       "9              Can I cancel a claim after submitting?      claim_process\n",
       "10            What is the current status of the case?        case_status\n",
       "11        Show me the progress of case number 511605.        case_status\n",
       "12             Has a judgment been issued in my case?        case_status\n",
       "13                          Is my case still pending?        case_status\n",
       "14              When is the next hearing for my case?        case_status\n",
       "15    What was the outcome of the last court session?        case_status\n",
       "16          Who is the presiding judge for this case?        case_status\n",
       "17                          Has an appeal been filed?        case_status\n",
       "18           Is there an order available for my case?        case_status\n",
       "19                  What is the next step in my case?        case_status\n",
       "20                Can I get a copy of the case order?   document_request\n",
       "21            How do I request the judgment document?   document_request\n",
       "22      I need certified copies of my case documents.   document_request\n",
       "23              Where can I download the court forms?   document_request\n",
       "24                 Request a copy of the final order.   document_request\n",
       "25      How do I obtain previous hearing transcripts?   document_request\n",
       "26          Can I get a digital copy of my case file?   document_request\n",
       "27             What is the fee for document requests?   document_request\n",
       "28  How long does it take to receive requested doc...   document_request\n",
       "29  Is there a limit to the number of documents I ...   document_request\n",
       "30              What skills are listed in the resume?        resume_info\n",
       "31  List the programming languages known by the ap...        resume_info\n",
       "32           Show me the candidate's work experience.        resume_info\n",
       "33       What certifications does the applicant have?        resume_info\n",
       "34     Summarize the professional experience section.        resume_info\n",
       "35  List the tools and technologies used by the ca...        resume_info\n",
       "36  What is the educational background of the appl...        resume_info\n",
       "37               What are the achievements or awards?        resume_info\n",
       "38     Show me the contact information in the resume.        resume_info\n",
       "39           What is the career objective or summary?        resume_info\n",
       "40          I have a technical issue with the system.  technical_support\n",
       "41               There is a problem with the website.  technical_support\n",
       "42                      I can't log in to my account.  technical_support\n",
       "43                  The upload button is not working.  technical_support\n",
       "44                        How do I reset my password?  technical_support\n",
       "45                   The page is loading very slowly.  technical_support\n",
       "46  I received an error message while submitting m...  technical_support\n",
       "47              The system crashed during my session.  technical_support\n",
       "48                How do I contact technical support?  technical_support\n",
       "49                            The app keeps freezing.  technical_support\n",
       "50                     Give me a summary of the file.       general_info\n",
       "51                       Tell me about this document.       general_info\n",
       "52              What is the purpose of this document?       general_info\n",
       "53        Provide general information about the case.       general_info\n",
       "54                         What are the office hours?       general_info\n",
       "55                 How do I contact the support team?       general_info\n",
       "56                       Where is the office located?       general_info\n",
       "57                         What services are offered?       general_info\n",
       "58                  How do I register for an account?       general_info\n",
       "59                         What is the refund policy?       general_info"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example labeled data for intent fine-tuning (expand with more real samples for best results)\n",
    "# data = [\n",
    "#     {\"text\": \"How do I file a claim?\", \"label\": \"claim_process\"},\n",
    "#     {\"text\": \"What is the process for submitting an insurance claim?\", \"label\": \"claim_process\"},\n",
    "#     {\"text\": \"What is the current status of the case?\", \"label\": \"case_status\"},\n",
    "#     {\"text\": \"Show me the progress of case number 511605.\", \"label\": \"case_status\"},\n",
    "#     {\"text\": \"Can I get a copy of the case order?\", \"label\": \"document_request\"},\n",
    "#     {\"text\": \"How do I request the judgment document?\", \"label\": \"document_request\"},\n",
    "#     {\"text\": \"What skills are listed in the resume?\", \"label\": \"resume_info\"},\n",
    "#     {\"text\": \"List the programming languages known by the applicant.\", \"label\": \"resume_info\"},\n",
    "#     {\"text\": \"Who is the presiding judge for this case?\", \"label\": \"court_details\"},\n",
    "#     {\"text\": \"Who are the parties involved in this case?\", \"label\": \"party_information\"},\n",
    "#     {\"text\": \"When was the last hearing held?\", \"label\": \"hearing_information\"},\n",
    "#     {\"text\": \"I have a technical issue with the system.\", \"label\": \"technical_support\"},\n",
    "#     {\"text\": \"Give me a summary of the file.\", \"label\": \"general_info\"},\n",
    "#     {\"text\": \"Tell me about this document.\", \"label\": \"general_info\"},\n",
    "# ]\n",
    "df = pd.DataFrame(expanded_data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a290cb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "class IntentClassifier:\n",
    "    def __init__(self, expanded_data, project_root, model_name=\"distilbert-base-uncased\"):\n",
    "        self.expanded_data = expanded_data\n",
    "        self.project_root = project_root\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = None\n",
    "        self.label2id = None\n",
    "        self.id2label = None\n",
    "        self.trainer = None\n",
    "        self.train_ds = None\n",
    "        self.test_ds = None\n",
    "\n",
    "    def build_label_mappings(self, df):\n",
    "        df['label_id'] = df['label'].astype('category').cat.codes\n",
    "        self.label2id = {label: i for i, label in enumerate(df['label'].astype('category').cat.categories)}\n",
    "        self.id2label = {i: label for label, i in self.label2id.items()}\n",
    "        return df\n",
    "\n",
    "    def prepare_dataset(self):\n",
    "        df = pd.DataFrame(self.expanded_data)\n",
    "        df = self.build_label_mappings(df)\n",
    "        df_for_hf = df.rename(columns={'label_id': 'labels'}).drop(columns=['label'])\n",
    "        dataset = Dataset.from_pandas(df_for_hf)\n",
    "        dataset = dataset.map(lambda example: self.tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=64), batched=True)\n",
    "        split = dataset.train_test_split(test_size=0.2)\n",
    "        self.train_ds, self.test_ds = split[\"train\"], split[\"test\"]\n",
    "\n",
    "    def setup_model_and_trainer(self):\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.model_name, num_labels=len(self.label2id), id2label=self.id2label, label2id=self.label2id)\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=\"./intent_model\",\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            do_eval=True,\n",
    "            per_device_train_batch_size=4,\n",
    "            per_device_eval_batch_size=4,\n",
    "            num_train_epochs=3,\n",
    "            logging_steps=10,\n",
    "            load_best_model_at_end=True,\n",
    "        )\n",
    "        self.trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=self.train_ds,\n",
    "            eval_dataset=self.test_ds,\n",
    "            tokenizer=self.tokenizer,\n",
    "        )\n",
    "\n",
    "    def train(self):\n",
    "        self.prepare_dataset()\n",
    "        self.setup_model_and_trainer()\n",
    "        self.trainer.train()\n",
    "\n",
    "    def predict_intent(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=64)\n",
    "        device = next(self.model.parameters()).device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        outputs = self.model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        pred_id = logits.argmax(dim=1).item()\n",
    "        intent = self.id2label[pred_id]\n",
    "        confidence = logits.softmax(dim=1)[0, pred_id].item()\n",
    "        return intent, confidence\n",
    "\n",
    "    def add_labeled_from_file(self, correct_intents_path):\n",
    "        chunk_dir = os.path.join(self.project_root, 'data', 'chunks')\n",
    "        labeled_from_file = []\n",
    "        with open(correct_intents_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                m = re.match(r'([^:]+):.*?\\'intent\\': \\'([^\\']+)\\'', line)\n",
    "                if m:\n",
    "                    fname, intent = m.group(1).strip(), m.group(2).strip()\n",
    "                    chunk_path = os.path.join(chunk_dir, fname)\n",
    "                    if os.path.exists(chunk_path):\n",
    "                        with open(chunk_path, 'r', encoding='utf-8') as cf:\n",
    "                            chunk_text = cf.read()\n",
    "                        labeled_from_file.append({\"text\": chunk_text, \"label\": intent})\n",
    "                    else:\n",
    "                        print(f\" Chunk file not found: {chunk_path}\")\n",
    "                else:\n",
    "                    print(f\" Could not parse line: {line.strip()}\")\n",
    "        print(f\"Loaded {len(labeled_from_file)} labeled examples from correct_intents.txt.\")\n",
    "        self.expanded_data.extend(labeled_from_file)\n",
    "        print(f\"expanded_data now has {len(self.expanded_data)} examples (including those from correct_intents.txt).\")\n",
    "\n",
    "    def batch_predict_chunks(self):\n",
    "        chunk_dir = os.path.join(self.project_root, 'data', 'chunks')\n",
    "        chunk_files = glob.glob(os.path.join(chunk_dir, '*.txt'))\n",
    "        for chunk_path in chunk_files:\n",
    "            with open(chunk_path, 'r', encoding='utf-8') as f:\n",
    "                chunk_text = f.read()\n",
    "            print(f\"{os.path.basename(chunk_path)}: {self.predict_intent(chunk_text) , self.get_embedding(chunk_text)}\")\n",
    "\n",
    "    def get_embedding(self, text):\n",
    "        # Get the embedding for the input text using the model's encoder\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=64)\n",
    "        device = next(self.model.parameters()).device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            if hasattr(self.model, \"distilbert\"):\n",
    "                outputs = self.model.distilbert(**inputs)\n",
    "                embedding = outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "            elif hasattr(self.model, \"bert\"):\n",
    "                outputs = self.model.bert(**inputs)\n",
    "                embedding = outputs.last_hidden_state[:, 0, :]\n",
    "            elif hasattr(self.model, \"roberta\"):\n",
    "                outputs = self.model.roberta(**inputs)\n",
    "                embedding = outputs.last_hidden_state[:, 0, :]\n",
    "            else:\n",
    "                outputs = self.model.base_model(**inputs)\n",
    "                embedding = outputs.last_hidden_state[:, 0, :]\n",
    "        return embedding.cpu().numpy().flatten()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d39a17b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'correct_intents_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m project_root \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m91807\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124msearch_models\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      2\u001b[0m classifier \u001b[38;5;241m=\u001b[39m IntentClassifier(expanded_data, project_root)\n\u001b[1;32m----> 3\u001b[0m classifier\u001b[38;5;241m.\u001b[39madd_labeled_from_file(\u001b[43mcorrect_intents_path\u001b[49m)\n\u001b[0;32m      4\u001b[0m classifier\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      5\u001b[0m classifier\u001b[38;5;241m.\u001b[39mbatch_predict_chunks()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'correct_intents_path' is not defined"
     ]
    }
   ],
   "source": [
    "project_root = r'C:\\Users\\91807\\Downloads\\search_models'\n",
    "classifier = IntentClassifier(expanded_data, project_root)\n",
    "classifier.add_labeled_from_file(correct_intents_path)\n",
    "classifier.train()\n",
    "classifier.batch_predict_chunks()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7aa69e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ac63eee",
   "metadata": {},
   "source": [
    "# keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbe2a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # 1. TF-IDF (Term FrequencyInverse Document Frequency)\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# text = open(r'C:\\New folder (5)\\new-search-models\\data\\chunks\\Jimson_Ratnam_JavaFullStackDeveloper_2+years_chunk1.txt', 'r', encoding='utf-8').read()\n",
    "\n",
    "# def extract_keywords_tfidf(text, top_n=10):\n",
    "#     vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,2))\n",
    "#     tfidf = vectorizer.fit_transform([text])\n",
    "#     scores = zip(vectorizer.get_feature_names_out(), tfidf.toarray()[0])\n",
    "#     sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "#     return [w for w, s in sorted_scores[:top_n]]\n",
    "\n",
    "# print('TF-IDF keywords:', extract_keywords_tfidf(text))\n",
    "\n",
    "# # 5. KeyBERT (Embedding-based, already in your notebook)\n",
    "# from keybert import KeyBERT\n",
    "# keyword_model = KeyBERT()\n",
    "# keywords = [kw for kw, _ in keyword_model.extract_keywords(text, top_n=10)]\n",
    "\n",
    "# print('KeyBERT keywords:', [kw for kw, _ in keyword_model.extract_keywords(text, top_n=10)])\n",
    "# # 6. spaCy POS-based (Nouns, Noun Phrases)\n",
    "# def extract_keywords_spacy(text, top_n=10):\n",
    "#     doc = nlp(text)\n",
    "#     noun_chunks = list(set(chunk.text.strip().lower() for chunk in doc.noun_chunks))\n",
    "#     nouns = list(set(token.lemma_ for token in doc if token.pos_ == 'NOUN' and not token.is_stop))\n",
    "#     return (noun_chunks + nouns)[:top_n]\n",
    "\n",
    "# print('spaCy POS keywords:', extract_keywords_spacy(text))\n",
    "\n",
    "\n",
    "# # --- Robust Keyword Extraction Pipeline ---\n",
    "# from keybert import KeyBERT\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# import spacy\n",
    "\n",
    "# # Load models (reuse if already loaded)\n",
    "# keyword_model = KeyBERT()\n",
    "# nlp = spacy.load(\"en_core_web_trf\") if spacy.util.is_package(\"en_core_web_trf\") else spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# # 1. KeyBERT keywords\n",
    "# keybert_keywords = [kw for kw, _ in keyword_model.extract_keywords(text, top_n=10)]\n",
    "\n",
    "# # 2. TF-IDF keywords\n",
    "# def extract_keywords_tfidf(text, top_n=10):\n",
    "#     vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,2))\n",
    "#     tfidf = vectorizer.fit_transform([text])\n",
    "#     scores = zip(vectorizer.get_feature_names_out(), tfidf.toarray()[0])\n",
    "#     sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "#     return [w for w, s in sorted_scores[:top_n]]\n",
    "# tfidf_keywords = extract_keywords_tfidf(text)\n",
    "\n",
    "# # 3. spaCy POS-based keywords\n",
    "# def extract_keywords_spacy(text, top_n=10):\n",
    "#     doc = nlp(text)\n",
    "#     noun_chunks = list(set(chunk.text.strip().lower() for chunk in doc.noun_chunks))\n",
    "#     nouns = list(set(token.lemma_ for token in doc if token.pos_ == 'NOUN' and not token.is_stop))\n",
    "#     return (noun_chunks + nouns)[:top_n]\n",
    "# spacy_keywords = extract_keywords_spacy(text)\n",
    "\n",
    "# # 4. Hybrid/ensemble: merge and deduplicate\n",
    "# all_keywords = keybert_keywords + tfidf_keywords + spacy_keywords\n",
    "# unique_keywords = []\n",
    "# for kw in all_keywords:\n",
    "#     if kw not in unique_keywords:\n",
    "#         unique_keywords.append(kw)\n",
    "\n",
    "# print(\"KeyBERT:\", keybert_keywords)\n",
    "# print(\"TF-IDF:\", tfidf_keywords)\n",
    "# print(\"spaCy POS:\", spacy_keywords)\n",
    "# print(\"\\n---\\nEnsemble (deduplicated):\", unique_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719ca9fc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c32bbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keybert import KeyBERT\n",
    "import spacy\n",
    "\n",
    "class KeywordExtractor:\n",
    "    def __init__(self, model=None, nlp_model=None):\n",
    "        self.keyword_model = model if model is not None else KeyBERT()\n",
    "        self.nlp = nlp_model if nlp_model is not None else spacy.load(\"en_core_web_trf\") if spacy.util.is_package(\"en_core_web_trf\") else spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    def extract_keywords_tfidf(self, text, top_n=10):\n",
    "        vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,2))\n",
    "        tfidf = vectorizer.fit_transform([text])\n",
    "        scores = zip(vectorizer.get_feature_names_out(), tfidf.toarray()[0])\n",
    "        sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "        return [w for w, s in sorted_scores[:top_n]]\n",
    "\n",
    "    def extract_keywords_keybert(self, text, top_n=10):\n",
    "        return [kw for kw, _ in self.keyword_model.extract_keywords(text, top_n=top_n)]\n",
    "\n",
    "    def extract_keywords_spacy(self, text, top_n=10):\n",
    "        doc = self.nlp(text)\n",
    "        noun_chunks = list(set(chunk.text.strip().lower() for chunk in doc.noun_chunks))\n",
    "        nouns = list(set(token.lemma_ for token in doc if token.pos_ == 'NOUN' and not token.is_stop))\n",
    "        return (noun_chunks + nouns)[:top_n]\n",
    "\n",
    "    def extract_all(self, text, top_n=10):\n",
    "        keybert_keywords = self.extract_keywords_keybert(text, top_n)\n",
    "        tfidf_keywords = self.extract_keywords_tfidf(text, top_n)\n",
    "        spacy_keywords = self.extract_keywords_spacy(text, top_n)\n",
    "        all_keywords = keybert_keywords + tfidf_keywords + spacy_keywords\n",
    "        unique_keywords = []\n",
    "        for kw in all_keywords:\n",
    "            if kw not in unique_keywords:\n",
    "                unique_keywords.append(kw)\n",
    "        return {\n",
    "            # \"keybert\": keybert_keywords,\n",
    "            # \"tfidf\": tfidf_keywords,\n",
    "            # \"spacy\": spacy_keywords,\n",
    "            \"ensemble\": unique_keywords\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a0018b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = KeywordExtractor(model=KeyBERT(), nlp_model=spacy.load(\"en_core_web_trf\") if spacy.util.is_package(\"en_core_web_trf\") else spacy.load(\"en_core_web_sm\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3ead1ea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ensemble': ['appeal',\n",
       "  'decree',\n",
       "  'procedure',\n",
       "  'petition',\n",
       "  'madras',\n",
       "  'filed',\n",
       "  'sriarumbayal',\n",
       "  'court',\n",
       "  'judicature',\n",
       "  'cmpno6648',\n",
       "  '2018',\n",
       "  'delay',\n",
       "  'assrno19304',\n",
       "  'assrno19304 2018',\n",
       "  'civil',\n",
       "  'cmpno6648 2018',\n",
       "  '2018 assrno19304',\n",
       "  '2540',\n",
       "  'a submission',\n",
       "  'perusal',\n",
       "  'respondents',\n",
       "  'the entire affidavit',\n",
       "  'their case',\n",
       "  'the appeal suit',\n",
       "  'the wife',\n",
       "  'behalf',\n",
       "  'they',\n",
       "  'the petitioners']}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword.extract_all(open(r'C:\\New folder (5)\\new-search-models\\data\\chunks\\MHC_CaseStatus_511605_chunk1.txt', 'r', encoding='utf-8').read(), top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c40296",
   "metadata": {},
   "source": [
    "# Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a370f857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. spaCy Named Entity Recognition (NER)\n",
    "# import spacy\n",
    "\n",
    "# # Load spaCy model (already loaded as nlp in previous cells)\n",
    "# doc = nlp(text)\n",
    "# spacy_entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "# print('spaCy Entities:', spacy_entities)\n",
    "\n",
    "# # 2. Transformers-based NER (e.g., HuggingFace pipeline)\n",
    "# from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "# ner_pipe = pipeline(\n",
    "#     \"ner\",\n",
    "#     model=AutoModelForTokenClassification.from_pretrained(\"Jean-Baptiste/roberta-large-ner-english\"),\n",
    "#     tokenizer=AutoTokenizer.from_pretrained(\"Jean-Baptiste/roberta-large-ner-english\"),\n",
    "#     aggregation_strategy=\"simple\",\n",
    "#     device=-1  # CPU\n",
    "# )\n",
    "# transformers_entities = [(ent['word'], ent['entity_group'], ent['score']) for ent in ner_pipe(text) if ent['score'] > 0.8]\n",
    "# print('Transformers NER Entities:', transformers_entities)\n",
    "\n",
    "# # 3. Regex-based Entity Extraction (for custom patterns)\n",
    "# import re\n",
    "\n",
    "# # Example: Extract email addresses and dates\n",
    "# emails = re.findall(r'[\\w\\.-]+@[\\w\\.-]+', text)\n",
    "# dates = re.findall(r'\\b\\d{4}-\\d{2}-\\d{2}\\b', text)\n",
    "# print('Emails:', emails)\n",
    "# print('Dates:', dates)\n",
    "# # Utility: Deduplicate entity_types in structured metadata output\n",
    "# # 4. Ensemble/Hybrid: Combine spaCy, Transformers, and Regex\n",
    "# def extract_entities_hybrid(text):\n",
    "#     entities = set()\n",
    "#     entity_types = []\n",
    "#     entity_details = []\n",
    "#     # spaCy\n",
    "#     for ent in nlp(text).ents:\n",
    "#         entities.add(ent.text)\n",
    "#         entity_types.append(ent.label_)\n",
    "#         entity_details.append({\n",
    "#             \"text\": ent.text,\n",
    "#             \"type\": ent.label_,\n",
    "#             \"score\": None\n",
    "#         })\n",
    "#     # Transformers\n",
    "#     for ent in ner_pipe(text):\n",
    "#         if ent['score'] > 0.8:\n",
    "#             entities.add(ent['word'])\n",
    "#             entity_types.append(ent['entity_group'])\n",
    "#             entity_details.append({\n",
    "#                 \"text\": ent['word'],\n",
    "#                 \"type\": ent['entity_group'],\n",
    "#                 \"score\": ent['score']\n",
    "#             })\n",
    "#     # Regex (add more patterns as needed)\n",
    "#     for email in re.findall(r'[\\w\\.-]+@[\\w\\.-]+', text):\n",
    "#         entities.add(email)\n",
    "#         entity_types.append(\"EMAIL\")\n",
    "#         entity_details.append({\n",
    "#             \"text\": email,\n",
    "#             \"type\": \"EMAIL\",\n",
    "#             \"score\": None\n",
    "#         })\n",
    "#     for date in re.findall(r'\\b\\d{4}-\\d{2}-\\d{2}\\b', text):\n",
    "#         entities.add(date)\n",
    "#         entity_types.append(\"DATE\")\n",
    "#         entity_details.append({\n",
    "#             \"text\": date,\n",
    "#             \"type\": \"DATE\",\n",
    "#             \"score\": None\n",
    "#         })\n",
    "#     return {\n",
    "#         \"entities\": sorted(entities),\n",
    "#         \"entity_types\": entity_types,\n",
    "#         \"entity_details\": entity_details\n",
    "#     }\n",
    "\n",
    "# # for chunk_path in chunk_files:\n",
    "# #     with open(chunk_path, 'r', encoding='utf-8') as f:\n",
    "# #         chunk_text = f.read()\n",
    "# #     print(f\"{os.path.basename(chunk_path)}: {extract_entities_hybrid(chunk_text)}\")\n",
    "\n",
    "#     # print('Hybrid Entities:', extract_entities_hybrid(text))\n",
    "# def get_metadata_structured_dedup(\n",
    "#     text, \n",
    "#     filename=\"\", \n",
    "#     document_name=\"\", \n",
    "#     summary=\"\", \n",
    "#     embedding=None\n",
    "# ):\n",
    "#     keywords = unique_keywords\n",
    "#     intent, intent_confidence = predict_intent(text)\n",
    "#     ner_results = extract_entities_hybrid(text)\n",
    "#     entities = ner_results.get(\"entities\", [])\n",
    "#     entity_types = list(dict.fromkeys(ner_results.get(\"entity_types\", [])))  # Deduplicate, preserve order\n",
    "#     entity_details = ner_results.get(\"entity_details\", [])\n",
    "#     if embedding is None:\n",
    "#         inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=64)\n",
    "#         device = next(model.parameters()).device\n",
    "#         inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model.distilbert(**inputs)\n",
    "#             embedding = outputs.last_hidden_state[:, 0, :].squeeze().cpu().tolist()\n",
    "#     return {\n",
    "#         \"keyword\": keywords,\n",
    "#         \"intent\": intent,\n",
    "#         \"intent_confidence\": intent_confidence,\n",
    "#         \"entities\": entities,\n",
    "#         \"entity_types\": entity_types,\n",
    "#         \"entity_details\": entity_details,\n",
    "#         \"summary\": summary,\n",
    "#         \"embedding\": embedding,\n",
    "#         \"text\": text,\n",
    "#         \"document_name\": document_name,\n",
    "#         \"filename\": filename\n",
    "#     }\n",
    "\n",
    "# # Example usage:\n",
    "# metadata_structured_dedup = get_metadata_structured_dedup(\n",
    "#     text=chunk_text,\n",
    "#     filename=chunk_path,\n",
    "#     document_name=\"Jimson_Ratnam_JavaFullStackDeveloper_2+years\",\n",
    "#     summary=\"\",\n",
    "# )\n",
    "# import pprint\n",
    "# pprint.pprint(metadata_structured_dedup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "114f9a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityExtractor:\n",
    "    def __init__(self, nlp, ner_pipe):\n",
    "        self.nlp = nlp\n",
    "        self.ner_pipe = ner_pipe\n",
    "        self.email_pattern = re.compile(r'[\\w\\.-]+@[\\w\\.-]+')\n",
    "        self.date_pattern = re.compile(r'\\b\\d{4}-\\d{2}-\\d{2}\\b')\n",
    "\n",
    "    def extract_spacy(self, text):\n",
    "        doc = self.nlp(text)\n",
    "        return [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "    def extract_transformers(self, text, score_threshold=0.8):\n",
    "        return [\n",
    "            (ent['word'], ent['entity_group'], ent['score'])\n",
    "            for ent in self.ner_pipe(text)\n",
    "            if ent['score'] > score_threshold\n",
    "        ]\n",
    "\n",
    "    def extract_regex(self, text):\n",
    "        emails = self.email_pattern.findall(text)\n",
    "        dates = self.date_pattern.findall(text)\n",
    "        return {'emails': emails, 'dates': dates}\n",
    "\n",
    "    def extract_entities_hybrid(self, text):\n",
    "        entities = set()\n",
    "        entity_types = []\n",
    "        entity_details = []\n",
    "\n",
    "        # spaCy\n",
    "        for ent in self.nlp(text).ents:\n",
    "            entities.add(ent.text)\n",
    "            entity_types.append(ent.label_)\n",
    "            entity_details.append({\n",
    "                \"text\": ent.text,\n",
    "                \"type\": ent.label_,\n",
    "                \"score\": None\n",
    "            })\n",
    "\n",
    "        # Transformers\n",
    "        for ent in self.ner_pipe(text):\n",
    "            if ent['score'] > 0.8:\n",
    "                entities.add(ent['word'])\n",
    "                entity_types.append(ent['entity_group'])\n",
    "                entity_details.append({\n",
    "                    \"text\": ent['word'],\n",
    "                    \"type\": ent['entity_group'],\n",
    "                    \"score\": ent['score']\n",
    "                })\n",
    "\n",
    "        # Regex\n",
    "        for email in self.email_pattern.findall(text):\n",
    "            entities.add(email)\n",
    "            entity_types.append(\"EMAIL\")\n",
    "            entity_details.append({\n",
    "                \"text\": email,\n",
    "                \"type\": \"EMAIL\",\n",
    "                \"score\": None\n",
    "            })\n",
    "        for date in self.date_pattern.findall(text):\n",
    "            entities.add(date)\n",
    "            entity_types.append(\"DATE\")\n",
    "            entity_details.append({\n",
    "                \"text\": date,\n",
    "                \"type\": \"DATE\",\n",
    "                \"score\": None\n",
    "            })\n",
    "\n",
    "        return {\n",
    "            \"entities\": sorted(entities),\n",
    "            \"entity_types\": set(entity_types),\n",
    "            \"entity_details\": entity_details\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbdac23c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "ner_pipe = pipeline(\n",
    "    \"ner\",\n",
    "    model=AutoModelForTokenClassification.from_pretrained(\"Jean-Baptiste/roberta-large-ner-english\"),\n",
    "    tokenizer=AutoTokenizer.from_pretrained(\"Jean-Baptiste/roberta-large-ner-english\"),\n",
    "    aggregation_strategy=\"simple\",\n",
    "    device=-1  # CPU\n",
    ")\n",
    "\n",
    "# Ensure nlp is defined (should already be available from previous cells)\n",
    "# If not, uncomment the following lines:\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_trf\") if spacy.util.is_package(\"en_core_web_trf\") else spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "entity = EntityExtractor(nlp, ner_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0651f0d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entities': ['20',\n",
       "  '25',\n",
       "  '40',\n",
       "  '50',\n",
       "  '90',\n",
       "  'AWS',\n",
       "  'AWS EC2 S3',\n",
       "  'Angular',\n",
       "  'Angular TypeScript',\n",
       "  'Devzen Software Solutions',\n",
       "  'Elasticsearch',\n",
       "  'MISC',\n",
       "  'ORG',\n",
       "  'RDS IAM',\n",
       "  'Spring Batch',\n",
       "  'Spring Boot',\n",
       "  'Spring Boot Angular',\n",
       "  'Spring Boot Spring Security',\n",
       "  'ach',\n",
       "  'achi',\n",
       "  'achie',\n",
       "  'achiev',\n",
       "  'achieve',\n",
       "  'act',\n",
       "  'acti',\n",
       "  'ang',\n",
       "  'angu',\n",
       "  'angul',\n",
       "  'angula',\n",
       "  'api',\n",
       "  'apis',\n",
       "  'aws',\n",
       "  'aws ec',\n",
       "  'boot',\n",
       "  'control',\n",
       "  'dev',\n",
       "  'deve',\n",
       "  'devz',\n",
       "  'devze',\n",
       "  'devzen',\n",
       "  'elastic',\n",
       "  'elasticse',\n",
       "  'elasticsea',\n",
       "  'elasticsear',\n",
       "  'elasticsearc',\n",
       "  'enhan',\n",
       "  'lev',\n",
       "  'leve',\n",
       "  'lever',\n",
       "  'levera',\n",
       "  'leverag',\n",
       "  'leverage',\n",
       "  'man',\n",
       "  'mana',\n",
       "  'manag',\n",
       "  'manage',\n",
       "  'managem',\n",
       "  'manageme',\n",
       "  'managemen',\n",
       "  'management',\n",
       "  'pre',\n",
       "  'pres',\n",
       "  'prese',\n",
       "  'presen',\n",
       "  'robust',\n",
       "  'robust admin control real',\n",
       "  'six months',\n",
       "  'software',\n",
       "  'sol',\n",
       "  'solu',\n",
       "  'solut',\n",
       "  'soluti',\n",
       "  'solutio',\n",
       "  'solution',\n",
       "  'spr',\n",
       "  'spri'],\n",
       " 'entity_types': {'CARDINAL', 'DATE', 'MISC', 'ORG', 'PRODUCT'},\n",
       " 'entity_details': [{'text': 'boot', 'type': 'ORG', 'score': None},\n",
       "  {'text': '20', 'type': 'CARDINAL', 'score': None},\n",
       "  {'text': '40', 'type': 'CARDINAL', 'score': None},\n",
       "  {'text': '50', 'type': 'CARDINAL', 'score': None},\n",
       "  {'text': '90', 'type': 'CARDINAL', 'score': None},\n",
       "  {'text': 'control', 'type': 'ORG', 'score': None},\n",
       "  {'text': 'dev', 'type': 'ORG', 'score': None},\n",
       "  {'text': 'deve', 'type': 'ORG', 'score': None},\n",
       "  {'text': 'devz', 'type': 'ORG', 'score': None},\n",
       "  {'text': 'devze', 'type': 'ORG', 'score': None},\n",
       "  {'text': 'devzen', 'type': 'ORG', 'score': None},\n",
       "  {'text': 'elastic', 'type': 'ORG', 'score': None},\n",
       "  {'text': 'elasticse', 'type': 'ORG', 'score': None},\n",
       "  {'text': 'elasticsea', 'type': 'ORG', 'score': None},\n",
       "  {'text': 'elasticsear', 'type': 'ORG', 'score': None},\n",
       "  {'text': 'elasticsearc', 'type': 'ORG', 'score': None},\n",
       "  {'text': 'enhan', 'type': 'ORG', 'score': None},\n",
       "  {'text': 'lev', 'type': 'ORG', 'score': None},\n",
       "  {'text': 'leve', 'type': 'ORG', 'score': None},\n",
       "  {'text': 'lever', 'type': 'ORG', 'score': None},\n",
       "  {'text': 'levera', 'type': 'ORG', 'score': None},\n",
       "  {'text': 'leverag', 'type': 'ORG', 'score': None},\n",
       "  {'text': 'leverage', 'type': 'ORG', 'score': None},\n",
       "  {'text': 'man', 'type': 'ORG', 'score': None},\n",
       "  {'text': 'mana', 'type': 'ORG', 'score': None},\n",
       "  {'text': 'manag', 'type': 'ORG', 'score': None},\n",
       "  {'text': 'manage', 'type': 'ORG', 'score': None},\n",
       "  {'text': 'managem', 'type': 'ORG', 'score': None},\n",
       "  {'text': 'manageme', 'type': 'ORG', 'score': None},\n",
       "  {'text': 'managemen', 'type': 'ORG', 'score': None},\n",
       "  {'text': 'management', 'type': 'ORG', 'score': None},\n",
       "  {'text': 'pre', 'type': 'ORG', 'score': None},\n",
       "  {'text': 'pres', 'type': 'ORG', 'score': None},\n",
       "  {'text': 'prese', 'type': 'ORG', 'score': None},\n",
       "  {'text': 'presen', 'type': 'ORG', 'score': None},\n",
       "  {'text': 'robust', 'type': 'ORG', 'score': None},\n",
       "  {'text': 'robust admin control real', 'type': 'ORG', 'score': None},\n",
       "  {'text': 'software', 'type': 'ORG', 'score': None},\n",
       "  {'text': 'sol', 'type': 'ORG', 'score': None},\n",
       "  {'text': 'solu', 'type': 'ORG', 'score': None},\n",
       "  {'text': 'solut', 'type': 'ORG', 'score': None},\n",
       "  {'text': 'soluti', 'type': 'ORG', 'score': None},\n",
       "  {'text': 'solutio', 'type': 'ORG', 'score': None},\n",
       "  {'text': 'solution', 'type': 'ORG', 'score': None},\n",
       "  {'text': 'spr', 'type': 'ORG', 'score': None},\n",
       "  {'text': 'spri', 'type': 'ORG', 'score': None},\n",
       "  {'text': 'devzen', 'type': 'ORG', 'score': None},\n",
       "  {'text': 'ORG', 'type': 'ORG', 'score': None},\n",
       "  {'text': 'MISC', 'type': 'ORG', 'score': None},\n",
       "  {'text': 'MISC', 'type': 'ORG', 'score': None},\n",
       "  {'text': 'MISC', 'type': 'ORG', 'score': None},\n",
       "  {'text': 'MISC', 'type': 'ORG', 'score': None},\n",
       "  {'text': 'MISC', 'type': 'ORG', 'score': None},\n",
       "  {'text': 'MISC', 'type': 'ORG', 'score': None},\n",
       "  {'text': 'MISC', 'type': 'ORG', 'score': None},\n",
       "  {'text': 'Devzen Software Solutions', 'type': 'ORG', 'score': None},\n",
       "  {'text': 'Spring Boot', 'type': 'PRODUCT', 'score': None},\n",
       "  {'text': 'Angular', 'type': 'PRODUCT', 'score': None},\n",
       "  {'text': 'AWS', 'type': 'ORG', 'score': None},\n",
       "  {'text': 'Devzen Software Solutions', 'type': 'ORG', 'score': None},\n",
       "  {'text': 'Spring Boot Spring Security', 'type': 'PRODUCT', 'score': None},\n",
       "  {'text': 'Spring Batch', 'type': 'PRODUCT', 'score': None},\n",
       "  {'text': 'Elasticsearch', 'type': 'PRODUCT', 'score': None},\n",
       "  {'text': 'Angular TypeScript', 'type': 'PRODUCT', 'score': None},\n",
       "  {'text': 'Spring Boot Angular', 'type': 'PRODUCT', 'score': None},\n",
       "  {'text': 'AWS EC2 S3', 'type': 'PRODUCT', 'score': None},\n",
       "  {'text': 'RDS IAM', 'type': 'PRODUCT', 'score': None},\n",
       "  {'text': 'Spring Boot', 'type': 'PRODUCT', 'score': None},\n",
       "  {'text': '25', 'type': 'CARDINAL', 'score': None},\n",
       "  {'text': '50', 'type': 'CARDINAL', 'score': None},\n",
       "  {'text': '20', 'type': 'CARDINAL', 'score': None},\n",
       "  {'text': 'six months', 'type': 'DATE', 'score': None},\n",
       "  {'text': '40', 'type': 'CARDINAL', 'score': None},\n",
       "  {'text': '90', 'type': 'CARDINAL', 'score': None},\n",
       "  {'text': 'apis', 'type': 'MISC', 'score': np.float32(0.9821881)},\n",
       "  {'text': 'api', 'type': 'MISC', 'score': np.float32(0.8888545)},\n",
       "  {'text': 'aws', 'type': 'MISC', 'score': np.float32(0.9064584)},\n",
       "  {'text': 'ach', 'type': 'MISC', 'score': np.float32(0.90290034)},\n",
       "  {'text': 'achi', 'type': 'MISC', 'score': np.float32(0.89944017)},\n",
       "  {'text': 'achie', 'type': 'MISC', 'score': np.float32(0.8379496)},\n",
       "  {'text': 'achiev', 'type': 'MISC', 'score': np.float32(0.8932801)},\n",
       "  {'text': 'achieve', 'type': 'MISC', 'score': np.float32(0.89781415)},\n",
       "  {'text': 'act', 'type': 'MISC', 'score': np.float32(0.84830976)},\n",
       "  {'text': 'acti', 'type': 'MISC', 'score': np.float32(0.8248422)},\n",
       "  {'text': 'ang', 'type': 'MISC', 'score': np.float32(0.9267342)},\n",
       "  {'text': 'angu', 'type': 'MISC', 'score': np.float32(0.9217035)},\n",
       "  {'text': 'angul', 'type': 'MISC', 'score': np.float32(0.93581474)},\n",
       "  {'text': 'angula', 'type': 'MISC', 'score': np.float32(0.9128641)},\n",
       "  {'text': 'api', 'type': 'MISC', 'score': np.float32(0.90031314)},\n",
       "  {'text': 'aws', 'type': 'MISC', 'score': np.float32(0.840629)},\n",
       "  {'text': 'aws ec', 'type': 'MISC', 'score': np.float32(0.8983599)}]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "entity.extract_entities_hybrid(open(r'C:\\New folder (5)\\new-search-models\\data\\output_data\\Jimson_Ratnam_JavaFullStackDeveloper_2+years_chunk1.json', 'r', encoding='utf-8').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa7bfaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "search_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
