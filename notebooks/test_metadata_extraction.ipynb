{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a09ded39",
   "metadata": {},
   "source": [
    "# 🔍 Metadata Extraction Test Notebook\n",
    "This notebook helps you test the `extract_metadata()` pipeline using a custom text chunk input.\n",
    "\n",
    "You can modify the `text` variable in the next cell to test different examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87095be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d0a35ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Load spaCy model\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_trf\")\n",
    "except Exception:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Helper: normalize entity\n",
    "def normalize_entity(e):\n",
    "    text = re.sub(r'\\s+', ' ', re.sub(r'\\.', '', e.lower())).strip()\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([token.lemma_ for token in doc])\n",
    "\n",
    "# Intent keywords and examples (copy from your scripts)\n",
    "intent_keywords = {\n",
    "    \"claim_process\": [\"claim\", \"process\", \"file\", \"submit\", \"insurance\"],\n",
    "    \"case_status\": [\"status\", \"case\", \"update\", \"progress\", \"judgement\", \"order\", \"appeal\"],\n",
    "    \"document_request\": [\"document\", \"request\", \"copies\", \"forms\"],\n",
    "    \"technical_support\": [\"error\", \"issue\", \"problem\", \"technical\"],\n",
    "    \"general_info\": [\"information\", \"contact\", \"hours\", \"location\"],\n",
    "    \"resume_info\": [\n",
    "        \"skills\", \"resume\", \"cv\", \"proficiencies\", \"abilities\", \"expertise\", \"competencies\", \"qualifications\",\n",
    "        \"experience\", \"work\", \"education\", \"background\", \"certifications\", \"projects\", \"programming\", \"languages\",\n",
    "        \"achievements\", \"awards\", \"contact\", \"career\", \"summary\", \"tools\", \"technologies\", \"roles\", \"responsibilities\",\n",
    "        \"soft skills\", \"applicant\", \"candidate\", \"developer\", \"engineer\", \"profile\", \"professional\", \"employment\", \"history\",\n",
    "        \"management\", \"software\", \"admin\", \"implementation\", \"tracking\", \"project\", \"system\", \"solution\", \"platform\", \"application\",\n",
    "        \"team\", \"lead\", \"player\", \"restocking\", \"inventory\", \"tournament\", \"manual\", \"implemented\"\n",
    "    ]\n",
    "}\n",
    "project_root = os.environ.get('PROJECT_ROOT', os.getcwd())\n",
    "intent_examples_path = os.path.join(project_root,'..','data', 'intent_categories', 'intent_examples.json')\n",
    "with open(intent_examples_path, 'r', encoding='utf-8') as f:\n",
    "    intent_examples = json.load(f)\n",
    "# intent_examples = {\n",
    "#     \"resume_info\": [\n",
    "#         \"What skills are listed in the resume?\",\n",
    "#         \"Show me the proficiencies in this CV.\",\n",
    "#         \"List the abilities mentioned in the candidate's resume.\",\n",
    "#         \"What expertise does the applicant have?\",\n",
    "#         \"Which competencies are present in the resume?\",\n",
    "#         \"What qualifications are included in the CV?\",\n",
    "#         \"List the work experience of the candidate.\",\n",
    "#         \"What is the educational background of the applicant?\",\n",
    "#         \"Show me the certifications in this resume.\",\n",
    "#         \"What are the technical skills mentioned?\",\n",
    "#         \"Summarize the professional experience section.\",\n",
    "#         \"What projects has the candidate worked on?\",\n",
    "#         \"List the programming languages known by the applicant.\",\n",
    "#         \"What are the achievements or awards?\",\n",
    "#         \"Show me the contact information in the resume.\",\n",
    "#         \"What is the career objective or summary?\",\n",
    "#         \"List the tools and technologies used by the candidate.\",\n",
    "#         \"What is the total experience in years?\",\n",
    "#         \"Show me the roles and responsibilities held.\",\n",
    "#         \"What are the soft skills mentioned?\",\n",
    "#         \"List the languages spoken by the applicant.\",\n",
    "#         \"What is the applicant's job title?\",\n",
    "#         \"Describe the applicant's professional profile.\",\n",
    "#         \"What companies has the candidate worked for?\",\n",
    "#         \"List the frameworks and libraries used.\",\n",
    "#         \"What cloud platforms does the candidate have experience with?\",\n",
    "#         \"What development methodologies are mentioned?\",\n",
    "#         \"List the certifications and licenses.\",\n",
    "#         \"What leadership roles has the candidate held?\",\n",
    "#         \"Summarize the applicant's employment history.\",\n",
    "#         \"What is the candidate's GitHub or portfolio link?\",\n",
    "#         \"Describe the candidate's experience in management and software projects.\",\n",
    "#         \"What inventory or tracking systems has the applicant implemented?\",\n",
    "#         \"List any admin or manual processes managed by the candidate.\",\n",
    "#         \"What experience does the candidate have with tournaments or players?\",\n",
    "#         \"Describe the candidate's role in restocking or inventory management.\",\n",
    "#         \"What solutions or platforms has the applicant developed or led?\",\n",
    "#         \"List any applications or systems the candidate has worked on.\",\n",
    "#         \"What teams has the candidate led or been a part of?\",\n",
    "#         \"Describe the candidate's experience with project implementation.\"\n",
    "#     ],\n",
    "#     \"claim_process\": [\"How do I file a claim?\", \"What is the process for submitting an insurance claim?\"],\n",
    "#     \"case_status\": [\"What is the current status of the case?\", \"Show me the progress of case number 511605.\"],\n",
    "#     \"document_request\": [\"Can I get a copy of the case order?\", \"How do I request the judgment document?\"],\n",
    "#     \"technical_support\": [\"I have a technical issue.\", \"There is a problem with the system.\"],\n",
    "#     \"general_info\": [\"What is the purpose of this document?\", \"Give me a summary of the file.\"]\n",
    "# }\n",
    "intent_model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "def get_intent(text):\n",
    "    doc = nlp(text.lower())\n",
    "    tokens = [token.lemma_ for token in doc]\n",
    "    detected_intent = None\n",
    "    max_matches = 0\n",
    "    for intent, keywords in intent_keywords.items():\n",
    "        matches = sum(kw in tokens for kw in keywords)\n",
    "        if matches > max_matches:\n",
    "            max_matches = matches\n",
    "            detected_intent = intent\n",
    "    intent_confidence = max_matches / max(1, len(intent_keywords.get(detected_intent, []))) if detected_intent else 0.0\n",
    "    if not detected_intent or max_matches == 0:\n",
    "        query_emb = intent_model.encode(text, convert_to_tensor=True)\n",
    "        best_intent, best_score = None, 0\n",
    "        for intent, examples in intent_examples.items():\n",
    "            example_embs = intent_model.encode(examples, convert_to_tensor=True)\n",
    "            scores = util.pytorch_cos_sim(query_emb, example_embs)\n",
    "            max_score = scores.max().item()\n",
    "            if max_score > best_score:\n",
    "                best_score = max_score\n",
    "                best_intent = intent\n",
    "        if best_score > 0.35:\n",
    "            detected_intent = best_intent\n",
    "            intent_confidence = best_score\n",
    "    if not detected_intent:\n",
    "        detected_intent = \"general_info\"\n",
    "        intent_confidence = 0.0\n",
    "    return detected_intent, intent_confidence, None\n",
    "\n",
    "def extract_metadata(text):\n",
    "    doc = nlp(text)\n",
    "    # Entities\n",
    "    entities = [normalize_entity(ent.text) for ent in doc.ents]\n",
    "    # Keywords (nouns, proper nouns, not stopwords)\n",
    "    keywords = [normalize_entity(token.text) for token in doc if token.pos_ in [\"NOUN\", \"PROPN\"] and not token.is_stop and token.lemma_.lower() not in ENGLISH_STOP_WORDS and len(token.text) > 2]\n",
    "    # Intent\n",
    "    detected_intent, intent_confidence, _ = get_intent(text)\n",
    "    return {\n",
    "        \"entities\": entities,\n",
    "        \"keywords\": keywords,\n",
    "        \"intent\": detected_intent,\n",
    "        \"intent_confidence\": intent_confidence\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f8da8c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy version: 3.8.7\n",
      "SentenceTransformers version: 4.1.0\n",
      "Transformers version: 4.52.4\n",
      "spaCy model: core_web_trf\n",
      "Intent keywords: {'claim_process': ['claim', 'process', 'file', 'submit', 'insurance'], 'case_status': ['status', 'case', 'update', 'progress', 'judgement', 'order', 'appeal'], 'document_request': ['document', 'request', 'copies', 'forms'], 'technical_support': ['error', 'issue', 'problem', 'technical'], 'general_info': ['information', 'contact', 'hours', 'location'], 'resume_info': ['skills', 'resume', 'cv', 'proficiencies', 'abilities', 'expertise', 'competencies', 'qualifications', 'experience', 'work', 'education', 'background', 'certifications', 'projects', 'programming', 'languages', 'achievements', 'awards', 'contact', 'career', 'summary', 'tools', 'technologies', 'roles', 'responsibilities', 'soft skills', 'applicant', 'candidate', 'developer', 'engineer', 'profile', 'professional', 'employment', 'history', 'management', 'software', 'admin', 'implementation', 'tracking', 'project', 'system', 'solution', 'platform', 'application', 'team', 'lead', 'player', 'restocking', 'inventory', 'tournament', 'manual', 'implemented']}\n",
      "Intent examples: ['general_info', 'resume_info', 'case_status', 'court_details', 'party_information', 'hearing_information', 'document_request']\n",
      "Text sample: 'Java Full Stack Developer 2023 - Present Devzen Software Solutions Developed secure JWT-based authentication verification APIs using Spring Boot Spring Security Built batch processing workflows with Spring Batch for large-scale data management Designed REST APIs for real-time inventory tracking alerts optimizing stock management Integrated Elasticsearch for efficient search retrieval in high-volume applications Developed dynamic responsive UI components using Angular TypeScript for product manag'\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import sentence_transformers\n",
    "import transformers\n",
    "import spacy\n",
    "\n",
    "# Print environment and config for debugging\n",
    "try:\n",
    "    print('spaCy version:', spacy.__version__)\n",
    "except Exception:\n",
    "    print('spaCy not available')\n",
    "try:\n",
    "    print('SentenceTransformers version:', sentence_transformers.__version__)\n",
    "except Exception:\n",
    "    print('SentenceTransformers not available')\n",
    "try:\n",
    "    print('Transformers version:', transformers.__version__)\n",
    "except Exception:\n",
    "    print('Transformers not available')\n",
    "try:\n",
    "    print('spaCy model:', nlp.meta['name'] if nlp else 'None')\n",
    "except Exception:\n",
    "    print('spaCy model: None')\n",
    "try:\n",
    "    print('Intent keywords:', intent_keywords)\n",
    "except Exception:\n",
    "    print('Intent keywords: not loaded')\n",
    "try:\n",
    "    print('Intent examples:', list(intent_examples.keys()))\n",
    "except Exception:\n",
    "    print('Intent examples: not loaded')\n",
    "if 'text' in globals():\n",
    "    print('Text sample:', repr(text[:500]))\n",
    "else:\n",
    "    print('No text loaded yet.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60d5d43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\new folder (5)\\new-search-models\\search_env\\lib\\site-packages (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "c:\\New folder (5)\\new-search-models\\search_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\New folder (5)\\new-search-models\\search_env\\lib\\site-packages\\thinc\\shims\\pytorch.py:261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(filelike, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv\n",
    "\n",
    "import sys\n",
    "import os, json, yaml\n",
    "\n",
    "project_root = 'c:\\\\New folder (5)\\\\new-search-models'\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "# Load config.yaml using the correct path\n",
    "config_path = os.path.join(project_root,  'config.yaml')\n",
    "if os.path.exists(config_path):\n",
    "\twith open(config_path, 'r') as f:\n",
    "\t\tconfig = yaml.safe_load(f)\n",
    "else:\n",
    "\tprint(f\"⚠️ config.yaml not found at {config_path}\")\n",
    "\tconfig = {}\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(os.path.join(project_root, 'config', '.env'))\n",
    "from scripts.entity_utils import normalize_entity, get_spacy_nlp\n",
    "from scripts.search_pipeline import get_openai_embedding\n",
    "from scripts.intent_utils import get_intent\n",
    "# from scripts.metadata import extract_metadata  # <-- You must have your full extraction code in this file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68df9c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sample Text Loaded\n"
     ]
    }
   ],
   "source": [
    "with open(r'C:\\New folder (5)\\new-search-models\\data\\chunks\\Jimson_Ratnam_JavaFullStackDeveloper_2+years_chunk1.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "print('✅ Sample Text Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "29006c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entities': ['devzen',\n",
       "  'spring boot spring security',\n",
       "  'spring batch',\n",
       "  'elasticsearch',\n",
       "  'angular typescript',\n",
       "  'spring boot angular',\n",
       "  'aw ec2 s3 rd iam',\n",
       "  'spring boot',\n",
       "  'angular',\n",
       "  '25',\n",
       "  '50',\n",
       "  'portal',\n",
       "  '20',\n",
       "  'six month of launch',\n",
       "  '40',\n",
       "  '90'],\n",
       " 'keywords': ['java',\n",
       "  'stack',\n",
       "  'developer',\n",
       "  'devzen',\n",
       "  'software',\n",
       "  'solution',\n",
       "  'jwt',\n",
       "  'authentication',\n",
       "  'verification',\n",
       "  'apis',\n",
       "  'spring',\n",
       "  'boot',\n",
       "  'spring',\n",
       "  'security',\n",
       "  'batch',\n",
       "  'processing',\n",
       "  'workflow',\n",
       "  'spring',\n",
       "  'batch',\n",
       "  'scale',\n",
       "  'datum',\n",
       "  'management',\n",
       "  'rest',\n",
       "  'apis',\n",
       "  'time',\n",
       "  'inventory',\n",
       "  'tracking',\n",
       "  'alert',\n",
       "  'stock',\n",
       "  'management',\n",
       "  'elasticsearch',\n",
       "  'search',\n",
       "  'retrieval',\n",
       "  'volume',\n",
       "  'application',\n",
       "  'component',\n",
       "  'angular',\n",
       "  'typescript',\n",
       "  'product',\n",
       "  'management',\n",
       "  'payment',\n",
       "  'gateway',\n",
       "  'integration',\n",
       "  'spring',\n",
       "  'boot',\n",
       "  'angular',\n",
       "  'transaction',\n",
       "  'dashboard',\n",
       "  'key',\n",
       "  'insight',\n",
       "  'tournament',\n",
       "  'inventory',\n",
       "  'management',\n",
       "  'application',\n",
       "  'docker',\n",
       "  'consistency',\n",
       "  'development',\n",
       "  'production',\n",
       "  'environment',\n",
       "  'cicd',\n",
       "  'pipeline',\n",
       "  'build',\n",
       "  'test',\n",
       "  'deployment',\n",
       "  'process',\n",
       "  'github',\n",
       "  'action',\n",
       "  'workflow',\n",
       "  'deployment',\n",
       "  'efficiency',\n",
       "  'application',\n",
       "  'aws',\n",
       "  'ec2',\n",
       "  'rds',\n",
       "  'iam',\n",
       "  'cloud',\n",
       "  'hosting',\n",
       "  'management',\n",
       "  'apis',\n",
       "  'spring',\n",
       "  'boot',\n",
       "  'document',\n",
       "  'management',\n",
       "  'user',\n",
       "  'authentication',\n",
       "  'security',\n",
       "  'efficiency',\n",
       "  'angular',\n",
       "  'user',\n",
       "  'experience',\n",
       "  'document',\n",
       "  'creation',\n",
       "  'socialization',\n",
       "  'publication',\n",
       "  'key',\n",
       "  'achievement',\n",
       "  'security',\n",
       "  'user',\n",
       "  'interaction',\n",
       "  'angular',\n",
       "  'api',\n",
       "  'user',\n",
       "  'authentication',\n",
       "  'verification',\n",
       "  'process',\n",
       "  'document',\n",
       "  'management',\n",
       "  'search',\n",
       "  'security',\n",
       "  'access',\n",
       "  'functionality',\n",
       "  'improvement',\n",
       "  'user',\n",
       "  'incident',\n",
       "  'satisfaction',\n",
       "  'score',\n",
       "  'document',\n",
       "  'management',\n",
       "  'improve',\n",
       "  'tournament',\n",
       "  'efficiency',\n",
       "  'feature',\n",
       "  'document',\n",
       "  'creation',\n",
       "  'end',\n",
       "  'end',\n",
       "  'platform',\n",
       "  'socialization',\n",
       "  'publication',\n",
       "  'portal',\n",
       "  'user',\n",
       "  'player',\n",
       "  'registration',\n",
       "  'tournament',\n",
       "  'scheduling',\n",
       "  'base',\n",
       "  'month',\n",
       "  'launch',\n",
       "  'administration',\n",
       "  'time',\n",
       "  'tournament',\n",
       "  'management',\n",
       "  'process',\n",
       "  'admin',\n",
       "  'control',\n",
       "  'time',\n",
       "  'scoring',\n",
       "  'score',\n",
       "  'update',\n",
       "  'player',\n",
       "  'admin',\n",
       "  'dashboard',\n",
       "  'spectator',\n",
       "  'score',\n",
       "  'update',\n",
       "  'delay',\n",
       "  'tournament',\n",
       "  'oversight'],\n",
       " 'intent': 'resume_info',\n",
       " 'intent_confidence': 0.28846153846153844}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = extract_metadata(text)\n",
    "\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7b04ab4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extracted Metadata:\n",
      "{'entities': ['devzen',\n",
      "              'spring boot spring security',\n",
      "              'spring batch',\n",
      "              'elasticsearch',\n",
      "              'angular typescript',\n",
      "              'spring boot angular',\n",
      "              'aw ec2 s3 rd iam',\n",
      "              'spring boot',\n",
      "              'angular',\n",
      "              '25',\n",
      "              '50',\n",
      "              'portal',\n",
      "              '20',\n",
      "              'six month of launch',\n",
      "              '40',\n",
      "              '90'],\n",
      " 'intent': 'resume_info',\n",
      " 'intent_confidence': 0.28846153846153844,\n",
      " 'keywords': ['java',\n",
      "              'stack',\n",
      "              'developer',\n",
      "              'devzen',\n",
      "              'software',\n",
      "              'solution',\n",
      "              'jwt',\n",
      "              'authentication',\n",
      "              'verification',\n",
      "              'apis',\n",
      "              'spring',\n",
      "              'boot',\n",
      "              'spring',\n",
      "              'security',\n",
      "              'batch',\n",
      "              'processing',\n",
      "              'workflow',\n",
      "              'spring',\n",
      "              'batch',\n",
      "              'scale',\n",
      "              'datum',\n",
      "              'management',\n",
      "              'rest',\n",
      "              'apis',\n",
      "              'time',\n",
      "              'inventory',\n",
      "              'tracking',\n",
      "              'alert',\n",
      "              'stock',\n",
      "              'management',\n",
      "              'elasticsearch',\n",
      "              'search',\n",
      "              'retrieval',\n",
      "              'volume',\n",
      "              'application',\n",
      "              'component',\n",
      "              'angular',\n",
      "              'typescript',\n",
      "              'product',\n",
      "              'management',\n",
      "              'payment',\n",
      "              'gateway',\n",
      "              'integration',\n",
      "              'spring',\n",
      "              'boot',\n",
      "              'angular',\n",
      "              'transaction',\n",
      "              'dashboard',\n",
      "              'key',\n",
      "              'insight',\n",
      "              'tournament',\n",
      "              'inventory',\n",
      "              'management',\n",
      "              'application',\n",
      "              'docker',\n",
      "              'consistency',\n",
      "              'development',\n",
      "              'production',\n",
      "              'environment',\n",
      "              'cicd',\n",
      "              'pipeline',\n",
      "              'build',\n",
      "              'test',\n",
      "              'deployment',\n",
      "              'process',\n",
      "              'github',\n",
      "              'action',\n",
      "              'workflow',\n",
      "              'deployment',\n",
      "              'efficiency',\n",
      "              'application',\n",
      "              'aws',\n",
      "              'ec2',\n",
      "              'rds',\n",
      "              'iam',\n",
      "              'cloud',\n",
      "              'hosting',\n",
      "              'management',\n",
      "              'apis',\n",
      "              'spring',\n",
      "              'boot',\n",
      "              'document',\n",
      "              'management',\n",
      "              'user',\n",
      "              'authentication',\n",
      "              'security',\n",
      "              'efficiency',\n",
      "              'angular',\n",
      "              'user',\n",
      "              'experience',\n",
      "              'document',\n",
      "              'creation',\n",
      "              'socialization',\n",
      "              'publication',\n",
      "              'key',\n",
      "              'achievement',\n",
      "              'security',\n",
      "              'user',\n",
      "              'interaction',\n",
      "              'angular',\n",
      "              'api',\n",
      "              'user',\n",
      "              'authentication',\n",
      "              'verification',\n",
      "              'process',\n",
      "              'document',\n",
      "              'management',\n",
      "              'search',\n",
      "              'security',\n",
      "              'access',\n",
      "              'functionality',\n",
      "              'improvement',\n",
      "              'user',\n",
      "              'incident',\n",
      "              'satisfaction',\n",
      "              'score',\n",
      "              'document',\n",
      "              'management',\n",
      "              'improve',\n",
      "              'tournament',\n",
      "              'efficiency',\n",
      "              'feature',\n",
      "              'document',\n",
      "              'creation',\n",
      "              'end',\n",
      "              'end',\n",
      "              'platform',\n",
      "              'socialization',\n",
      "              'publication',\n",
      "              'portal',\n",
      "              'user',\n",
      "              'player',\n",
      "              'registration',\n",
      "              'tournament',\n",
      "              'scheduling',\n",
      "              'base',\n",
      "              'month',\n",
      "              'launch',\n",
      "              'administration',\n",
      "              'time',\n",
      "              'tournament',\n",
      "              'management',\n",
      "              'process',\n",
      "              'admin',\n",
      "              'control',\n",
      "              'time',\n",
      "              'scoring',\n",
      "              'score',\n",
      "              'update',\n",
      "              'player',\n",
      "              'admin',\n",
      "              'dashboard',\n",
      "              'spectator',\n",
      "              'score',\n",
      "              'update',\n",
      "              'delay',\n",
      "              'tournament',\n",
      "              'oversight']}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "print('✅ Extracted Metadata:')\n",
    "pprint(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "278cad73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Simple Keyword Matching ==\n",
      "Intent: resume_info, Confidence: 0.29\n",
      "\n",
      "== Embedding Similarity ==\n",
      "Intent: resume_info, Confidence: 0.29\n",
      "\n",
      "== Embedding Similarity ==\n",
      "Intent: resume_info, Similarity: 0.41\n",
      "\n",
      "== Regex-based Detection ==\n",
      "Intent: resume_info, Confidence: 1.00\n",
      "\n",
      "== Majority Voting ==\n",
      "Intent: resume_info, Confidence: 1.00\n",
      "\n",
      "== Bag-of-Words Cosine Similarity ==\n",
      "Intent: resume_info, Similarity: 0.32\n",
      "Intent: resume_info, Similarity: 0.41\n",
      "\n",
      "== Regex-based Detection ==\n",
      "Intent: resume_info, Confidence: 1.00\n",
      "\n",
      "== Majority Voting ==\n",
      "Intent: resume_info, Confidence: 1.00\n",
      "\n",
      "== Bag-of-Words Cosine Similarity ==\n",
      "Intent: resume_info, Similarity: 0.32\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Test alternative intent detection techniques\n",
    "\n",
    "# 1. Simple keyword matching (baseline)\n",
    "def detect_intent_keywords(text, intent_keywords):\n",
    "    doc = nlp(text.lower())\n",
    "    tokens = [token.lemma_ for token in doc]\n",
    "    scores = {}\n",
    "    for intent, keywords in intent_keywords.items():\n",
    "        matches = sum(kw in tokens for kw in keywords)\n",
    "        scores[intent] = matches\n",
    "    best_intent = max(scores, key=scores.get)\n",
    "    confidence = scores[best_intent] / max(1, len(intent_keywords[best_intent]))\n",
    "    return best_intent, confidence\n",
    "\n",
    "# 2. Embedding similarity (SentenceTransformer, already used in get_intent)\n",
    "def detect_intent_embedding(text, intent_examples, intent_model):\n",
    "    query_emb = intent_model.encode(text, convert_to_tensor=True)\n",
    "    best_intent, best_score = None, 0\n",
    "    for intent, examples in intent_examples.items():\n",
    "        example_embs = intent_model.encode(examples, convert_to_tensor=True)\n",
    "        scores = util.pytorch_cos_sim(query_emb, example_embs)\n",
    "        max_score = scores.max().item()\n",
    "        if max_score > best_score:\n",
    "            best_score = max_score\n",
    "            best_intent = intent\n",
    "    return best_intent, best_score\n",
    "\n",
    "# 3. Regex-based intent detection (very basic)\n",
    "def detect_intent_regex(text):\n",
    "    patterns = {\n",
    "        \"resume_info\": r\"\\b(resume|cv|skills|experience|project|education|certification)\\b\",\n",
    "        \"claim_process\": r\"\\b(claim|insurance|submit|file)\\b\",\n",
    "        \"case_status\": r\"\\b(status|case|update|progress|judgement|order|appeal)\\b\",\n",
    "        \"document_request\": r\"\\b(document|request|copy|copies|form)\\b\",\n",
    "        \"technical_support\": r\"\\b(error|issue|problem|technical)\\b\",\n",
    "        \"general_info\": r\"\\b(information|contact|hours|location|summary|purpose)\\b\"\n",
    "    }\n",
    "    for intent, pattern in patterns.items():\n",
    "        if re.search(pattern, text, re.IGNORECASE):\n",
    "            return intent, 1.0\n",
    "    return \"general_info\", 0.0\n",
    "\n",
    "# Run all techniques on the loaded text\n",
    "print(\"== Simple Keyword Matching ==\")\n",
    "intent_kw, conf_kw = detect_intent_keywords(text, intent_keywords)\n",
    "print(f\"Intent: {intent_kw}, Confidence: {conf_kw:.2f}\")\n",
    "\n",
    "print(\"\\n== Embedding Similarity ==\")\n",
    "intent_emb, conf_emb = detect_intent_embedding(text, intent_examples, intent_model)\n",
    "print(f\"Intent: {intent_emb}, Similarity: {conf_emb:.2f}\")\n",
    "\n",
    "print(\"\\n== Regex-based Detection ==\")\n",
    "intent_rgx, conf_rgx = detect_intent_regex(text)\n",
    "print(f\"Intent: {intent_rgx}, Confidence: {conf_rgx:.2f}\")\n",
    "# 4. Majority voting among techniques\n",
    "\n",
    "def majority_vote(*intents):\n",
    "    count = Counter(intents)\n",
    "    best, freq = count.most_common(1)[0]\n",
    "    return best, freq / len(intents)\n",
    "\n",
    "intents = [intent_kw, intent_emb, intent_rgx]\n",
    "majority_intent, majority_conf = majority_vote(*intents)\n",
    "print(\"\\n== Majority Voting ==\")\n",
    "print(f\"Intent: {majority_intent}, Confidence: {majority_conf:.2f}\")\n",
    "\n",
    "# 5. Bag-of-words cosine similarity (very basic)\n",
    "\n",
    "def detect_intent_bow(text, intent_examples):\n",
    "    vectorizer = CountVectorizer().fit([text] + [ex for exs in intent_examples.values() for ex in exs])\n",
    "    text_vec = vectorizer.transform([text])\n",
    "    best_intent, best_score = None, 0\n",
    "    for intent, examples in intent_examples.items():\n",
    "        ex_vecs = vectorizer.transform(examples)\n",
    "        sim = cosine_similarity(text_vec, ex_vecs).max()\n",
    "        if sim > best_score:\n",
    "            best_score = sim\n",
    "            best_intent = intent\n",
    "    return best_intent, best_score\n",
    "\n",
    "intent_bow, conf_bow = detect_intent_bow(text, intent_examples)\n",
    "print(\"\\n== Bag-of-Words Cosine Similarity ==\")\n",
    "print(f\"Intent: {intent_bow}, Similarity: {conf_bow:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70297b57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9ad41c6",
   "metadata": {},
   "source": [
    "## using both regex and the Transformers NER pipeline, along with KeyBERT for keyword extraction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87e3eeab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# --- Alternative Metadata Extraction: Regex + Transformers NER ---\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\n",
    "from keybert import KeyBERT\n",
    "import re\n",
    "\n",
    "# Load NER pipeline (Roberta large NER)\n",
    "ner_pipe = pipeline(\n",
    "    \"ner\",\n",
    "    model=AutoModelForTokenClassification.from_pretrained(\"Jean-Baptiste/roberta-large-ner-english\"),\n",
    "    tokenizer=AutoTokenizer.from_pretrained(\"Jean-Baptiste/roberta-large-ner-english\"),\n",
    "    aggregation_strategy=\"simple\",\n",
    "    device=-1  # CPU\n",
    ")\n",
    "\n",
    "# KeyBERT for keyword extraction\n",
    "keyword_model = KeyBERT()\n",
    "\n",
    "def extract_metadata_alt(text):\n",
    "    # Entities using transformers NER\n",
    "    entities = set()\n",
    "    for ent in ner_pipe(text):\n",
    "        if ent['score'] > 0.8:\n",
    "            entities.add(ent['word'].strip().lower())\n",
    "    # Regex for names (simple)\n",
    "    name_matches = re.findall(r'([A-Z][a-z]+(?: [A-Z][a-z]+)+)', text)\n",
    "    for name in name_matches:\n",
    "        entities.add(name.lower())\n",
    "    # Keywords using KeyBERT\n",
    "    keywords = [kw for kw, _ in keyword_model.extract_keywords(text, top_n=10)]\n",
    "    # Intent using previous get_intent\n",
    "    detected_intent, intent_confidence, _ = get_intent(text)\n",
    "    return {\n",
    "        \"entities\": sorted(list(entities)),\n",
    "        \"keywords\": keywords,\n",
    "        \"intent\": detected_intent,\n",
    "        \"intent_confidence\": intent_confidence\n",
    "    }\n",
    "\n",
    "# alt_metadata = extract_metadata_alt(text)\n",
    "# print('✅ Alternative Metadata Extraction:')\n",
    "# from pprint import pprint\n",
    "# pprint(alt_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac0ca12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c87473e",
   "metadata": {},
   "source": [
    "## 🚀 Production-Grade Intent Detection Solutions\n",
    "\n",
    "For real-world, scalable, and robust intent detection, consider these best practices:\n",
    "\n",
    "- **Hybrid Pipeline:** Combine fast rule-based/keyword/regex checks for high-precision intents with embedding-based similarity for flexible, robust matching.\n",
    "- **ML/Deep Learning Models:** Use fine-tuned transformer models (e.g., BERT, RoBERTa, DistilBERT) for intent classification if you have enough labeled data.\n",
    "- **Fallbacks:** Always provide a fallback (e.g., \"general_info\") for ambiguous or low-confidence cases.\n",
    "- **Confidence Thresholds:** Use thresholds to decide when to trust a prediction or escalate to a human/manual review.\n",
    "- **Monitoring & Logging:** Log predictions, confidence, and input for continuous improvement and error analysis.\n",
    "- **Versioning:** Version your models, configs, and intent definitions for reproducibility and safe updates.\n",
    "- **Batch & Real-Time Support:** Design your pipeline to work both in batch (offline) and real-time (API) modes.\n",
    "\n",
    "Below is a modular, production-ready intent detection pipeline you can adapt and extend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e8fca9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'intent_keywords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 52\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     47\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mintent\u001b[39m\u001b[38;5;124m\"\u001b[39m: detected_intent,\n\u001b[0;32m     48\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mintent_confidence\u001b[39m\u001b[38;5;124m\"\u001b[39m: intent_confidence\n\u001b[0;32m     49\u001b[0m         }\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Usage example\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m prod_detector \u001b[38;5;241m=\u001b[39m ProductionIntentDetector(\u001b[43mintent_keywords\u001b[49m, intent_examples, intent_model, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.35\u001b[39m)\n\u001b[0;32m     53\u001b[0m chunk_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(project_root, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunks\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     54\u001b[0m chunk_files \u001b[38;5;241m=\u001b[39m glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(chunk_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*.txt\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'intent_keywords' is not defined"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from typing import Dict, Any\n",
    "\n",
    "# Configure logging for production\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "class ProductionIntentDetector:\n",
    "    def __init__(self, intent_keywords, intent_examples, intent_model, threshold=0.35):\n",
    "        self.intent_keywords = intent_keywords\n",
    "        self.intent_examples = intent_examples\n",
    "        self.intent_model = intent_model\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def detect(self, text: str) -> Dict[str, Any]:\n",
    "        # 1. Fast rule-based keyword/lemmatized match\n",
    "        doc = nlp(text.lower())\n",
    "        tokens = [token.lemma_ for token in doc]\n",
    "        detected_intent = None\n",
    "        max_matches = 0\n",
    "        for intent, keywords in self.intent_keywords.items():\n",
    "            matches = sum(kw in tokens for kw in keywords)\n",
    "            if matches > max_matches:\n",
    "                max_matches = matches\n",
    "                detected_intent = intent\n",
    "        intent_confidence = max_matches / max(1, len(self.intent_keywords.get(detected_intent, []))) if detected_intent else 0.0\n",
    "        # 2. Embedding similarity fallback\n",
    "        if not detected_intent or max_matches == 0:\n",
    "            query_emb = self.intent_model.encode(text, convert_to_tensor=True)\n",
    "            best_intent, best_score = None, 0\n",
    "            for intent, examples in self.intent_examples.items():\n",
    "                example_embs = self.intent_model.encode(examples, convert_to_tensor=True)\n",
    "                scores = util.pytorch_cos_sim(query_emb, example_embs)\n",
    "                max_score = scores.max().item()\n",
    "                if max_score > best_score:\n",
    "                    best_score = max_score\n",
    "                    best_intent = intent\n",
    "            if best_score > self.threshold:\n",
    "                detected_intent = best_intent\n",
    "                intent_confidence = best_score\n",
    "        # 3. Fallback to general_info\n",
    "        if not detected_intent:\n",
    "            detected_intent = \"general_info\"\n",
    "            intent_confidence = 0.0\n",
    "        # 4. Logging for monitoring\n",
    "        logging.info(f\"Intent: {detected_intent}, Confidence: {intent_confidence:.2f}, Text: {text[:80]}...\")\n",
    "        return {\n",
    "            \"intent\": detected_intent,\n",
    "            \"intent_confidence\": intent_confidence\n",
    "        }\n",
    "\n",
    "# Usage example\n",
    "prod_detector = ProductionIntentDetector(intent_keywords, intent_examples, intent_model, threshold=0.35)\n",
    "chunk_dir = os.path.join(project_root, '..', 'data', 'chunks')\n",
    "chunk_files = glob.glob(os.path.join(chunk_dir, '*.txt'))\n",
    "\n",
    "chunk_intents = {}\n",
    "for chunk_path in chunk_files:\n",
    "    with open(chunk_path, 'r', encoding='utf-8') as f:\n",
    "        chunk_text = f.read()\n",
    "    result = prod_detector.detect(chunk_text)\n",
    "    chunk_intents[os.path.basename(chunk_path)] = result\n",
    "\n",
    "print('✅ Intent detection for all chunks:')\n",
    "for fname, res in chunk_intents.items():\n",
    "    print(f\"{fname}: {res}\")\n",
    "# prod_result = prod_detector.detect(text)\n",
    "# print('✅ Production-Grade Intent Detection:')\n",
    "# print(prod_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d31c4abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== spaCy + SentenceTransformer Pipeline ==\n",
      "{'entities': ['devzen',\n",
      "              'spring boot spring security',\n",
      "              'spring batch',\n",
      "              'elasticsearch',\n",
      "              'angular typescript',\n",
      "              'spring boot angular',\n",
      "              'aw ec2 s3 rd iam',\n",
      "              'spring boot',\n",
      "              'angular',\n",
      "              '25',\n",
      "              '50',\n",
      "              'portal',\n",
      "              '20',\n",
      "              'six month of launch',\n",
      "              '40',\n",
      "              '90'],\n",
      " 'intent': 'resume_info',\n",
      " 'intent_confidence': 0.28846153846153844,\n",
      " 'keywords': ['java',\n",
      "              'stack',\n",
      "              'developer',\n",
      "              'devzen',\n",
      "              'software',\n",
      "              'solution',\n",
      "              'jwt',\n",
      "              'authentication',\n",
      "              'verification',\n",
      "              'apis',\n",
      "              'spring',\n",
      "              'boot',\n",
      "              'spring',\n",
      "              'security',\n",
      "              'batch',\n",
      "              'processing',\n",
      "              'workflow',\n",
      "              'spring',\n",
      "              'batch',\n",
      "              'scale',\n",
      "              'datum',\n",
      "              'management',\n",
      "              'rest',\n",
      "              'apis',\n",
      "              'time',\n",
      "              'inventory',\n",
      "              'tracking',\n",
      "              'alert',\n",
      "              'stock',\n",
      "              'management',\n",
      "              'elasticsearch',\n",
      "              'search',\n",
      "              'retrieval',\n",
      "              'volume',\n",
      "              'application',\n",
      "              'component',\n",
      "              'angular',\n",
      "              'typescript',\n",
      "              'product',\n",
      "              'management',\n",
      "              'payment',\n",
      "              'gateway',\n",
      "              'integration',\n",
      "              'spring',\n",
      "              'boot',\n",
      "              'angular',\n",
      "              'transaction',\n",
      "              'dashboard',\n",
      "              'key',\n",
      "              'insight',\n",
      "              'tournament',\n",
      "              'inventory',\n",
      "              'management',\n",
      "              'application',\n",
      "              'docker',\n",
      "              'consistency',\n",
      "              'development',\n",
      "              'production',\n",
      "              'environment',\n",
      "              'cicd',\n",
      "              'pipeline',\n",
      "              'build',\n",
      "              'test',\n",
      "              'deployment',\n",
      "              'process',\n",
      "              'github',\n",
      "              'action',\n",
      "              'workflow',\n",
      "              'deployment',\n",
      "              'efficiency',\n",
      "              'application',\n",
      "              'aws',\n",
      "              'ec2',\n",
      "              'rds',\n",
      "              'iam',\n",
      "              'cloud',\n",
      "              'hosting',\n",
      "              'management',\n",
      "              'apis',\n",
      "              'spring',\n",
      "              'boot',\n",
      "              'document',\n",
      "              'management',\n",
      "              'user',\n",
      "              'authentication',\n",
      "              'security',\n",
      "              'efficiency',\n",
      "              'angular',\n",
      "              'user',\n",
      "              'experience',\n",
      "              'document',\n",
      "              'creation',\n",
      "              'socialization',\n",
      "              'publication',\n",
      "              'key',\n",
      "              'achievement',\n",
      "              'security',\n",
      "              'user',\n",
      "              'interaction',\n",
      "              'angular',\n",
      "              'api',\n",
      "              'user',\n",
      "              'authentication',\n",
      "              'verification',\n",
      "              'process',\n",
      "              'document',\n",
      "              'management',\n",
      "              'search',\n",
      "              'security',\n",
      "              'access',\n",
      "              'functionality',\n",
      "              'improvement',\n",
      "              'user',\n",
      "              'incident',\n",
      "              'satisfaction',\n",
      "              'score',\n",
      "              'document',\n",
      "              'management',\n",
      "              'improve',\n",
      "              'tournament',\n",
      "              'efficiency',\n",
      "              'feature',\n",
      "              'document',\n",
      "              'creation',\n",
      "              'end',\n",
      "              'end',\n",
      "              'platform',\n",
      "              'socialization',\n",
      "              'publication',\n",
      "              'portal',\n",
      "              'user',\n",
      "              'player',\n",
      "              'registration',\n",
      "              'tournament',\n",
      "              'scheduling',\n",
      "              'base',\n",
      "              'month',\n",
      "              'launch',\n",
      "              'administration',\n",
      "              'time',\n",
      "              'tournament',\n",
      "              'management',\n",
      "              'process',\n",
      "              'admin',\n",
      "              'control',\n",
      "              'time',\n",
      "              'scoring',\n",
      "              'score',\n",
      "              'update',\n",
      "              'player',\n",
      "              'admin',\n",
      "              'dashboard',\n",
      "              'spectator',\n",
      "              'score',\n",
      "              'update',\n",
      "              'delay',\n",
      "              'tournament',\n",
      "              'oversight']}\n",
      "\n",
      "== Transformers NER + KeyBERT Pipeline ==\n",
      "{'entities': ['devzen',\n",
      "              'spring boot spring security',\n",
      "              'spring batch',\n",
      "              'elasticsearch',\n",
      "              'angular typescript',\n",
      "              'spring boot angular',\n",
      "              'aw ec2 s3 rd iam',\n",
      "              'spring boot',\n",
      "              'angular',\n",
      "              '25',\n",
      "              '50',\n",
      "              'portal',\n",
      "              '20',\n",
      "              'six month of launch',\n",
      "              '40',\n",
      "              '90'],\n",
      " 'intent': 'resume_info',\n",
      " 'intent_confidence': 0.28846153846153844,\n",
      " 'keywords': ['java',\n",
      "              'stack',\n",
      "              'developer',\n",
      "              'devzen',\n",
      "              'software',\n",
      "              'solution',\n",
      "              'jwt',\n",
      "              'authentication',\n",
      "              'verification',\n",
      "              'apis',\n",
      "              'spring',\n",
      "              'boot',\n",
      "              'spring',\n",
      "              'security',\n",
      "              'batch',\n",
      "              'processing',\n",
      "              'workflow',\n",
      "              'spring',\n",
      "              'batch',\n",
      "              'scale',\n",
      "              'datum',\n",
      "              'management',\n",
      "              'rest',\n",
      "              'apis',\n",
      "              'time',\n",
      "              'inventory',\n",
      "              'tracking',\n",
      "              'alert',\n",
      "              'stock',\n",
      "              'management',\n",
      "              'elasticsearch',\n",
      "              'search',\n",
      "              'retrieval',\n",
      "              'volume',\n",
      "              'application',\n",
      "              'component',\n",
      "              'angular',\n",
      "              'typescript',\n",
      "              'product',\n",
      "              'management',\n",
      "              'payment',\n",
      "              'gateway',\n",
      "              'integration',\n",
      "              'spring',\n",
      "              'boot',\n",
      "              'angular',\n",
      "              'transaction',\n",
      "              'dashboard',\n",
      "              'key',\n",
      "              'insight',\n",
      "              'tournament',\n",
      "              'inventory',\n",
      "              'management',\n",
      "              'application',\n",
      "              'docker',\n",
      "              'consistency',\n",
      "              'development',\n",
      "              'production',\n",
      "              'environment',\n",
      "              'cicd',\n",
      "              'pipeline',\n",
      "              'build',\n",
      "              'test',\n",
      "              'deployment',\n",
      "              'process',\n",
      "              'github',\n",
      "              'action',\n",
      "              'workflow',\n",
      "              'deployment',\n",
      "              'efficiency',\n",
      "              'application',\n",
      "              'aws',\n",
      "              'ec2',\n",
      "              'rds',\n",
      "              'iam',\n",
      "              'cloud',\n",
      "              'hosting',\n",
      "              'management',\n",
      "              'apis',\n",
      "              'spring',\n",
      "              'boot',\n",
      "              'document',\n",
      "              'management',\n",
      "              'user',\n",
      "              'authentication',\n",
      "              'security',\n",
      "              'efficiency',\n",
      "              'angular',\n",
      "              'user',\n",
      "              'experience',\n",
      "              'document',\n",
      "              'creation',\n",
      "              'socialization',\n",
      "              'publication',\n",
      "              'key',\n",
      "              'achievement',\n",
      "              'security',\n",
      "              'user',\n",
      "              'interaction',\n",
      "              'angular',\n",
      "              'api',\n",
      "              'user',\n",
      "              'authentication',\n",
      "              'verification',\n",
      "              'process',\n",
      "              'document',\n",
      "              'management',\n",
      "              'search',\n",
      "              'security',\n",
      "              'access',\n",
      "              'functionality',\n",
      "              'improvement',\n",
      "              'user',\n",
      "              'incident',\n",
      "              'satisfaction',\n",
      "              'score',\n",
      "              'document',\n",
      "              'management',\n",
      "              'improve',\n",
      "              'tournament',\n",
      "              'efficiency',\n",
      "              'feature',\n",
      "              'document',\n",
      "              'creation',\n",
      "              'end',\n",
      "              'end',\n",
      "              'platform',\n",
      "              'socialization',\n",
      "              'publication',\n",
      "              'portal',\n",
      "              'user',\n",
      "              'player',\n",
      "              'registration',\n",
      "              'tournament',\n",
      "              'scheduling',\n",
      "              'base',\n",
      "              'month',\n",
      "              'launch',\n",
      "              'administration',\n",
      "              'time',\n",
      "              'tournament',\n",
      "              'management',\n",
      "              'process',\n",
      "              'admin',\n",
      "              'control',\n",
      "              'time',\n",
      "              'scoring',\n",
      "              'score',\n",
      "              'update',\n",
      "              'player',\n",
      "              'admin',\n",
      "              'dashboard',\n",
      "              'spectator',\n",
      "              'score',\n",
      "              'update',\n",
      "              'delay',\n",
      "              'tournament',\n",
      "              'oversight']}\n",
      "\n",
      "== Transformers NER + KeyBERT Pipeline ==\n",
      "{'entities': ['angular',\n",
      "              'angular type',\n",
      "              'angular typescript',\n",
      "              'aws ec2 s3 rds iam',\n",
      "              'devzen software solutions',\n",
      "              'docker',\n",
      "              'elasticsearch',\n",
      "              'enhanced document management improved tournament efficiency '\n",
      "              'implemented',\n",
      "              'github actions',\n",
      "              'hub actions',\n",
      "              'integrated elasticsearch',\n",
      "              'java full stack developer',\n",
      "              'java full stack developer 20',\n",
      "              'jwt-based',\n",
      "              'present devzen software solutions developed',\n",
      "              'robust admin control real',\n",
      "              'spring batch',\n",
      "              'spring boot',\n",
      "              'spring boot angular',\n",
      "              'spring boot spring security',\n",
      "              'spring boot spring security built',\n",
      "              'strengthened system security improved user interaction '\n",
      "              'leveraged angular',\n",
      "              'time scoring implemented',\n",
      "              'utilized git'],\n",
      " 'intent': 'resume_info',\n",
      " 'intent_confidence': 0.28846153846153844,\n",
      " 'keywords': ['apis',\n",
      "              'authentication',\n",
      "              'api',\n",
      "              'aws',\n",
      "              'jwt',\n",
      "              'cloud',\n",
      "              'workflows',\n",
      "              'angular',\n",
      "              'secure',\n",
      "              'java']}\n",
      "\n",
      "== Comparison ==\n",
      "Entities overlap: {'angular typescript', 'spring boot angular', 'spring boot', 'angular', 'elasticsearch', 'spring batch', 'spring boot spring security'}\n",
      "Keywords overlap: {'angular', 'apis', 'java', 'aws', 'cloud', 'authentication', 'jwt', 'api'}\n",
      "Intent (spaCy pipeline): resume_info | Confidence: 0.28846153846153844\n",
      "Intent (Transformers+KeyBERT): resume_info | Confidence: 0.28846153846153844\n",
      "{'entities': ['angular',\n",
      "              'angular type',\n",
      "              'angular typescript',\n",
      "              'aws ec2 s3 rds iam',\n",
      "              'devzen software solutions',\n",
      "              'docker',\n",
      "              'elasticsearch',\n",
      "              'enhanced document management improved tournament efficiency '\n",
      "              'implemented',\n",
      "              'github actions',\n",
      "              'hub actions',\n",
      "              'integrated elasticsearch',\n",
      "              'java full stack developer',\n",
      "              'java full stack developer 20',\n",
      "              'jwt-based',\n",
      "              'present devzen software solutions developed',\n",
      "              'robust admin control real',\n",
      "              'spring batch',\n",
      "              'spring boot',\n",
      "              'spring boot angular',\n",
      "              'spring boot spring security',\n",
      "              'spring boot spring security built',\n",
      "              'strengthened system security improved user interaction '\n",
      "              'leveraged angular',\n",
      "              'time scoring implemented',\n",
      "              'utilized git'],\n",
      " 'intent': 'resume_info',\n",
      " 'intent_confidence': 0.28846153846153844,\n",
      " 'keywords': ['apis',\n",
      "              'authentication',\n",
      "              'api',\n",
      "              'aws',\n",
      "              'jwt',\n",
      "              'cloud',\n",
      "              'workflows',\n",
      "              'angular',\n",
      "              'secure',\n",
      "              'java']}\n",
      "\n",
      "== Comparison ==\n",
      "Entities overlap: {'angular typescript', 'spring boot angular', 'spring boot', 'angular', 'elasticsearch', 'spring batch', 'spring boot spring security'}\n",
      "Keywords overlap: {'angular', 'apis', 'java', 'aws', 'cloud', 'authentication', 'jwt', 'api'}\n",
      "Intent (spaCy pipeline): resume_info | Confidence: 0.28846153846153844\n",
      "Intent (Transformers+KeyBERT): resume_info | Confidence: 0.28846153846153844\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Test and compare all metadata extraction techniques side by side\n",
    "\n",
    "print(\"== spaCy + SentenceTransformer Pipeline ==\")\n",
    "spacy_metadata = extract_metadata(text)\n",
    "pprint(spacy_metadata)\n",
    "\n",
    "print(\"\\n== Transformers NER + KeyBERT Pipeline ==\")\n",
    "alt_metadata = extract_metadata_alt(text)\n",
    "pprint(alt_metadata)\n",
    "\n",
    "print(\"\\n== Comparison ==\")\n",
    "print(\"Entities overlap:\", set(spacy_metadata['entities']) & set(alt_metadata['entities']))\n",
    "print(\"Keywords overlap:\", set(spacy_metadata['keywords']) & set(alt_metadata['keywords']))\n",
    "print(\"Intent (spaCy pipeline):\", spacy_metadata['intent'], \"| Confidence:\", spacy_metadata['intent_confidence'])\n",
    "print(\"Intent (Transformers+KeyBERT):\", alt_metadata['intent'], \"| Confidence:\", alt_metadata['intent_confidence'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "785763ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Zero-Shot Classification ==\n",
      "Intent: case_status, Confidence: 0.18\n",
      "\n",
      "== Ensemble Intent Detection ==\n",
      "Intent: resume_info, Confidence: 0.75\n",
      "All results: [('resume_info', 0.28846153846153844), ('resume_info', 0.4149797260761261), ('resume_info', 1.0), ('case_status', 0.17770253121852875)]\n",
      "\n",
      "== Ensemble Intent Detection ==\n",
      "Intent: resume_info, Confidence: 0.75\n",
      "All results: [('resume_info', 0.28846153846153844), ('resume_info', 0.4149797260761261), ('resume_info', 1.0), ('case_status', 0.17770253121852875)]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from collections import Counter\n",
    "\n",
    "# Advanced Intent Detection Techniques\n",
    "\n",
    "# 1. Zero-shot classification with HuggingFace Transformers (e.g., facebook/bart-large-mnli)\n",
    "\n",
    "zero_shot_classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "def detect_intent_zero_shot(text, candidate_labels):\n",
    "    result = zero_shot_classifier(text, candidate_labels)\n",
    "    best_intent = result['labels'][0]\n",
    "    confidence = result['scores'][0]\n",
    "    return best_intent, confidence\n",
    "\n",
    "candidate_labels = list(intent_examples.keys())\n",
    "intent_zs, conf_zs = detect_intent_zero_shot(text, candidate_labels)\n",
    "print(\"== Zero-Shot Classification ==\")\n",
    "print(f\"Intent: {intent_zs}, Confidence: {conf_zs:.2f}\")\n",
    "\n",
    "# 2. Fine-tuned intent classification model (if you have labeled data)\n",
    "# (Placeholder: You would train a classifier using your labeled dataset, e.g., using sklearn, Keras, or HuggingFace Trainer.)\n",
    "\n",
    "# 3. Ensemble: Combine multiple techniques (majority vote, weighted average, etc.)\n",
    "# Ensure detect_intent_keywords is defined (use from cell 7 if available)\n",
    "if 'detect_intent_keywords' not in globals():\n",
    "    def detect_intent_keywords(text, intent_keywords):\n",
    "        doc = nlp(text.lower())\n",
    "        tokens = [token.lemma_ for token in doc]\n",
    "        scores = {}\n",
    "        for intent, keywords in intent_keywords.items():\n",
    "            matches = sum(kw in tokens for kw in keywords)\n",
    "            scores[intent] = matches\n",
    "        best_intent = max(scores, key=scores.get)\n",
    "        confidence = scores[best_intent] / max(1, len(intent_keywords[best_intent]))\n",
    "        return best_intent, confidence\n",
    "\n",
    "def ensemble_intent_detection(text):\n",
    "    results = []\n",
    "    # Keyword\n",
    "    kw_intent, kw_conf = detect_intent_keywords(text, intent_keywords)\n",
    "    results.append((kw_intent, kw_conf))\n",
    "    # Embedding\n",
    "    emb_intent, emb_conf = detect_intent_embedding(text, intent_examples, intent_model)\n",
    "    results.append((emb_intent, emb_conf))\n",
    "    # Regex\n",
    "    rgx_intent, rgx_conf = detect_intent_regex(text)\n",
    "    results.append((rgx_intent, rgx_conf))\n",
    "    # Zero-shot\n",
    "    zs_intent, zs_conf = detect_intent_zero_shot(text, candidate_labels)\n",
    "    results.append((zs_intent, zs_conf))\n",
    "    # Majority vote\n",
    "    intents = [intent for intent, _ in results]\n",
    "    best, freq = Counter(intents).most_common(1)[0]\n",
    "    return best, freq / len(results), results\n",
    "\n",
    "ensemble_intent, ensemble_conf, all_results = ensemble_intent_detection(text)\n",
    "print(\"\\n== Ensemble Intent Detection ==\")\n",
    "print(f\"Intent: {ensemble_intent}, Confidence: {ensemble_conf:.2f}\")\n",
    "print(\"All results:\", all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6e53fb",
   "metadata": {},
   "source": [
    "## Fine-Tuned Transformer Classifier for Intent Detection\n",
    "\n",
    "This section demonstrates how to train and use a transformer (DistilBERT) for intent classification using Hugging Face Transformers. You need a labeled dataset (text, intent) for this. The example below uses realistic intent examples from your domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15517ba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How do I file a claim?</td>\n",
       "      <td>claim_process</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the process for submitting an insuranc...</td>\n",
       "      <td>claim_process</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I want to submit a new claim for my car accident.</td>\n",
       "      <td>claim_process</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Guide me through the claim submission steps.</td>\n",
       "      <td>claim_process</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Where do I upload my claim documents?</td>\n",
       "      <td>claim_process</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>by exercising the power of discretion by the C...</td>\n",
       "      <td>claim_process</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>for the revision petitioner has relied on the ...</td>\n",
       "      <td>claim_process</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>Once it is held that a party has lost his righ...</td>\n",
       "      <td>case_status</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>that the party who failed to approach the Cour...</td>\n",
       "      <td>case_status</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>2001 6 Supreme Court Cases 176 which was relie...</td>\n",
       "      <td>case_status</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text          label\n",
       "0                               How do I file a claim?  claim_process\n",
       "1    What is the process for submitting an insuranc...  claim_process\n",
       "2    I want to submit a new claim for my car accident.  claim_process\n",
       "3         Guide me through the claim submission steps.  claim_process\n",
       "4                Where do I upload my claim documents?  claim_process\n",
       "..                                                 ...            ...\n",
       "127  by exercising the power of discretion by the C...  claim_process\n",
       "128  for the revision petitioner has relied on the ...  claim_process\n",
       "129  Once it is held that a party has lost his righ...    case_status\n",
       "130  that the party who failed to approach the Cour...    case_status\n",
       "131  2001 6 Supreme Court Cases 176 which was relie...    case_status\n",
       "\n",
       "[132 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example labeled data for intent fine-tuning (expand with more real samples for best results)\n",
    "data = [\n",
    "    {\"text\": \"How do I file a claim?\", \"label\": \"claim_process\"},\n",
    "    {\"text\": \"What is the process for submitting an insurance claim?\", \"label\": \"claim_process\"},\n",
    "    {\"text\": \"What is the current status of the case?\", \"label\": \"case_status\"},\n",
    "    {\"text\": \"Show me the progress of case number 511605.\", \"label\": \"case_status\"},\n",
    "    {\"text\": \"Can I get a copy of the case order?\", \"label\": \"document_request\"},\n",
    "    {\"text\": \"How do I request the judgment document?\", \"label\": \"document_request\"},\n",
    "    {\"text\": \"What skills are listed in the resume?\", \"label\": \"resume_info\"},\n",
    "    {\"text\": \"List the programming languages known by the applicant.\", \"label\": \"resume_info\"},\n",
    "    {\"text\": \"Who is the presiding judge for this case?\", \"label\": \"court_details\"},\n",
    "    {\"text\": \"Who are the parties involved in this case?\", \"label\": \"party_information\"},\n",
    "    {\"text\": \"When was the last hearing held?\", \"label\": \"hearing_information\"},\n",
    "    {\"text\": \"I have a technical issue with the system.\", \"label\": \"technical_support\"},\n",
    "    {\"text\": \"Give me a summary of the file.\", \"label\": \"general_info\"},\n",
    "    {\"text\": \"Tell me about this document.\", \"label\": \"general_info\"},\n",
    "]\n",
    "df = pd.DataFrame(expanded_data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9d14a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 132/132 [00:00<00:00, 4163.93 examples/s]\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\91807\\AppData\\Local\\Temp\\ipykernel_18352\\3636785226.py:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='81' max='81' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [81/81 01:39, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.455200</td>\n",
       "      <td>1.407624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.119300</td>\n",
       "      <td>1.184576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.842100</td>\n",
       "      <td>1.077852</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=81, training_loss=1.180923052776007, metrics={'train_runtime': 101.8366, 'train_samples_per_second': 3.093, 'train_steps_per_second': 0.795, 'total_flos': 5216275895040.0, 'train_loss': 1.180923052776007, 'epoch': 3.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# 1. Prepare label mappings\n",
    "df['label_id'] = df['label'].astype('category').cat.codes\n",
    "label2id = {label: i for i, label in enumerate(df['label'].astype('category').cat.categories)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "# 2. Convert to Hugging Face Dataset\n",
    "# Rename 'label_id' to 'labels' for Trainer compatibility\n",
    "df_for_hf = df.rename(columns={'label_id': 'labels'})\n",
    "# Remove the 'label' column (string) to avoid Trainer confusion\n",
    "df_for_hf = df_for_hf.drop(columns=['label'])\n",
    "dataset = Dataset.from_pandas(df_for_hf)\n",
    "\n",
    "# 3. Tokenize\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "def preprocess(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=64)\n",
    "dataset = dataset.map(preprocess, batched=True)\n",
    "\n",
    "# 4. Train/Test split\n",
    "split = dataset.train_test_split(test_size=0.2)\n",
    "train_ds, test_ds = split[\"train\"], split[\"test\"]\n",
    "\n",
    "# 5. Model and Trainer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, num_labels=len(label2id), id2label=id2label, label2id=label2id)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./intent_model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# 6. Train (this will take a few minutes on CPU, much faster on GPU)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55357a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jyothika_Java_Fullstack_chunk1.txt: ('resume_info', 0.4410419464111328)\n",
      "Jyothika_Java_Fullstack_chunk2.txt: ('resume_info', 0.47217509150505066)\n",
      "MHC_CaseStatus_511605_chunk1.txt: ('case_status', 0.7151972055435181)\n",
      "MHC_CaseStatus_511605_chunk10.txt: ('claim_process', 0.430624783039093)\n",
      "MHC_CaseStatus_511605_chunk11.txt: ('case_status', 0.5998450517654419)\n",
      "MHC_CaseStatus_511605_chunk12.txt: ('case_status', 0.6954925060272217)\n",
      "MHC_CaseStatus_511605_chunk13.txt: ('case_status', 0.6783062815666199)\n",
      "MHC_CaseStatus_511605_chunk14.txt: ('claim_process', 0.5124401450157166)\n",
      "MHC_CaseStatus_511605_chunk15.txt: ('case_status', 0.6326937079429626)\n",
      "MHC_CaseStatus_511605_chunk16.txt: ('claim_process', 0.4597802460193634)\n",
      "MHC_CaseStatus_511605_chunk17.txt: ('claim_process', 0.3984483480453491)\n",
      "MHC_CaseStatus_511605_chunk18.txt: ('claim_process', 0.4138122797012329)\n",
      "MHC_CaseStatus_511605_chunk19.txt: ('case_status', 0.750511646270752)\n",
      "MHC_CaseStatus_511605_chunk2.txt: ('case_status', 0.6310802698135376)\n",
      "MHC_CaseStatus_511605_chunk20.txt: ('case_status', 0.7495293617248535)\n",
      "MHC_CaseStatus_511605_chunk21.txt: ('case_status', 0.7338912487030029)\n",
      "MHC_CaseStatus_511605_chunk22.txt: ('case_status', 0.7358413934707642)\n",
      "MHC_CaseStatus_511605_chunk23.txt: ('case_status', 0.742721438407898)\n",
      "MHC_CaseStatus_511605_chunk24.txt: ('case_status', 0.70554518699646)\n",
      "MHC_CaseStatus_511605_chunk25.txt: ('case_status', 0.707427442073822)\n",
      "MHC_CaseStatus_511605_chunk26.txt: ('case_status', 0.7467585802078247)\n",
      "MHC_CaseStatus_511605_chunk27.txt: ('claim_process', 0.42228373885154724)\n",
      "MHC_CaseStatus_511605_chunk28.txt: ('case_status', 0.6789276003837585)\n",
      "MHC_CaseStatus_511605_chunk29.txt: ('claim_process', 0.37935400009155273)\n",
      "MHC_CaseStatus_511605_chunk3.txt: ('case_status', 0.7473439574241638)\n",
      "MHC_CaseStatus_511605_chunk30.txt: ('case_status', 0.745917797088623)\n",
      "MHC_CaseStatus_511605_chunk31.txt: ('claim_process', 0.3045327365398407)\n",
      "MHC_CaseStatus_511605_chunk32.txt: ('case_status', 0.3267141282558441)\n",
      "MHC_CaseStatus_511605_chunk33.txt: ('case_status', 0.7030060887336731)\n",
      "MHC_CaseStatus_511605_chunk34.txt: ('case_status', 0.6845911145210266)\n",
      "MHC_CaseStatus_511605_chunk4.txt: ('claim_process', 0.4797057509422302)\n",
      "MHC_CaseStatus_511605_chunk5.txt: ('case_status', 0.6173275113105774)\n",
      "MHC_CaseStatus_511605_chunk6.txt: ('claim_process', 0.4554695188999176)\n",
      "MHC_CaseStatus_511605_chunk7.txt: ('case_status', 0.7532203197479248)\n",
      "MHC_CaseStatus_511605_chunk8.txt: ('case_status', 0.7419642210006714)\n",
      "MHC_CaseStatus_511605_chunk9.txt: ('case_status', 0.7060062289237976)\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# 7. Inference: Predict intent for new text\n",
    "def predict_intent(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=64)\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    pred_id = logits.argmax(dim=1).item()\n",
    "    intent = id2label[pred_id]\n",
    "    confidence = logits.softmax(dim=1)[0, pred_id].item()\n",
    "    return intent, confidence\n",
    "# def predict_intent(text):\n",
    "#     inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=64)\n",
    "#     device = next(model.parameters()).device\n",
    "#     inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "#     outputs = model(**inputs)\n",
    "#     pred_id = outputs.logits.argmax(dim=1).item()\n",
    "#     return id2label[pred_id]\n",
    "chunk_dir = os.path.join(r'C:\\Users\\91807\\Downloads\\search_models', 'data', 'chunks')\n",
    "chunk_files = glob.glob(os.path.join(chunk_dir, '*.txt'))\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# print(predict_intent(open(r'C:\\New folder (5)\\new-search-models\\data\\chunks\\MHC_CaseStatus_511605_chunk1.txt', 'r', encoding='utf-8').read()))\n",
    "for chunk_path in chunk_files:\n",
    "    with open(chunk_path, 'r', encoding='utf-8') as f:\n",
    "        chunk_text = f.read()\n",
    "    print(f\"{os.path.basename(chunk_path)}: {predict_intent(chunk_text)}\")\n",
    "# print(predict_intent(\"List the skills in this resume.\"))\n",
    "# print(predict_intent(\"Who is the presiding judge?\"))\n",
    "# print(predict_intent(\"I have a technical issue with the system.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5e2705",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "387d3c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Chunk file not found: C:\\Users\\91807\\Downloads\\search_models\\data\\chunks\\MHC_CaseStatus_511692_chunk1.txt\n",
      "⚠️ Chunk file not found: C:\\Users\\91807\\Downloads\\search_models\\data\\chunks\\MHC_CaseStatus_511692_chunk10.txt\n",
      "⚠️ Chunk file not found: C:\\Users\\91807\\Downloads\\search_models\\data\\chunks\\MHC_CaseStatus_511692_chunk11.txt\n",
      "⚠️ Chunk file not found: C:\\Users\\91807\\Downloads\\search_models\\data\\chunks\\MHC_CaseStatus_511692_chunk12.txt\n",
      "⚠️ Chunk file not found: C:\\Users\\91807\\Downloads\\search_models\\data\\chunks\\MHC_CaseStatus_511692_chunk13.txt\n",
      "⚠️ Chunk file not found: C:\\Users\\91807\\Downloads\\search_models\\data\\chunks\\MHC_CaseStatus_511692_chunk14.txt\n",
      "⚠️ Chunk file not found: C:\\Users\\91807\\Downloads\\search_models\\data\\chunks\\MHC_CaseStatus_511692_chunk15.txt\n",
      "⚠️ Chunk file not found: C:\\Users\\91807\\Downloads\\search_models\\data\\chunks\\MHC_CaseStatus_511692_chunk16.txt\n",
      "⚠️ Chunk file not found: C:\\Users\\91807\\Downloads\\search_models\\data\\chunks\\MHC_CaseStatus_511692_chunk17.txt\n",
      "⚠️ Chunk file not found: C:\\Users\\91807\\Downloads\\search_models\\data\\chunks\\MHC_CaseStatus_511692_chunk18.txt\n",
      "⚠️ Chunk file not found: C:\\Users\\91807\\Downloads\\search_models\\data\\chunks\\MHC_CaseStatus_511692_chunk19.txt\n",
      "⚠️ Chunk file not found: C:\\Users\\91807\\Downloads\\search_models\\data\\chunks\\MHC_CaseStatus_511692_chunk2.txt\n",
      "⚠️ Chunk file not found: C:\\Users\\91807\\Downloads\\search_models\\data\\chunks\\MHC_CaseStatus_511692_chunk20.txt\n",
      "⚠️ Chunk file not found: C:\\Users\\91807\\Downloads\\search_models\\data\\chunks\\MHC_CaseStatus_511692_chunk21.txt\n",
      "⚠️ Chunk file not found: C:\\Users\\91807\\Downloads\\search_models\\data\\chunks\\MHC_CaseStatus_511692_chunk22.txt\n",
      "⚠️ Chunk file not found: C:\\Users\\91807\\Downloads\\search_models\\data\\chunks\\MHC_CaseStatus_511692_chunk23.txt\n",
      "⚠️ Chunk file not found: C:\\Users\\91807\\Downloads\\search_models\\data\\chunks\\MHC_CaseStatus_511692_chunk24.txt\n",
      "⚠️ Chunk file not found: C:\\Users\\91807\\Downloads\\search_models\\data\\chunks\\MHC_CaseStatus_511692_chunk25.txt\n",
      "⚠️ Chunk file not found: C:\\Users\\91807\\Downloads\\search_models\\data\\chunks\\MHC_CaseStatus_511692_chunk26.txt\n",
      "⚠️ Chunk file not found: C:\\Users\\91807\\Downloads\\search_models\\data\\chunks\\MHC_CaseStatus_511692_chunk27.txt\n",
      "⚠️ Chunk file not found: C:\\Users\\91807\\Downloads\\search_models\\data\\chunks\\MHC_CaseStatus_511692_chunk28.txt\n",
      "⚠️ Chunk file not found: C:\\Users\\91807\\Downloads\\search_models\\data\\chunks\\MHC_CaseStatus_511692_chunk29.txt\n",
      "⚠️ Chunk file not found: C:\\Users\\91807\\Downloads\\search_models\\data\\chunks\\MHC_CaseStatus_511692_chunk3.txt\n",
      "⚠️ Chunk file not found: C:\\Users\\91807\\Downloads\\search_models\\data\\chunks\\MHC_CaseStatus_511692_chunk30.txt\n",
      "⚠️ Chunk file not found: C:\\Users\\91807\\Downloads\\search_models\\data\\chunks\\MHC_CaseStatus_511692_chunk31.txt\n",
      "⚠️ Chunk file not found: C:\\Users\\91807\\Downloads\\search_models\\data\\chunks\\MHC_CaseStatus_511692_chunk32.txt\n",
      "⚠️ Chunk file not found: C:\\Users\\91807\\Downloads\\search_models\\data\\chunks\\MHC_CaseStatus_511692_chunk33.txt\n",
      "⚠️ Chunk file not found: C:\\Users\\91807\\Downloads\\search_models\\data\\chunks\\MHC_CaseStatus_511692_chunk34.txt\n",
      "⚠️ Chunk file not found: C:\\Users\\91807\\Downloads\\search_models\\data\\chunks\\MHC_CaseStatus_511692_chunk35.txt\n",
      "⚠️ Chunk file not found: C:\\Users\\91807\\Downloads\\search_models\\data\\chunks\\MHC_CaseStatus_511692_chunk36.txt\n",
      "⚠️ Chunk file not found: C:\\Users\\91807\\Downloads\\search_models\\data\\chunks\\MHC_CaseStatus_511692_chunk37.txt\n",
      "⚠️ Chunk file not found: C:\\Users\\91807\\Downloads\\search_models\\data\\chunks\\MHC_CaseStatus_511692_chunk38.txt\n",
      "⚠️ Chunk file not found: C:\\Users\\91807\\Downloads\\search_models\\data\\chunks\\MHC_CaseStatus_511692_chunk39.txt\n",
      "⚠️ Chunk file not found: C:\\Users\\91807\\Downloads\\search_models\\data\\chunks\\MHC_CaseStatus_511692_chunk4.txt\n",
      "⚠️ Chunk file not found: C:\\Users\\91807\\Downloads\\search_models\\data\\chunks\\MHC_CaseStatus_511692_chunk40.txt\n",
      "⚠️ Chunk file not found: C:\\Users\\91807\\Downloads\\search_models\\data\\chunks\\MHC_CaseStatus_511692_chunk41.txt\n",
      "⚠️ Chunk file not found: C:\\Users\\91807\\Downloads\\search_models\\data\\chunks\\MHC_CaseStatus_511692_chunk42.txt\n",
      "⚠️ Chunk file not found: C:\\Users\\91807\\Downloads\\search_models\\data\\chunks\\MHC_CaseStatus_511692_chunk43.txt\n",
      "⚠️ Chunk file not found: C:\\Users\\91807\\Downloads\\search_models\\data\\chunks\\MHC_CaseStatus_511692_chunk44.txt\n",
      "⚠️ Chunk file not found: C:\\Users\\91807\\Downloads\\search_models\\data\\chunks\\MHC_CaseStatus_511692_chunk45.txt\n",
      "⚠️ Chunk file not found: C:\\Users\\91807\\Downloads\\search_models\\data\\chunks\\MHC_CaseStatus_511692_chunk46.txt\n",
      "⚠️ Chunk file not found: C:\\Users\\91807\\Downloads\\search_models\\data\\chunks\\MHC_CaseStatus_511692_chunk47.txt\n",
      "⚠️ Chunk file not found: C:\\Users\\91807\\Downloads\\search_models\\data\\chunks\\MHC_CaseStatus_511692_chunk48.txt\n",
      "⚠️ Chunk file not found: C:\\Users\\91807\\Downloads\\search_models\\data\\chunks\\MHC_CaseStatus_511692_chunk49.txt\n",
      "⚠️ Chunk file not found: C:\\Users\\91807\\Downloads\\search_models\\data\\chunks\\MHC_CaseStatus_511692_chunk5.txt\n",
      "⚠️ Chunk file not found: C:\\Users\\91807\\Downloads\\search_models\\data\\chunks\\MHC_CaseStatus_511692_chunk50.txt\n",
      "⚠️ Chunk file not found: C:\\Users\\91807\\Downloads\\search_models\\data\\chunks\\MHC_CaseStatus_511692_chunk6.txt\n",
      "⚠️ Chunk file not found: C:\\Users\\91807\\Downloads\\search_models\\data\\chunks\\MHC_CaseStatus_511692_chunk7.txt\n",
      "⚠️ Chunk file not found: C:\\Users\\91807\\Downloads\\search_models\\data\\chunks\\MHC_CaseStatus_511692_chunk8.txt\n",
      "⚠️ Chunk file not found: C:\\Users\\91807\\Downloads\\search_models\\data\\chunks\\MHC_CaseStatus_511692_chunk9.txt\n",
      "Loaded 36 labeled examples from correct_intents.txt.\n"
     ]
    }
   ],
   "source": [
    "# 📥 Build labeled training data from correct_intents.txt\n",
    "import re\n",
    "import os\n",
    "correct_intents_path = r'C:\\Users\\91807\\Downloads\\search_models\\correct_intents.txt'\n",
    "chunk_dir = os.path.join(r'C:\\Users\\91807\\Downloads\\search_models', 'data', 'chunks')\n",
    "\n",
    "labeled_from_file = []\n",
    "with open(correct_intents_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        m = re.match(r'([^:]+):.*?\\'intent\\': \\'([^\\']+)\\'', line)\n",
    "        if m:\n",
    "            fname, intent = m.group(1).strip(), m.group(2).strip()\n",
    "            chunk_path = os.path.join(chunk_dir, fname)\n",
    "            if os.path.exists(chunk_path):\n",
    "                with open(chunk_path, 'r', encoding='utf-8') as cf:\n",
    "                    chunk_text = cf.read()\n",
    "                labeled_from_file.append({\"text\": chunk_text, \"label\": intent})\n",
    "            else:\n",
    "                print(f\"⚠️ Chunk file not found: {chunk_path}\")\n",
    "        else:\n",
    "            print(f\"⚠️ Could not parse line: {line.strip()}\")\n",
    "\n",
    "print(f\"Loaded {len(labeled_from_file)} labeled examples from correct_intents.txt.\")\n",
    "# Add these to your expanded_data list before retraining:\n",
    "# expanded_data.extend(labeled_from_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f25a1314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expanded_data now has 132 examples (including those from correct_intents.txt).\n"
     ]
    }
   ],
   "source": [
    "# ➕ Add labeled examples from correct_intents.txt to your training data\n",
    "if 'expanded_data' in globals() and 'labeled_from_file' in globals():\n",
    "    expanded_data.extend(labeled_from_file)\n",
    "    print(f\"expanded_data now has {len(expanded_data)} examples (including those from correct_intents.txt).\")\n",
    "else:\n",
    "    print(\"⚠️ Make sure both expanded_data and labeled_from_file are defined before running this cell.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66792358",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 🛠️ Iterative Improvement: Add Misclassified Examples\n",
    "\n",
    "# If you notice misclassified chunks, copy their text and true intent below. Add them to your training data to help the model learn from its mistakes. Retrain and re-evaluate for better accuracy.\n",
    "# Example: Add misclassified examples to your training data\n",
    "# Replace the text and label with your real misclassified cases\n",
    "misclassified_examples = [\n",
    "    {\"text\": \"<Paste misclassified chunk text here>\", \"label\": \"<correct_intent>\"},\n",
    "    # Example:\n",
    "    # {\"text\": \"The case was closed on 2023-05-01.\", \"label\": \"case_status\"},\n",
    "    # {\"text\": \"Please send me a copy of the final judgment.\", \"label\": \"document_request\"},\n",
    "]\n",
    "\n",
    "# Add these to your expanded_data list before retraining:\n",
    "# expanded_data.extend(misclassified_examples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9b3279",
   "metadata": {},
   "source": [
    "## ⚠️ Expand Your Labeled Dataset for Better Classifier Performance\n",
    "\n",
    "Your current labeled dataset is too small and unbalanced, which causes the model to predict the same intent for most inputs. \n",
    "\n",
    "- Add at least 10–20 diverse, realistic examples for each intent category.\n",
    "- Include edge cases and ambiguous queries.\n",
    "- The more varied and representative your data, the better your classifier will perform.\n",
    "\n",
    "After expanding, retrain the model and re-run the evaluation cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "18794fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.58\n",
      "\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "      case_status       1.00      1.00      1.00         2\n",
      "    claim_process       0.50      0.50      0.50         2\n",
      " document_request       0.50      1.00      0.67         3\n",
      "     general_info       0.00      0.00      0.00         3\n",
      "      resume_info       0.50      1.00      0.67         1\n",
      "technical_support       0.00      0.00      0.00         1\n",
      "\n",
      "         accuracy                           0.58        12\n",
      "        macro avg       0.42      0.58      0.47        12\n",
      "     weighted avg       0.42      0.58      0.47        12\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2 0 0 0 0 0]\n",
      " [0 1 1 0 0 0]\n",
      " [0 0 3 0 0 0]\n",
      " [0 1 1 0 1 0]\n",
      " [0 0 0 0 1 0]\n",
      " [0 0 1 0 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\New folder (5)\\new-search-models\\search_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\New folder (5)\\new-search-models\\search_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\New folder (5)\\new-search-models\\search_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 8. Evaluate classifier performance on the test set\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Get true and predicted labels for the test set\n",
    "true_labels = [id2label[i] for i in test_ds['labels']]\n",
    "pred_labels = []\n",
    "for text in test_ds['text']:\n",
    "    pred = predict_intent(text)\n",
    "    pred_labels.append(pred)\n",
    "\n",
    "# Accuracy\n",
    "acc = accuracy_score(true_labels, pred_labels)\n",
    "print(f\"Test Accuracy: {acc:.2f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, pred_labels))\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(true_labels, pred_labels, labels=list(label2id.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2b8b2f",
   "metadata": {},
   "source": [
    "## 🏷️ Expand Labeled Data for Better Intent Classification\n",
    "\n",
    "To improve classifier performance, add more diverse and realistic examples for each intent. This helps the model generalize and reduces bias toward majority classes. Below is an expanded dataset template you can use and modify for your domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "726005e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 🏷️ Expand Labeled Data for Better Intent Classification\n",
    "\n",
    "# To improve classifier performance, add more diverse and realistic examples for each intent. This helps the model generalize and reduces bias toward majority classes. Below is an expanded dataset template you can use and modify for your domain.\n",
    "# Template: Expanded labeled data for intent fine-tuning\n",
    "# Copy, edit, and expand this list with your real examples\n",
    "expanded_data = [\n",
    "    # claim_process\n",
    "    {\"text\": \"How do I file a claim?\", \"label\": \"claim_process\"},\n",
    "    {\"text\": \"What is the process for submitting an insurance claim?\", \"label\": \"claim_process\"},\n",
    "    {\"text\": \"I want to submit a new claim for my car accident.\", \"label\": \"claim_process\"},\n",
    "    {\"text\": \"Guide me through the claim submission steps.\", \"label\": \"claim_process\"},\n",
    "    {\"text\": \"Where do I upload my claim documents?\", \"label\": \"claim_process\"},\n",
    "    {\"text\": \"How long does it take to process a claim?\", \"label\": \"claim_process\"},\n",
    "    {\"text\": \"Can I check the status of my insurance claim?\", \"label\": \"claim_process\"},\n",
    "    {\"text\": \"What documents are needed to file a claim?\", \"label\": \"claim_process\"},\n",
    "    {\"text\": \"Is there a deadline for submitting claims?\", \"label\": \"claim_process\"},\n",
    "    {\"text\": \"Can I cancel a claim after submitting?\", \"label\": \"claim_process\"},\n",
    "    # case_status\n",
    "    {\"text\": \"What is the current status of the case?\", \"label\": \"case_status\"},\n",
    "    {\"text\": \"Show me the progress of case number 511605.\", \"label\": \"case_status\"},\n",
    "    {\"text\": \"Has a judgment been issued in my case?\", \"label\": \"case_status\"},\n",
    "    {\"text\": \"Is my case still pending?\", \"label\": \"case_status\"},\n",
    "    {\"text\": \"When is the next hearing for my case?\", \"label\": \"case_status\"},\n",
    "    {\"text\": \"What was the outcome of the last court session?\", \"label\": \"case_status\"},\n",
    "    {\"text\": \"Who is the presiding judge for this case?\", \"label\": \"case_status\"},\n",
    "    {\"text\": \"Has an appeal been filed?\", \"label\": \"case_status\"},\n",
    "    {\"text\": \"Is there an order available for my case?\", \"label\": \"case_status\"},\n",
    "    {\"text\": \"What is the next step in my case?\", \"label\": \"case_status\"},\n",
    "    # document_request\n",
    "    {\"text\": \"Can I get a copy of the case order?\", \"label\": \"document_request\"},\n",
    "    {\"text\": \"How do I request the judgment document?\", \"label\": \"document_request\"},\n",
    "    {\"text\": \"I need certified copies of my case documents.\", \"label\": \"document_request\"},\n",
    "    {\"text\": \"Where can I download the court forms?\", \"label\": \"document_request\"},\n",
    "    {\"text\": \"Request a copy of the final order.\", \"label\": \"document_request\"},\n",
    "    {\"text\": \"How do I obtain previous hearing transcripts?\", \"label\": \"document_request\"},\n",
    "    {\"text\": \"Can I get a digital copy of my case file?\", \"label\": \"document_request\"},\n",
    "    {\"text\": \"What is the fee for document requests?\", \"label\": \"document_request\"},\n",
    "    {\"text\": \"How long does it take to receive requested documents?\", \"label\": \"document_request\"},\n",
    "    {\"text\": \"Is there a limit to the number of documents I can request?\", \"label\": \"document_request\"},\n",
    "    # resume_info\n",
    "    {\"text\": \"What skills are listed in the resume?\", \"label\": \"resume_info\"},\n",
    "    {\"text\": \"List the programming languages known by the applicant.\", \"label\": \"resume_info\"},\n",
    "    {\"text\": \"Show me the candidate's work experience.\", \"label\": \"resume_info\"},\n",
    "    {\"text\": \"What certifications does the applicant have?\", \"label\": \"resume_info\"},\n",
    "    {\"text\": \"Summarize the professional experience section.\", \"label\": \"resume_info\"},\n",
    "    {\"text\": \"List the tools and technologies used by the candidate.\", \"label\": \"resume_info\"},\n",
    "    {\"text\": \"What is the educational background of the applicant?\", \"label\": \"resume_info\"},\n",
    "    {\"text\": \"What are the achievements or awards?\", \"label\": \"resume_info\"},\n",
    "    {\"text\": \"Show me the contact information in the resume.\", \"label\": \"resume_info\"},\n",
    "    {\"text\": \"What is the career objective or summary?\", \"label\": \"resume_info\"},\n",
    "    # technical_support\n",
    "    {\"text\": \"I have a technical issue with the system.\", \"label\": \"technical_support\"},\n",
    "    {\"text\": \"There is a problem with the website.\", \"label\": \"technical_support\"},\n",
    "    {\"text\": \"I can't log in to my account.\", \"label\": \"technical_support\"},\n",
    "    {\"text\": \"The upload button is not working.\", \"label\": \"technical_support\"},\n",
    "    {\"text\": \"How do I reset my password?\", \"label\": \"technical_support\"},\n",
    "    {\"text\": \"The page is loading very slowly.\", \"label\": \"technical_support\"},\n",
    "    {\"text\": \"I received an error message while submitting my form.\", \"label\": \"technical_support\"},\n",
    "    {\"text\": \"The system crashed during my session.\", \"label\": \"technical_support\"},\n",
    "    {\"text\": \"How do I contact technical support?\", \"label\": \"technical_support\"},\n",
    "    {\"text\": \"The app keeps freezing.\", \"label\": \"technical_support\"},\n",
    "    # general_info\n",
    "    {\"text\": \"Give me a summary of the file.\", \"label\": \"general_info\"},\n",
    "    {\"text\": \"Tell me about this document.\", \"label\": \"general_info\"},\n",
    "    {\"text\": \"What is the purpose of this document?\", \"label\": \"general_info\"},\n",
    "    {\"text\": \"Provide general information about the case.\", \"label\": \"general_info\"},\n",
    "    {\"text\": \"What are the office hours?\", \"label\": \"general_info\"},\n",
    "    {\"text\": \"How do I contact the support team?\", \"label\": \"general_info\"},\n",
    "    {\"text\": \"Where is the office located?\", \"label\": \"general_info\"},\n",
    "    {\"text\": \"What services are offered?\", \"label\": \"general_info\"},\n",
    "    {\"text\": \"How do I register for an account?\", \"label\": \"general_info\"},\n",
    "    {\"text\": \"What is the refund policy?\", \"label\": \"general_info\"},\n",
    "]\n",
    "\n",
    "# You can now use expanded_data instead of the old 'data' list for training your classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513680b2",
   "metadata": {},
   "source": [
    "## keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea975b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a34421eb",
   "metadata": {},
   "source": [
    "# 🔑 Keyword Extraction Techniques: Code Examples\n",
    "\n",
    "Below are code snippets for the most common keyword extraction methods. You can run these to compare results on your text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "271baba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF keywords: ['management', 'developed', 'user', 'document', 'spring', 'tournament', 'angular', 'implemented', 'secure', 'security']\n"
     ]
    }
   ],
   "source": [
    "# 1. TF-IDF (Term Frequency–Inverse Document Frequency)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "text = open(r'C:\\New folder (5)\\new-search-models\\data\\chunks\\Jimson_Ratnam_JavaFullStackDeveloper_2+years_chunk1.txt', 'r', encoding='utf-8').read()\n",
    "\n",
    "def extract_keywords_tfidf(text, top_n=10):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,2))\n",
    "    tfidf = vectorizer.fit_transform([text])\n",
    "    scores = zip(vectorizer.get_feature_names_out(), tfidf.toarray()[0])\n",
    "    sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "    return [w for w, s in sorted_scores[:top_n]]\n",
    "\n",
    "print('TF-IDF keywords:', extract_keywords_tfidf(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a552215d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rake-nltk\n",
      "  Downloading rake_nltk-1.0.6-py3-none-any.whl (9.1 kB)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.6.2 in c:\\new folder (5)\\new-search-models\\search_env\\lib\\site-packages (from rake-nltk) (3.9.1)\n",
      "Requirement already satisfied: tqdm in c:\\new folder (5)\\new-search-models\\search_env\\lib\\site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (4.67.1)\n",
      "Requirement already satisfied: click in c:\\new folder (5)\\new-search-models\\search_env\\lib\\site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (8.2.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\new folder (5)\\new-search-models\\search_env\\lib\\site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (2024.11.6)\n",
      "Requirement already satisfied: joblib in c:\\new folder (5)\\new-search-models\\search_env\\lib\\site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (1.5.1)\n",
      "Requirement already satisfied: colorama in c:\\new folder (5)\\new-search-models\\search_env\\lib\\site-packages (from click->nltk<4.0.0,>=3.6.2->rake-nltk) (0.4.6)\n",
      "Installing collected packages: rake-nltk\n",
      "Successfully installed rake-nltk-1.0.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install rake-nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3252db81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAKE keywords: ['based authentication verification apis using spring boot spring security built batch processing workflows', '50 satisfaction scores enhanced document management improved tournament efficiency implemented robust features', 'publication key achievements strengthened system security improved user interaction leveraged angular', 'docker ensuring consistency across development production environments designed cicd pipelines', 'volume applications developed dynamic responsive ui components using angular typescript', 'time inventory tracking alerts optimizing stock management integrated elasticsearch', 'automate workflow improve deployment efficiency deployed scalable applications', 'automated build test deployment processes utilized github actions', 'overall tournament management process robust admin control real', 'responsive ui using angular enhancing user experience']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 2. RAKE (Rapid Automatic Keyword Extraction)\n",
    "from rake_nltk import Rake\n",
    "\n",
    "def extract_keywords_rake(text, top_n=10):\n",
    "    rake = Rake()\n",
    "    rake.extract_keywords_from_text(text)\n",
    "    return rake.get_ranked_phrases()[:top_n]\n",
    "\n",
    "print('RAKE keywords:', extract_keywords_rake(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bc6fcaae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yake\n",
      "  Downloading yake-0.4.8-py2.py3-none-any.whl (60 kB)\n",
      "     ---------------------------------------- 0.0/60.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 60.2/60.2 kB 3.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: networkx in c:\\new folder (5)\\new-search-models\\search_env\\lib\\site-packages (from yake) (3.4.2)\n",
      "Collecting tabulate\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: numpy in c:\\new folder (5)\\new-search-models\\search_env\\lib\\site-packages (from yake) (2.2.6)\n",
      "Requirement already satisfied: click>=6.0 in c:\\new folder (5)\\new-search-models\\search_env\\lib\\site-packages (from yake) (8.2.1)\n",
      "Collecting jellyfish\n",
      "  Downloading jellyfish-1.2.0-cp310-cp310-win_amd64.whl (217 kB)\n",
      "     ---------------------------------------- 0.0/217.1 kB ? eta -:--:--\n",
      "     -------------------------------------- 217.1/217.1 kB 6.7 MB/s eta 0:00:00\n",
      "Collecting segtok\n",
      "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: colorama in c:\\new folder (5)\\new-search-models\\search_env\\lib\\site-packages (from click>=6.0->yake) (0.4.6)\n",
      "Requirement already satisfied: regex in c:\\new folder (5)\\new-search-models\\search_env\\lib\\site-packages (from segtok->yake) (2024.11.6)\n",
      "Installing collected packages: tabulate, segtok, jellyfish, yake\n",
      "Successfully installed jellyfish-1.2.0 segtok-1.5.11 tabulate-0.9.0 yake-0.4.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ef1493c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YAKE keywords: ['Spring Boot Spring', 'Spring Boot Angular', 'Boot Spring Security', 'Spring Security Built', 'Software Solutions Developed', 'Interaction Leveraged Angular', 'Full Stack Developer', 'Present Devzen Software', 'Devzen Software Solutions', 'System Security Improved']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 3. YAKE (Yet Another Keyword Extractor)\n",
    "import yake\n",
    "\n",
    "def extract_keywords_yake(text, top_n=10):\n",
    "    kw_extractor = yake.KeywordExtractor(top=top_n, stopwords=None)\n",
    "    keywords = kw_extractor.extract_keywords(text)\n",
    "    return [kw for kw, score in keywords]\n",
    "\n",
    "print('YAKE keywords:', extract_keywords_yake(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "47b5b532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyBERT keywords: ['apis', 'authentication', 'api', 'aws', 'jwt', 'cloud', 'workflows', 'angular', 'secure', 'java']\n"
     ]
    }
   ],
   "source": [
    "# 5. KeyBERT (Embedding-based, already in your notebook)\n",
    "from keybert import KeyBERT\n",
    "keyword_model = KeyBERT()\n",
    "keywords = [kw for kw, _ in keyword_model.extract_keywords(text, top_n=10)]\n",
    "\n",
    "print('KeyBERT keywords:', [kw for kw, _ in keyword_model.extract_keywords(text, top_n=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e3c3700f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy POS keywords: ['large-scale data management', 'dynamic responsive ui components', 'product management', 'seamless document creation socialization', 'a', 'real-time inventory tracking alerts', 'user authentication', 'spring boot', 'a secure api', 'github actions']\n"
     ]
    }
   ],
   "source": [
    "# 6. spaCy POS-based (Nouns, Noun Phrases)\n",
    "def extract_keywords_spacy(text, top_n=10):\n",
    "    doc = nlp(text)\n",
    "    noun_chunks = list(set(chunk.text.strip().lower() for chunk in doc.noun_chunks))\n",
    "    nouns = list(set(token.lemma_ for token in doc if token.pos_ == 'NOUN' and not token.is_stop))\n",
    "    return (noun_chunks + nouns)[:top_n]\n",
    "\n",
    "print('spaCy POS keywords:', extract_keywords_spacy(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8b8dcd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "You can compare the outputs of these techniques on your sample text. For best results, try ensemble or hybrid approaches (e.g., combine TF-IDF, KeyBERT, and POS-based keywords)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23e9a43",
   "metadata": {},
   "source": [
    "# 🏆 Recommended Industry Pipeline: Robust Keyword Extraction\n",
    "\n",
    "This pipeline combines the strengths of multiple methods for high accuracy and robustness, suitable for production and real-world data:\n",
    "\n",
    "- **KeyBERT** (embedding-based, semantic): Best for context-aware, relevant keywords.\n",
    "- **TF-IDF** (statistical): Captures frequent, document-specific terms.\n",
    "- **spaCy POS-based**: Ensures inclusion of important noun phrases.\n",
    "- **Ensemble/Hybrid**: Merges and deduplicates keywords from all methods.\n",
    "\n",
    "You can further filter or rank keywords by frequency, relevance, or domain-specific rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e67476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyBERT: ['apis', 'authentication', 'api', 'aws', 'jwt', 'cloud', 'workflows', 'angular', 'secure', 'java']\n",
      "TF-IDF: ['management', 'developed', 'user', 'document', 'spring', 'tournament', 'angular', 'implemented', 'secure', 'security']\n",
      "spaCy POS: ['high-volume applications', 'user authentication', 'unauthorized access functionality', 'large-scale data management', '50 satisfaction scores', 'live score updates', 'user interaction', 'seamless transactions', 'a secure api', 'scalable applications']\n",
      "\n",
      "---\n",
      "Ensemble (deduplicated): ['apis', 'authentication', 'api', 'aws', 'jwt', 'cloud', 'workflows', 'angular', 'secure', 'java', 'management', 'developed', 'user', 'document', 'spring', 'tournament', 'implemented', 'security', 'high-volume applications', 'user authentication', 'unauthorized access functionality', 'large-scale data management', '50 satisfaction scores', 'live score updates', 'user interaction', 'seamless transactions', 'a secure api', 'scalable applications']\n"
     ]
    }
   ],
   "source": [
    "# --- Robust Keyword Extraction Pipeline ---\n",
    "from keybert import KeyBERT\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy\n",
    "\n",
    "# Load models (reuse if already loaded)\n",
    "keyword_model = KeyBERT()\n",
    "nlp = spacy.load(\"en_core_web_trf\") if spacy.util.is_package(\"en_core_web_trf\") else spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 1. KeyBERT keywords\n",
    "keybert_keywords = [kw for kw, _ in keyword_model.extract_keywords(text, top_n=10)]\n",
    "\n",
    "# 2. TF-IDF keywords\n",
    "def extract_keywords_tfidf(text, top_n=10):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,2))\n",
    "    tfidf = vectorizer.fit_transform([text])\n",
    "    scores = zip(vectorizer.get_feature_names_out(), tfidf.toarray()[0])\n",
    "    sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "    return [w for w, s in sorted_scores[:top_n]]\n",
    "tfidf_keywords = extract_keywords_tfidf(text)\n",
    "\n",
    "# 3. spaCy POS-based keywords\n",
    "def extract_keywords_spacy(text, top_n=10):\n",
    "    doc = nlp(text)\n",
    "    noun_chunks = list(set(chunk.text.strip().lower() for chunk in doc.noun_chunks))\n",
    "    nouns = list(set(token.lemma_ for token in doc if token.pos_ == 'NOUN' and not token.is_stop))\n",
    "    return (noun_chunks + nouns)[:top_n]\n",
    "spacy_keywords = extract_keywords_spacy(text)\n",
    "\n",
    "# 4. Hybrid/ensemble: merge and deduplicate\n",
    "all_keywords = keybert_keywords + tfidf_keywords + spacy_keywords\n",
    "unique_keywords = []\n",
    "for kw in all_keywords:\n",
    "    if kw not in unique_keywords:\n",
    "        unique_keywords.append(kw)\n",
    "\n",
    "print(\"KeyBERT:\", keybert_keywords)\n",
    "print(\"TF-IDF:\", tfidf_keywords)\n",
    "print(\"spaCy POS:\", spacy_keywords)\n",
    "print(\"\\n---\\nEnsemble (deduplicated):\", unique_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f80426",
   "metadata": {},
   "source": [
    "**Best Practices:**\n",
    "- Adjust `top_n` for each method based on your needs.\n",
    "- Optionally, filter out keywords that are too short, too common, or not domain-relevant.\n",
    "- For domain-specific tasks, add custom rules or fine-tune KeyBERT with a domain model.\n",
    "- For very high accuracy, consider adding a supervised NER/sequence labeling model as a final filter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb1979b",
   "metadata": {},
   "source": [
    "## Entity Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba9c402",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec5c8913",
   "metadata": {},
   "source": [
    "# 🏷️ Entity Extraction Techniques: Code Examples\n",
    "\n",
    "Below are code snippets for common entity extraction methods. You can run these to compare results on your text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3f1fdd95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy Entities: [('Devzen', 'ORG'), ('Spring Boot Spring Security', 'PRODUCT'), ('Spring Batch', 'PRODUCT'), ('Elasticsearch', 'PRODUCT'), ('Angular TypeScript', 'PRODUCT'), ('Spring Boot Angular', 'PRODUCT'), ('AWS EC2 S3 RDS IAM', 'PRODUCT'), ('Spring Boot', 'PRODUCT'), ('Angular', 'PRODUCT'), ('25', 'CARDINAL'), ('50', 'CARDINAL'), ('portals', 'ORG'), ('20', 'CARDINAL'), ('six months of launch', 'DATE'), ('40', 'CARDINAL'), ('90', 'CARDINAL')]\n"
     ]
    }
   ],
   "source": [
    "# 1. spaCy Named Entity Recognition (NER)\n",
    "import spacy\n",
    "\n",
    "# Load spaCy model (already loaded as nlp in previous cells)\n",
    "doc = nlp(text)\n",
    "spacy_entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "print('spaCy Entities:', spacy_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a5a2e32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers NER Entities: [(' Java Full Stack Developer 20', 'MISC', np.float32(0.8558725)), (' Devzen Software Solutions', 'ORG', np.float32(0.99328136)), (' JWT-based', 'MISC', np.float32(0.95012635)), (' Spring Boot Spring Security', 'MISC', np.float32(0.98164487)), (' Spring Batch', 'MISC', np.float32(0.9609308)), (' Elasticsearch', 'MISC', np.float32(0.9405725)), (' Angular TypeScript', 'MISC', np.float32(0.990668)), (' Spring Boot Angular', 'MISC', np.float32(0.9826927)), (' Docker', 'MISC', np.float32(0.97396314)), (' GitHub Actions', 'MISC', np.float32(0.9847411)), (' AWS EC2 S3 RDS IAM', 'MISC', np.float32(0.9680392)), (' Spring Boot', 'MISC', np.float32(0.9820712)), (' Angular', 'MISC', np.float32(0.98616624)), (' Angular', 'MISC', np.float32(0.98883575))]\n"
     ]
    }
   ],
   "source": [
    "# 2. Transformers-based NER (e.g., HuggingFace pipeline)\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "ner_pipe = pipeline(\n",
    "    \"ner\",\n",
    "    model=AutoModelForTokenClassification.from_pretrained(\"Jean-Baptiste/roberta-large-ner-english\"),\n",
    "    tokenizer=AutoTokenizer.from_pretrained(\"Jean-Baptiste/roberta-large-ner-english\"),\n",
    "    aggregation_strategy=\"simple\",\n",
    "    device=-1  # CPU\n",
    ")\n",
    "transformers_entities = [(ent['word'], ent['entity_group'], ent['score']) for ent in ner_pipe(text) if ent['score'] > 0.8]\n",
    "print('Transformers NER Entities:', transformers_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b9922c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emails: []\n",
      "Dates: []\n"
     ]
    }
   ],
   "source": [
    "# 3. Regex-based Entity Extraction (for custom patterns)\n",
    "import re\n",
    "\n",
    "# Example: Extract email addresses and dates\n",
    "emails = re.findall(r'[\\w\\.-]+@[\\w\\.-]+', text)\n",
    "dates = re.findall(r'\\b\\d{4}-\\d{2}-\\d{2}\\b', text)\n",
    "print('Emails:', emails)\n",
    "print('Dates:', dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "390a39ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Ensemble/Hybrid: Combine spaCy, Transformers, and Regex\n",
    "def extract_entities_hybrid(text):\n",
    "    entities = set()\n",
    "    entity_types = []\n",
    "    entity_details = []\n",
    "    # spaCy\n",
    "    for ent in nlp(text).ents:\n",
    "        entities.add(ent.text)\n",
    "        entity_types.append(ent.label_)\n",
    "        entity_details.append({\n",
    "            \"text\": ent.text,\n",
    "            \"type\": ent.label_,\n",
    "            \"score\": None\n",
    "        })\n",
    "    # Transformers\n",
    "    for ent in ner_pipe(text):\n",
    "        if ent['score'] > 0.8:\n",
    "            entities.add(ent['word'])\n",
    "            entity_types.append(ent['entity_group'])\n",
    "            entity_details.append({\n",
    "                \"text\": ent['word'],\n",
    "                \"type\": ent['entity_group'],\n",
    "                \"score\": ent['score']\n",
    "            })\n",
    "    # Regex (add more patterns as needed)\n",
    "    for email in re.findall(r'[\\w\\.-]+@[\\w\\.-]+', text):\n",
    "        entities.add(email)\n",
    "        entity_types.append(\"EMAIL\")\n",
    "        entity_details.append({\n",
    "            \"text\": email,\n",
    "            \"type\": \"EMAIL\",\n",
    "            \"score\": None\n",
    "        })\n",
    "    for date in re.findall(r'\\b\\d{4}-\\d{2}-\\d{2}\\b', text):\n",
    "        entities.add(date)\n",
    "        entity_types.append(\"DATE\")\n",
    "        entity_details.append({\n",
    "            \"text\": date,\n",
    "            \"type\": \"DATE\",\n",
    "            \"score\": None\n",
    "        })\n",
    "    return {\n",
    "        \"entities\": sorted(entities),\n",
    "        \"entity_types\": entity_types,\n",
    "        \"entity_details\": entity_details\n",
    "    }\n",
    "\n",
    "# for chunk_path in chunk_files:\n",
    "#     with open(chunk_path, 'r', encoding='utf-8') as f:\n",
    "#         chunk_text = f.read()\n",
    "#     print(f\"{os.path.basename(chunk_path)}: {extract_entities_hybrid(chunk_text)}\")\n",
    "\n",
    "    # print('Hybrid Entities:', extract_entities_hybrid(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db9271e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "You can compare the outputs of these techniques on your sample text. For best results, use a hybrid approach and add domain-specific regex patterns as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dddc464",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f90ff20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'document_name': 'Jimson_Ratnam_JavaFullStackDeveloper_2+years',\n",
      " 'embedding': [-0.3491048812866211,\n",
      "               0.14838749170303345,\n",
      "               -0.98483806848526,\n",
      "               -1.0254418849945068,\n",
      "               1.3563048839569092,\n",
      "               0.4853346049785614,\n",
      "               -0.3664737641811371,\n",
      "               1.0934932231903076,\n",
      "               -0.041868966072797775,\n",
      "               0.3456886410713196,\n",
      "               -0.9854051470756531,\n",
      "               0.397349089384079,\n",
      "               -0.6585312485694885,\n",
      "               0.2738799750804901,\n",
      "               0.8393633961677551,\n",
      "               1.3065966367721558,\n",
      "               0.025908133015036583,\n",
      "               0.10038670897483826,\n",
      "               -0.8247460722923279,\n",
      "               0.09762020409107208,\n",
      "               0.32058557868003845,\n",
      "               -0.1351531445980072,\n",
      "               -0.3738860785961151,\n",
      "               0.3770873546600342,\n",
      "               -0.10935547202825546,\n",
      "               -0.12841953337192535,\n",
      "               0.5968868732452393,\n",
      "               0.9730381965637207,\n",
      "               -1.177925944328308,\n",
      "               0.5810787677764893,\n",
      "               -0.07196735590696335,\n",
      "               0.23208141326904297,\n",
      "               0.19561141729354858,\n",
      "               0.757622241973877,\n",
      "               0.5397658348083496,\n",
      "               -0.8168906569480896,\n",
      "               -0.04755200818181038,\n",
      "               -0.9888477921485901,\n",
      "               -0.8746240139007568,\n",
      "               0.32201308012008667,\n",
      "               0.33997198939323425,\n",
      "               -0.24015964567661285,\n",
      "               0.16876916587352753,\n",
      "               -0.9428987503051758,\n",
      "               0.41769859194755554,\n",
      "               0.5574105978012085,\n",
      "               -0.40695661306381226,\n",
      "               0.07097967714071274,\n",
      "               0.7136660218238831,\n",
      "               -0.25156939029693604,\n",
      "               0.15477514266967773,\n",
      "               0.21889272332191467,\n",
      "               -0.3017265796661377,\n",
      "               -0.52826988697052,\n",
      "               0.8995062112808228,\n",
      "               0.8906257748603821,\n",
      "               -0.41788139939308167,\n",
      "               -0.03603212907910347,\n",
      "               -0.055212169885635376,\n",
      "               -0.8549683094024658,\n",
      "               0.8473892211914062,\n",
      "               -0.4362633228302002,\n",
      "               0.2933747470378876,\n",
      "               0.7044830322265625,\n",
      "               0.6651892066001892,\n",
      "               0.3960687816143036,\n",
      "               -0.24316950142383575,\n",
      "               -0.3169558346271515,\n",
      "               -0.48451071977615356,\n",
      "               0.6345298886299133,\n",
      "               -0.38246652483940125,\n",
      "               -0.33790212869644165,\n",
      "               -0.5052288174629211,\n",
      "               0.5468796491622925,\n",
      "               0.11824744939804077,\n",
      "               0.6444907188415527,\n",
      "               -0.17866410315036774,\n",
      "               0.25997066497802734,\n",
      "               -0.7716037631034851,\n",
      "               -0.6971216797828674,\n",
      "               0.9460515975952148,\n",
      "               1.427734613418579,\n",
      "               0.9213004112243652,\n",
      "               -0.18442951142787933,\n",
      "               -0.43008923530578613,\n",
      "               -0.4660058915615082,\n",
      "               -0.8542302846908569,\n",
      "               0.44331783056259155,\n",
      "               -0.4967937767505646,\n",
      "               0.2901189625263214,\n",
      "               0.5657852292060852,\n",
      "               1.0281339883804321,\n",
      "               -0.5431565642356873,\n",
      "               1.0202807188034058,\n",
      "               0.8329213261604309,\n",
      "               -0.6352890729904175,\n",
      "               0.04332732781767845,\n",
      "               -0.28344646096229553,\n",
      "               0.30150195956230164,\n",
      "               -0.28106725215911865,\n",
      "               -0.0931018739938736,\n",
      "               -0.10721389204263687,\n",
      "               0.3386324942111969,\n",
      "               -0.109697125852108,\n",
      "               0.022384531795978546,\n",
      "               2.1222987174987793,\n",
      "               0.47672367095947266,\n",
      "               -0.2258087545633316,\n",
      "               0.7937804460525513,\n",
      "               -0.012296981178224087,\n",
      "               0.5674686431884766,\n",
      "               -0.5327544808387756,\n",
      "               0.1518602967262268,\n",
      "               -0.16901926696300507,\n",
      "               -0.15129895508289337,\n",
      "               0.4590267539024353,\n",
      "               0.04460607096552849,\n",
      "               -0.5595721006393433,\n",
      "               -0.7420617938041687,\n",
      "               0.23713575303554535,\n",
      "               1.316455364227295,\n",
      "               -0.18631985783576965,\n",
      "               0.07232417911291122,\n",
      "               0.8031054139137268,\n",
      "               0.11829104274511337,\n",
      "               -0.11674701422452927,\n",
      "               1.0396736860275269,\n",
      "               -0.792576014995575,\n",
      "               0.6820218563079834,\n",
      "               -0.24722374975681305,\n",
      "               -0.04280253127217293,\n",
      "               -0.355999618768692,\n",
      "               0.20388557016849518,\n",
      "               -0.20922234654426575,\n",
      "               0.07263033837080002,\n",
      "               -0.2953645884990692,\n",
      "               -0.986359179019928,\n",
      "               -0.7032946348190308,\n",
      "               -1.2991671562194824,\n",
      "               -0.23880407214164734,\n",
      "               -0.3455839455127716,\n",
      "               0.24074968695640564,\n",
      "               -0.3063165843486786,\n",
      "               0.1415761560201645,\n",
      "               -0.3782195746898651,\n",
      "               0.51988285779953,\n",
      "               -0.8096199631690979,\n",
      "               -0.2066107541322708,\n",
      "               -0.15770678222179413,\n",
      "               1.6191929578781128,\n",
      "               -0.49466797709465027,\n",
      "               0.4882064163684845,\n",
      "               -0.7838966846466064,\n",
      "               -0.38742709159851074,\n",
      "               -0.36627715826034546,\n",
      "               0.4051612615585327,\n",
      "               -0.6943061947822571,\n",
      "               0.39465153217315674,\n",
      "               0.04683122783899307,\n",
      "               0.3502250015735626,\n",
      "               -0.38618040084838867,\n",
      "               0.07572633773088455,\n",
      "               -0.21424099802970886,\n",
      "               -0.20567218959331512,\n",
      "               0.365092933177948,\n",
      "               0.399214506149292,\n",
      "               0.20350757241249084,\n",
      "               -1.6090978384017944,\n",
      "               -0.6921907067298889,\n",
      "               0.15301209688186646,\n",
      "               -0.35869088768959045,\n",
      "               -0.9655185341835022,\n",
      "               0.9874845147132874,\n",
      "               -0.0729188323020935,\n",
      "               0.06958827376365662,\n",
      "               0.8596193194389343,\n",
      "               0.006880752742290497,\n",
      "               -0.20287100970745087,\n",
      "               0.8635373115539551,\n",
      "               -0.47695454955101013,\n",
      "               0.22661234438419342,\n",
      "               0.9510711431503296,\n",
      "               0.45784223079681396,\n",
      "               -0.10582685470581055,\n",
      "               0.5524535775184631,\n",
      "               0.20254039764404297,\n",
      "               -0.11273939162492752,\n",
      "               -0.914693295955658,\n",
      "               0.6895945072174072,\n",
      "               -0.016128988936543465,\n",
      "               -1.6407843828201294,\n",
      "               -0.3886379599571228,\n",
      "               -0.08327141404151917,\n",
      "               -1.196645975112915,\n",
      "               -0.35097166895866394,\n",
      "               -0.043187644332647324,\n",
      "               -0.6160760521888733,\n",
      "               -1.4917033910751343,\n",
      "               0.4550556540489197,\n",
      "               0.8796628713607788,\n",
      "               0.8343568444252014,\n",
      "               0.8471999168395996,\n",
      "               0.47377192974090576,\n",
      "               0.7881984114646912,\n",
      "               0.2182917296886444,\n",
      "               -0.42326685786247253,\n",
      "               -0.029124289751052856,\n",
      "               0.2001035511493683,\n",
      "               0.28566229343414307,\n",
      "               0.07870014756917953,\n",
      "               1.3138762712478638,\n",
      "               0.579440712928772,\n",
      "               -0.3452388644218445,\n",
      "               0.7316839098930359,\n",
      "               -0.20687709748744965,\n",
      "               0.7401608824729919,\n",
      "               1.3051117658615112,\n",
      "               -0.7484760284423828,\n",
      "               0.666804313659668,\n",
      "               -0.48791974782943726,\n",
      "               -0.2412915825843811,\n",
      "               -0.2580631971359253,\n",
      "               -0.2540338337421417,\n",
      "               0.7645546793937683,\n",
      "               -0.3810294568538666,\n",
      "               0.6788439154624939,\n",
      "               -0.4336298108100891,\n",
      "               -0.5245422720909119,\n",
      "               -0.17919600009918213,\n",
      "               -0.041806284338235855,\n",
      "               0.3458402752876282,\n",
      "               -0.590101420879364,\n",
      "               0.46870172023773193,\n",
      "               -0.9275110363960266,\n",
      "               0.39165160059928894,\n",
      "               -0.01575355790555477,\n",
      "               0.44866567850112915,\n",
      "               0.490174263715744,\n",
      "               -0.06335845589637756,\n",
      "               -0.6093703508377075,\n",
      "               -0.24420227110385895,\n",
      "               0.11456351727247238,\n",
      "               0.793527364730835,\n",
      "               0.5885174870491028,\n",
      "               -0.2923835217952728,\n",
      "               0.6403897404670715,\n",
      "               -0.12484091520309448,\n",
      "               -0.8001587986946106,\n",
      "               -0.3864738345146179,\n",
      "               0.3859090209007263,\n",
      "               -0.16374953091144562,\n",
      "               -0.17066338658332825,\n",
      "               0.885165274143219,\n",
      "               0.2666451036930084,\n",
      "               -0.8514155149459839,\n",
      "               -0.5538374185562134,\n",
      "               -0.7005472183227539,\n",
      "               1.1416038274765015,\n",
      "               -1.1268439292907715,\n",
      "               0.37561386823654175,\n",
      "               -0.6273121237754822,\n",
      "               -0.20126457512378693,\n",
      "               0.49704235792160034,\n",
      "               0.39291080832481384,\n",
      "               -0.9870092868804932,\n",
      "               -0.39892202615737915,\n",
      "               1.2982386350631714,\n",
      "               -0.10963893681764603,\n",
      "               0.2782130241394043,\n",
      "               -0.398682177066803,\n",
      "               1.095752477645874,\n",
      "               -0.2046891301870346,\n",
      "               -0.395475834608078,\n",
      "               0.4524279832839966,\n",
      "               -0.4641297459602356,\n",
      "               0.021806713193655014,\n",
      "               0.17263361811637878,\n",
      "               1.1038511991500854,\n",
      "               -0.7727354168891907,\n",
      "               0.054091937839984894,\n",
      "               0.21307653188705444,\n",
      "               1.3177499771118164,\n",
      "               -0.8753107190132141,\n",
      "               -0.869166910648346,\n",
      "               -0.3330548405647278,\n",
      "               0.6375837326049805,\n",
      "               -0.5036404132843018,\n",
      "               0.1283450424671173,\n",
      "               -0.09750258922576904,\n",
      "               1.0930719375610352,\n",
      "               0.5765106678009033,\n",
      "               -0.5100371837615967,\n",
      "               -0.2143775224685669,\n",
      "               -1.0576624870300293,\n",
      "               0.0021219290792942047,\n",
      "               -0.3336818814277649,\n",
      "               0.23895958065986633,\n",
      "               1.0276374816894531,\n",
      "               -0.8300528526306152,\n",
      "               -0.7191077470779419,\n",
      "               -0.3783474862575531,\n",
      "               0.05415954440832138,\n",
      "               -0.4739921987056732,\n",
      "               -0.3918622136116028,\n",
      "               -0.5661032795906067,\n",
      "               -0.10774365812540054,\n",
      "               -0.5741378664970398,\n",
      "               -0.3424457907676697,\n",
      "               0.4394993185997009,\n",
      "               0.6313785910606384,\n",
      "               -0.856218159198761,\n",
      "               0.03485976532101631,\n",
      "               -0.25576820969581604,\n",
      "               0.9128378033638,\n",
      "               1.440467357635498,\n",
      "               -0.43119990825653076,\n",
      "               0.45419472455978394,\n",
      "               0.4634329080581665,\n",
      "               0.24870194494724274,\n",
      "               -1.3336998224258423,\n",
      "               1.4853166341781616,\n",
      "               0.20135080814361572,\n",
      "               -0.08316290378570557,\n",
      "               0.35055622458457947,\n",
      "               -0.3615853190422058,\n",
      "               -0.6915519833564758,\n",
      "               0.1360863894224167,\n",
      "               0.8432243466377258,\n",
      "               0.6394553780555725,\n",
      "               -0.902729332447052,\n",
      "               0.42062532901763916,\n",
      "               -0.6657415628433228,\n",
      "               -0.42119666934013367,\n",
      "               -0.3486804962158203,\n",
      "               -0.8857949376106262,\n",
      "               -0.8728200197219849,\n",
      "               -0.7447062730789185,\n",
      "               -0.9283035397529602,\n",
      "               0.4717971384525299,\n",
      "               -0.33065885305404663,\n",
      "               0.4550776183605194,\n",
      "               -0.12710732221603394,\n",
      "               -0.7153914570808411,\n",
      "               0.6815975308418274,\n",
      "               0.5254205465316772,\n",
      "               -0.37854817509651184,\n",
      "               0.10450318455696106,\n",
      "               -1.5510880947113037,\n",
      "               -0.5000811219215393,\n",
      "               -1.0939741134643555,\n",
      "               -0.6162667274475098,\n",
      "               -0.26137059926986694,\n",
      "               0.3336821496486664,\n",
      "               0.13064280152320862,\n",
      "               -0.8748242855072021,\n",
      "               -0.25517910718917847,\n",
      "               0.5155229568481445,\n",
      "               0.8659865260124207,\n",
      "               0.8361302614212036,\n",
      "               0.4829832911491394,\n",
      "               -0.04516484588384628,\n",
      "               0.20797084271907806,\n",
      "               -0.726901113986969,\n",
      "               1.0197772979736328,\n",
      "               -0.19083662331104279,\n",
      "               -0.5709028840065002,\n",
      "               0.05402129143476486,\n",
      "               -0.304313063621521,\n",
      "               -0.25960689783096313,\n",
      "               -0.5781075954437256,\n",
      "               -0.7496992349624634,\n",
      "               -0.09268839657306671,\n",
      "               0.41372326016426086,\n",
      "               -0.5326465368270874,\n",
      "               0.7147708535194397,\n",
      "               0.14101390540599823,\n",
      "               0.4145263731479645,\n",
      "               0.149174302816391,\n",
      "               0.09434038400650024,\n",
      "               -0.22476954758167267,\n",
      "               -0.33282268047332764,\n",
      "               -0.15619277954101562,\n",
      "               0.10850619524717331,\n",
      "               -0.21195515990257263,\n",
      "               0.18098068237304688,\n",
      "               -0.021809224039316177,\n",
      "               0.5972816348075867,\n",
      "               -0.4181959629058838,\n",
      "               -0.5295480489730835,\n",
      "               -1.1612941026687622,\n",
      "               0.23978474736213684,\n",
      "               -0.3320610225200653,\n",
      "               -0.07696734368801117,\n",
      "               -0.19194084405899048,\n",
      "               -0.16550566256046295,\n",
      "               0.6033574938774109,\n",
      "               -0.5843963027000427,\n",
      "               -0.06549756973981857,\n",
      "               -0.4238905906677246,\n",
      "               1.0306874513626099,\n",
      "               -0.608191967010498,\n",
      "               0.5823947191238403,\n",
      "               0.08648059517145157,\n",
      "               -0.053555965423583984,\n",
      "               -0.5494234561920166,\n",
      "               -0.0606006383895874,\n",
      "               -0.1062202900648117,\n",
      "               0.7855398654937744,\n",
      "               -0.4380098581314087,\n",
      "               -0.8118072152137756,\n",
      "               0.4625541567802429,\n",
      "               0.07488000392913818,\n",
      "               -0.6106182932853699,\n",
      "               -0.28664058446884155,\n",
      "               0.08654800802469254,\n",
      "               -0.24287238717079163,\n",
      "               -0.23862530291080475,\n",
      "               0.6803103685379028,\n",
      "               1.295117735862732,\n",
      "               -0.343445360660553,\n",
      "               0.6154831647872925,\n",
      "               -0.18892697989940643,\n",
      "               -0.1412150114774704,\n",
      "               -0.18251074850559235,\n",
      "               -0.041890036314725876,\n",
      "               0.7982379198074341,\n",
      "               1.103078007698059,\n",
      "               0.5863146185874939,\n",
      "               -0.6330686211585999,\n",
      "               -0.8574750423431396,\n",
      "               -0.7526564002037048,\n",
      "               -0.12868352234363556,\n",
      "               -1.2735199928283691,\n",
      "               0.6870015859603882,\n",
      "               -0.43882763385772705,\n",
      "               -0.23413746058940887,\n",
      "               -0.812268853187561,\n",
      "               0.7892391681671143,\n",
      "               -0.3311558961868286,\n",
      "               -0.7400733828544617,\n",
      "               -0.2899267375469208,\n",
      "               -0.3092508912086487,\n",
      "               -0.29536131024360657,\n",
      "               0.8120574355125427,\n",
      "               -0.38419222831726074,\n",
      "               0.823623538017273,\n",
      "               0.18568366765975952,\n",
      "               0.9041916728019714,\n",
      "               -0.7385395765304565,\n",
      "               -0.16589359939098358,\n",
      "               0.13834087550640106,\n",
      "               -0.3937278687953949,\n",
      "               0.42287012934684753,\n",
      "               0.12163981050252914,\n",
      "               0.8492952585220337,\n",
      "               0.26221418380737305,\n",
      "               -1.3804184198379517,\n",
      "               -0.7848007082939148,\n",
      "               -0.6916130781173706,\n",
      "               -1.0319550037384033,\n",
      "               -0.8651708960533142,\n",
      "               -0.40426400303840637,\n",
      "               -1.017132043838501,\n",
      "               -0.13333262503147125,\n",
      "               0.07207933068275452,\n",
      "               0.6713741421699524,\n",
      "               0.23913592100143433,\n",
      "               0.17897330224514008,\n",
      "               0.3166232705116272,\n",
      "               0.24866417050361633,\n",
      "               -0.1794004738330841,\n",
      "               -0.007105030585080385,\n",
      "               -0.5265267491340637,\n",
      "               1.0267037153244019,\n",
      "               -0.5097006559371948,\n",
      "               -0.2532626688480377,\n",
      "               -0.2391899675130844,\n",
      "               0.04383464902639389,\n",
      "               -0.6563225388526917,\n",
      "               0.12211661785840988,\n",
      "               -1.2307285070419312,\n",
      "               -1.2831978797912598,\n",
      "               0.1581151783466339,\n",
      "               1.5513765811920166,\n",
      "               -0.6162447333335876,\n",
      "               -0.14645706117153168,\n",
      "               1.0536489486694336,\n",
      "               0.5499429702758789,\n",
      "               0.4278842508792877,\n",
      "               0.6571787595748901,\n",
      "               -0.7000760436058044,\n",
      "               0.09998222440481186,\n",
      "               -0.43240681290626526,\n",
      "               -0.571881890296936,\n",
      "               -0.29222816228866577,\n",
      "               -0.41817185282707214,\n",
      "               -0.4719918966293335,\n",
      "               -0.7352657318115234,\n",
      "               -0.5443745255470276,\n",
      "               0.03948626667261124,\n",
      "               0.868658185005188,\n",
      "               -0.8425562977790833,\n",
      "               -0.10804552584886551,\n",
      "               -0.3424266576766968,\n",
      "               -0.19866497814655304,\n",
      "               -1.7361700534820557,\n",
      "               0.8057288527488708,\n",
      "               -0.8887489438056946,\n",
      "               -1.0008444786071777,\n",
      "               0.3571227192878723,\n",
      "               -0.09989435225725174,\n",
      "               0.09343264251947403,\n",
      "               0.5154296159744263,\n",
      "               0.8378040194511414,\n",
      "               -0.8828262686729431,\n",
      "               0.18956413865089417,\n",
      "               -0.19100110232830048,\n",
      "               -0.6047572493553162,\n",
      "               -0.09437670558691025,\n",
      "               0.27686554193496704,\n",
      "               0.6796579957008362,\n",
      "               0.30342021584510803,\n",
      "               0.4321540594100952,\n",
      "               -0.6106857657432556,\n",
      "               -0.08450505137443542,\n",
      "               0.5355954170227051,\n",
      "               -0.7044835686683655,\n",
      "               0.2895523011684418,\n",
      "               -0.09006968885660172,\n",
      "               0.3846207559108734,\n",
      "               -0.04121464863419533,\n",
      "               -0.07651398330926895,\n",
      "               -0.6262636184692383,\n",
      "               0.05671820417046547,\n",
      "               -0.2574039399623871,\n",
      "               -0.7114493250846863,\n",
      "               -0.12151958793401718,\n",
      "               0.5646759271621704,\n",
      "               -0.18556977808475494,\n",
      "               -0.13286909461021423,\n",
      "               0.2849932014942169,\n",
      "               -0.11689618229866028,\n",
      "               -0.356029212474823,\n",
      "               0.1857115775346756,\n",
      "               -0.974502682685852,\n",
      "               -0.6029226779937744,\n",
      "               -0.6752908229827881,\n",
      "               -0.5340989828109741,\n",
      "               0.4589654803276062,\n",
      "               0.716723620891571,\n",
      "               -0.5096901059150696,\n",
      "               0.31758418679237366,\n",
      "               -0.2087915986776352,\n",
      "               0.29855597019195557,\n",
      "               0.7942979335784912,\n",
      "               0.3537490963935852,\n",
      "               0.08928324282169342,\n",
      "               0.6712331771850586,\n",
      "               -0.47418802976608276,\n",
      "               -0.8974087834358215,\n",
      "               -0.8946473598480225,\n",
      "               -0.5859742760658264,\n",
      "               0.18023374676704407,\n",
      "               -0.6226868033409119,\n",
      "               -0.38920846581459045,\n",
      "               -0.6030035018920898,\n",
      "               -0.4451647102832794,\n",
      "               0.4960143566131592,\n",
      "               -0.49871116876602173,\n",
      "               0.47518038749694824,\n",
      "               -0.35266047716140747,\n",
      "               -0.9432482719421387,\n",
      "               -0.4452489912509918,\n",
      "               -0.5345372557640076,\n",
      "               -0.20809993147850037,\n",
      "               0.16144953668117523,\n",
      "               -0.5395455956459045,\n",
      "               -0.6586838960647583,\n",
      "               0.9187489151954651,\n",
      "               -0.5352290272712708,\n",
      "               -0.3051629066467285,\n",
      "               -0.14184018969535828,\n",
      "               1.4292616844177246,\n",
      "               0.5239065885543823,\n",
      "               -0.19670289754867554,\n",
      "               -0.180014967918396,\n",
      "               0.09352228045463562,\n",
      "               -1.3114697933197021,\n",
      "               0.2764897644519806,\n",
      "               0.5369921922683716,\n",
      "               1.0999881029129028,\n",
      "               0.2649890184402466,\n",
      "               -0.6169208288192749,\n",
      "               -0.06193617731332779,\n",
      "               0.5385681986808777,\n",
      "               0.16684061288833618,\n",
      "               -0.05319827422499657,\n",
      "               -0.6788719296455383,\n",
      "               -0.1785469353199005,\n",
      "               -0.18755333125591278,\n",
      "               0.30895695090293884,\n",
      "               1.0165364742279053,\n",
      "               -0.4002378284931183,\n",
      "               -0.16545292735099792,\n",
      "               0.08250150829553604,\n",
      "               -0.3877876400947571,\n",
      "               -1.0547417402267456,\n",
      "               0.0022803794126957655,\n",
      "               -0.5094919204711914,\n",
      "               -0.022282248362898827,\n",
      "               -0.5057979226112366,\n",
      "               -1.0097589492797852,\n",
      "               -0.644874095916748,\n",
      "               0.7596234083175659,\n",
      "               -0.3421221077442169,\n",
      "               0.03263039141893387,\n",
      "               -0.4137461185455322,\n",
      "               1.088900089263916,\n",
      "               1.0719070434570312,\n",
      "               -1.5749725103378296,\n",
      "               0.6126213669776917,\n",
      "               0.029844820499420166,\n",
      "               -0.4909879267215729,\n",
      "               -0.46908751130104065,\n",
      "               -0.18851149082183838,\n",
      "               -1.1058205366134644,\n",
      "               -0.18621744215488434,\n",
      "               -0.7497823238372803,\n",
      "               -0.28913724422454834,\n",
      "               -0.6225278973579407,\n",
      "               -0.2223055362701416,\n",
      "               -0.06419842690229416,\n",
      "               0.9218245148658752,\n",
      "               0.6788337826728821,\n",
      "               0.013970054686069489,\n",
      "               -0.2843831777572632,\n",
      "               0.6985164284706116,\n",
      "               0.37397584319114685,\n",
      "               -0.011721453629434109,\n",
      "               -0.3359527289867401,\n",
      "               -0.43812137842178345,\n",
      "               0.85850989818573,\n",
      "               0.007967671379446983,\n",
      "               0.5833077430725098,\n",
      "               -0.5465825200080872,\n",
      "               0.08101876080036163,\n",
      "               0.00534507492557168,\n",
      "               0.06590408831834793,\n",
      "               -0.6117874979972839,\n",
      "               0.4893600642681122,\n",
      "               0.454020619392395,\n",
      "               0.8701596856117249,\n",
      "               -0.042853448539972305,\n",
      "               -0.008699794299900532,\n",
      "               0.7200814485549927,\n",
      "               -0.01604258269071579,\n",
      "               0.653635561466217,\n",
      "               0.756640613079071,\n",
      "               0.7231033444404602,\n",
      "               0.348919153213501,\n",
      "               0.7566232085227966,\n",
      "               -0.5842320919036865,\n",
      "               1.2058836221694946,\n",
      "               1.4891517162322998,\n",
      "               0.138410747051239,\n",
      "               -0.331512451171875,\n",
      "               0.3507765531539917,\n",
      "               0.7056322693824768,\n",
      "               0.17544233798980713,\n",
      "               -0.46352633833885193,\n",
      "               0.7959806323051453,\n",
      "               0.2971594035625458,\n",
      "               -0.5067993998527527,\n",
      "               -0.7161148190498352,\n",
      "               -0.11644888669252396,\n",
      "               0.21713042259216309,\n",
      "               -0.9205626845359802,\n",
      "               -0.20521432161331177,\n",
      "               0.4299813210964203,\n",
      "               -0.3976459205150604,\n",
      "               -0.44756069779396057,\n",
      "               -0.18900644779205322,\n",
      "               0.5387308597564697,\n",
      "               1.2567219734191895,\n",
      "               1.382169246673584,\n",
      "               0.19773802161216736,\n",
      "               0.007880192250013351,\n",
      "               0.3239043056964874,\n",
      "               -0.8613789677619934,\n",
      "               0.6921166777610779,\n",
      "               -0.5326367616653442,\n",
      "               -1.1651448011398315,\n",
      "               -1.2710479497909546,\n",
      "               -0.24647212028503418,\n",
      "               -0.5400263071060181,\n",
      "               0.20444586873054504,\n",
      "               0.272844523191452,\n",
      "               -0.3747330904006958,\n",
      "               -0.02119082771241665,\n",
      "               -0.2741425335407257,\n",
      "               0.18519701063632965,\n",
      "               -0.25860273838043213,\n",
      "               0.2827630341053009,\n",
      "               -0.26389333605766296,\n",
      "               1.2686902284622192,\n",
      "               -1.2226645946502686,\n",
      "               0.04036594554781914,\n",
      "               -0.6766870021820068,\n",
      "               0.5978862047195435,\n",
      "               0.18178439140319824,\n",
      "               0.24627190828323364,\n",
      "               0.10913266241550446,\n",
      "               0.3288564383983612,\n",
      "               1.6112873554229736,\n",
      "               -0.5231671929359436,\n",
      "               0.6361413598060608,\n",
      "               -0.2351669818162918,\n",
      "               -1.0436075925827026,\n",
      "               -0.11539804190397263,\n",
      "               0.3675985634326935,\n",
      "               -0.68697190284729,\n",
      "               -0.4061608910560608,\n",
      "               -0.028899317607283592,\n",
      "               -0.9981079697608948,\n",
      "               -0.2806629240512848,\n",
      "               -0.7787072062492371,\n",
      "               -0.13146688044071198,\n",
      "               -0.028316985815763474,\n",
      "               0.3748962879180908,\n",
      "               0.7700419425964355,\n",
      "               0.7469895482063293,\n",
      "               -1.014299988746643,\n",
      "               -0.5921362042427063,\n",
      "               -0.9552545547485352,\n",
      "               0.3457697033882141,\n",
      "               0.7267693877220154,\n",
      "               -0.01679885946214199,\n",
      "               -0.15741869807243347,\n",
      "               0.47799354791641235,\n",
      "               0.6042678952217102,\n",
      "               0.0687517523765564,\n",
      "               1.0663912296295166,\n",
      "               0.10798521339893341,\n",
      "               -0.7631306648254395,\n",
      "               -0.2501550614833832,\n",
      "               1.3962479829788208,\n",
      "               -1.2413239479064941,\n",
      "               1.1861836910247803,\n",
      "               0.2978432774543762,\n",
      "               0.2643023133277893,\n",
      "               0.022983035072684288,\n",
      "               -0.513460099697113,\n",
      "               -0.16971172392368317,\n",
      "               -0.8103477954864502,\n",
      "               0.7942071557044983,\n",
      "               -0.9528504610061646,\n",
      "               0.34188008308410645,\n",
      "               -0.8668632507324219,\n",
      "               1.2021592855453491,\n",
      "               1.0462194681167603,\n",
      "               -0.2584814429283142,\n",
      "               0.3800414204597473,\n",
      "               0.19594940543174744,\n",
      "               -0.27718639373779297,\n",
      "               -0.679633378982544,\n",
      "               -0.44762560725212097,\n",
      "               -0.9976599812507629,\n",
      "               0.01867794245481491],\n",
      " 'entities': [' CTC',\n",
      "              ' Gobardhan Sap Others',\n",
      "              ' High Court',\n",
      "              ' Ram Nath Sao Ram Sahu',\n",
      "              ' Subraniam',\n",
      "              ' Supreme Court',\n",
      "              ' Tamil Nadu Housing Board',\n",
      "              '12',\n",
      "              '1833',\n",
      "              '2018',\n",
      "              '3',\n",
      "              '31 31',\n",
      "              'Gobardhan Sap',\n",
      "              'MANUSC01352002 20023 SCC 195',\n",
      "              'Tamil Nadu Housing Board'],\n",
      " 'entity_details': [{'score': None,\n",
      "                     'text': 'MANUSC01352002 20023 SCC 195',\n",
      "                     'type': 'LAW'},\n",
      "                    {'score': None, 'text': 'Gobardhan Sap', 'type': 'PERSON'},\n",
      "                    {'score': None, 'text': '12', 'type': 'CARDINAL'},\n",
      "                    {'score': None, 'text': '1833', 'type': 'CARDINAL'},\n",
      "                    {'score': None, 'text': '2018', 'type': 'DATE'},\n",
      "                    {'score': None, 'text': '2018', 'type': 'DATE'},\n",
      "                    {'score': None,\n",
      "                     'text': 'Tamil Nadu Housing Board',\n",
      "                     'type': 'ORG'},\n",
      "                    {'score': None, 'text': '31 31', 'type': 'CARDINAL'},\n",
      "                    {'score': None, 'text': '3', 'type': 'CARDINAL'},\n",
      "                    {'score': np.float32(0.99815184),\n",
      "                     'text': ' Supreme Court',\n",
      "                     'type': 'ORG'},\n",
      "                    {'score': np.float32(0.9755476),\n",
      "                     'text': ' Ram Nath Sao Ram Sahu',\n",
      "                     'type': 'PER'},\n",
      "                    {'score': np.float32(0.89375305),\n",
      "                     'text': ' Gobardhan Sap Others',\n",
      "                     'type': 'PER'},\n",
      "                    {'score': np.float32(0.98408455),\n",
      "                     'text': ' High Court',\n",
      "                     'type': 'ORG'},\n",
      "                    {'score': np.float32(0.85458666),\n",
      "                     'text': ' CTC',\n",
      "                     'type': 'ORG'},\n",
      "                    {'score': np.float32(0.99498886),\n",
      "                     'text': ' Subraniam',\n",
      "                     'type': 'PER'},\n",
      "                    {'score': np.float32(0.9985092),\n",
      "                     'text': ' Tamil Nadu Housing Board',\n",
      "                     'type': 'ORG'}],\n",
      " 'entity_types': ['LAW', 'PERSON', 'CARDINAL', 'DATE', 'ORG', 'PER'],\n",
      " 'filename': 'c:\\\\New folder '\n",
      "             '(5)\\\\new-search-models\\\\notebooks\\\\..\\\\data\\\\chunks\\\\MHC_CaseStatus_511605_chunk20.txt',\n",
      " 'intent': 'case_status',\n",
      " 'intent_confidence': 0.8062899112701416,\n",
      " 'keyword': ['apis',\n",
      "             'authentication',\n",
      "             'api',\n",
      "             'aws',\n",
      "             'jwt',\n",
      "             'cloud',\n",
      "             'workflows',\n",
      "             'angular',\n",
      "             'secure',\n",
      "             'java',\n",
      "             'management',\n",
      "             'developed',\n",
      "             'user',\n",
      "             'document',\n",
      "             'spring',\n",
      "             'tournament',\n",
      "             'implemented',\n",
      "             'security',\n",
      "             'high-volume applications',\n",
      "             'user authentication',\n",
      "             'unauthorized access functionality',\n",
      "             'large-scale data management',\n",
      "             '50 satisfaction scores',\n",
      "             'live score updates',\n",
      "             'user interaction',\n",
      "             'seamless transactions',\n",
      "             'a secure api',\n",
      "             'scalable applications'],\n",
      " 'summary': '',\n",
      " 'text': 'showing mala fide or deliberate delay as a dilatory tactic the court '\n",
      "         'should normally condone the delay Underlining is ours 10In a recent '\n",
      "         'Judgment of the Honourable Supreme Court reported in MANUSC01352002 '\n",
      "         '20023 SCC 195 2002-3-LW417 Ram Nath Sao Ram Sahu Others versus '\n",
      "         'Gobardhan Sap Others the position has been succinctly set out in '\n",
      "         'para 12 which reads as under 12 Acceptance of explanation furnished '\n",
      "         'should be the rule and refusal an exception more so when no '\n",
      "         'negligence or inaction or want of bona fides can be imputed to the '\n",
      "         'defaulting party On the other hand while considering the matter the '\n",
      "         'courts should not lose sight of the fact that by not taking steps '\n",
      "         'within the time prescribed a valuable right 1833 httpwwwjudisnicin '\n",
      "         'CMPNo6648 of 2018 in ASSRNo19304 of 2018 has accrued to the other '\n",
      "         'party which should not be lightly defeated by condoning delay in a '\n",
      "         'routine-like manner Underlining is ours 11 In the Division Bench '\n",
      "         'Judgment of our High Court in the Judgment reported in '\n",
      "         'MANUTN02522000 2000 3 CTC 727 2000 3 LW 938 C Subraniam versus Tamil '\n",
      "         'Nadu Housing Board rep by its Chairman And Managing Director the '\n",
      "         'position has been stated as under in para 31 31 To turn up the legal '\n",
      "         'position 1 the work sufficient cause should receive liberal '\n",
      "         'construction to do substantial justice 2 what is sufficient cause is '\n",
      "         'a question of fact in a given circumstances of the case 3 it is '\n",
      "         'axiomatic that condonation of delay is discretion of the Court 4 '\n",
      "         'length of delay is no matter but acceptability of the explanation is '\n",
      "         'the only criterion 5 once the Court accepts the explanation as '\n",
      "         'sufficient it is the result of positive exercise of discretion and '\n",
      "         'normally the superior court should not disturb in such finding '\n",
      "         'unless the discretion was exercised'}\n"
     ]
    }
   ],
   "source": [
    "# Utility: Deduplicate entity_types in structured metadata output\n",
    "\n",
    "def get_metadata_structured_dedup(\n",
    "    text, \n",
    "    filename=\"\", \n",
    "    document_name=\"\", \n",
    "    summary=\"\", \n",
    "    embedding=None\n",
    "):\n",
    "    keywords = unique_keywords\n",
    "    intent, intent_confidence = predict_intent(text)\n",
    "    ner_results = extract_entities_hybrid(text)\n",
    "    entities = ner_results.get(\"entities\", [])\n",
    "    entity_types = list(dict.fromkeys(ner_results.get(\"entity_types\", [])))  # Deduplicate, preserve order\n",
    "    entity_details = ner_results.get(\"entity_details\", [])\n",
    "    if embedding is None:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=64)\n",
    "        device = next(model.parameters()).device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model.distilbert(**inputs)\n",
    "            embedding = outputs.last_hidden_state[:, 0, :].squeeze().cpu().tolist()\n",
    "    return {\n",
    "        \"keyword\": keywords,\n",
    "        \"intent\": intent,\n",
    "        \"intent_confidence\": intent_confidence,\n",
    "        \"entities\": entities,\n",
    "        \"entity_types\": entity_types,\n",
    "        \"entity_details\": entity_details,\n",
    "        \"summary\": summary,\n",
    "        \"embedding\": embedding,\n",
    "        \"text\": text,\n",
    "        \"document_name\": document_name,\n",
    "        \"filename\": filename\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "metadata_structured_dedup = get_metadata_structured_dedup(\n",
    "    text=chunk_text,\n",
    "    filename=chunk_path,\n",
    "    document_name=\"Jimson_Ratnam_JavaFullStackDeveloper_2+years\",\n",
    "    summary=\"\",\n",
    ")\n",
    "import pprint\n",
    "pprint.pprint(metadata_structured_dedup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12046d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "search_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
