{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a09ded39",
   "metadata": {},
   "source": [
    "# üîç Metadata Extraction Test Notebook\n",
    "This notebook helps you test the `extract_metadata()` pipeline using a custom text chunk input.\n",
    "\n",
    "You can modify the `text` variable in the next cell to test different examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1d0a35ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\New folder (5)\\new-search-models\\search_env\\lib\\site-packages\\thinc\\shims\\pytorch.py:261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(filelike, map_location=device))\n",
      "2025-06-22 11:55:24,699 INFO Use pytorch device_name: cuda:0\n",
      "2025-06-22 11:55:24,700 INFO Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Load spaCy model\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_trf\")\n",
    "except Exception:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Helper: normalize entity\n",
    "def normalize_entity(e):\n",
    "    text = re.sub(r'\\s+', ' ', re.sub(r'\\.', '', e.lower())).strip()\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([token.lemma_ for token in doc])\n",
    "\n",
    "# Intent keywords and examples (copy from your scripts)\n",
    "intent_keywords = {\n",
    "    \"claim_process\": [\"claim\", \"process\", \"file\", \"submit\", \"insurance\"],\n",
    "    \"case_status\": [\"status\", \"case\", \"update\", \"progress\", \"judgement\", \"order\", \"appeal\"],\n",
    "    \"document_request\": [\"document\", \"request\", \"copies\", \"forms\"],\n",
    "    \"technical_support\": [\"error\", \"issue\", \"problem\", \"technical\"],\n",
    "    \"general_info\": [\"information\", \"contact\", \"hours\", \"location\"],\n",
    "    \"resume_info\": [\n",
    "        \"skills\", \"resume\", \"cv\", \"proficiencies\", \"abilities\", \"expertise\", \"competencies\", \"qualifications\",\n",
    "        \"experience\", \"work\", \"education\", \"background\", \"certifications\", \"projects\", \"programming\", \"languages\",\n",
    "        \"achievements\", \"awards\", \"contact\", \"career\", \"summary\", \"tools\", \"technologies\", \"roles\", \"responsibilities\",\n",
    "        \"soft skills\", \"applicant\", \"candidate\", \"developer\", \"engineer\", \"profile\", \"professional\", \"employment\", \"history\",\n",
    "        \"management\", \"software\", \"admin\", \"implementation\", \"tracking\", \"project\", \"system\", \"solution\", \"platform\", \"application\",\n",
    "        \"team\", \"lead\", \"player\", \"restocking\", \"inventory\", \"tournament\", \"manual\", \"implemented\"\n",
    "    ]\n",
    "}\n",
    "project_root = os.environ.get('PROJECT_ROOT', os.getcwd())\n",
    "intent_examples_path = os.path.join(project_root,'..','data', 'intent_categories', 'intent_examples.json')\n",
    "with open(intent_examples_path, 'r', encoding='utf-8') as f:\n",
    "    intent_examples = json.load(f)\n",
    "# intent_examples = {\n",
    "#     \"resume_info\": [\n",
    "#         \"What skills are listed in the resume?\",\n",
    "#         \"Show me the proficiencies in this CV.\",\n",
    "#         \"List the abilities mentioned in the candidate's resume.\",\n",
    "#         \"What expertise does the applicant have?\",\n",
    "#         \"Which competencies are present in the resume?\",\n",
    "#         \"What qualifications are included in the CV?\",\n",
    "#         \"List the work experience of the candidate.\",\n",
    "#         \"What is the educational background of the applicant?\",\n",
    "#         \"Show me the certifications in this resume.\",\n",
    "#         \"What are the technical skills mentioned?\",\n",
    "#         \"Summarize the professional experience section.\",\n",
    "#         \"What projects has the candidate worked on?\",\n",
    "#         \"List the programming languages known by the applicant.\",\n",
    "#         \"What are the achievements or awards?\",\n",
    "#         \"Show me the contact information in the resume.\",\n",
    "#         \"What is the career objective or summary?\",\n",
    "#         \"List the tools and technologies used by the candidate.\",\n",
    "#         \"What is the total experience in years?\",\n",
    "#         \"Show me the roles and responsibilities held.\",\n",
    "#         \"What are the soft skills mentioned?\",\n",
    "#         \"List the languages spoken by the applicant.\",\n",
    "#         \"What is the applicant's job title?\",\n",
    "#         \"Describe the applicant's professional profile.\",\n",
    "#         \"What companies has the candidate worked for?\",\n",
    "#         \"List the frameworks and libraries used.\",\n",
    "#         \"What cloud platforms does the candidate have experience with?\",\n",
    "#         \"What development methodologies are mentioned?\",\n",
    "#         \"List the certifications and licenses.\",\n",
    "#         \"What leadership roles has the candidate held?\",\n",
    "#         \"Summarize the applicant's employment history.\",\n",
    "#         \"What is the candidate's GitHub or portfolio link?\",\n",
    "#         \"Describe the candidate's experience in management and software projects.\",\n",
    "#         \"What inventory or tracking systems has the applicant implemented?\",\n",
    "#         \"List any admin or manual processes managed by the candidate.\",\n",
    "#         \"What experience does the candidate have with tournaments or players?\",\n",
    "#         \"Describe the candidate's role in restocking or inventory management.\",\n",
    "#         \"What solutions or platforms has the applicant developed or led?\",\n",
    "#         \"List any applications or systems the candidate has worked on.\",\n",
    "#         \"What teams has the candidate led or been a part of?\",\n",
    "#         \"Describe the candidate's experience with project implementation.\"\n",
    "#     ],\n",
    "#     \"claim_process\": [\"How do I file a claim?\", \"What is the process for submitting an insurance claim?\"],\n",
    "#     \"case_status\": [\"What is the current status of the case?\", \"Show me the progress of case number 511605.\"],\n",
    "#     \"document_request\": [\"Can I get a copy of the case order?\", \"How do I request the judgment document?\"],\n",
    "#     \"technical_support\": [\"I have a technical issue.\", \"There is a problem with the system.\"],\n",
    "#     \"general_info\": [\"What is the purpose of this document?\", \"Give me a summary of the file.\"]\n",
    "# }\n",
    "intent_model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "def get_intent(text):\n",
    "    doc = nlp(text.lower())\n",
    "    tokens = [token.lemma_ for token in doc]\n",
    "    detected_intent = None\n",
    "    max_matches = 0\n",
    "    for intent, keywords in intent_keywords.items():\n",
    "        matches = sum(kw in tokens for kw in keywords)\n",
    "        if matches > max_matches:\n",
    "            max_matches = matches\n",
    "            detected_intent = intent\n",
    "    intent_confidence = max_matches / max(1, len(intent_keywords.get(detected_intent, []))) if detected_intent else 0.0\n",
    "    if not detected_intent or max_matches == 0:\n",
    "        query_emb = intent_model.encode(text, convert_to_tensor=True)\n",
    "        best_intent, best_score = None, 0\n",
    "        for intent, examples in intent_examples.items():\n",
    "            example_embs = intent_model.encode(examples, convert_to_tensor=True)\n",
    "            scores = util.pytorch_cos_sim(query_emb, example_embs)\n",
    "            max_score = scores.max().item()\n",
    "            if max_score > best_score:\n",
    "                best_score = max_score\n",
    "                best_intent = intent\n",
    "        if best_score > 0.35:\n",
    "            detected_intent = best_intent\n",
    "            intent_confidence = best_score\n",
    "    if not detected_intent:\n",
    "        detected_intent = \"general_info\"\n",
    "        intent_confidence = 0.0\n",
    "    return detected_intent, intent_confidence, None\n",
    "\n",
    "def extract_metadata(text):\n",
    "    doc = nlp(text)\n",
    "    # Entities\n",
    "    entities = [normalize_entity(ent.text) for ent in doc.ents]\n",
    "    # Keywords (nouns, proper nouns, not stopwords)\n",
    "    keywords = [normalize_entity(token.text) for token in doc if token.pos_ in [\"NOUN\", \"PROPN\"] and not token.is_stop and token.lemma_.lower() not in ENGLISH_STOP_WORDS and len(token.text) > 2]\n",
    "    # Intent\n",
    "    detected_intent, intent_confidence, _ = get_intent(text)\n",
    "    return {\n",
    "        \"entities\": entities,\n",
    "        \"keywords\": keywords,\n",
    "        \"intent\": detected_intent,\n",
    "        \"intent_confidence\": intent_confidence\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f8da8c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy version: 3.8.7\n",
      "SentenceTransformers version: 4.1.0\n",
      "Transformers version: 4.52.4\n",
      "spaCy model: core_web_trf\n",
      "Intent keywords: {'claim_process': ['claim', 'process', 'file', 'submit', 'insurance'], 'case_status': ['status', 'case', 'update', 'progress', 'judgement', 'order', 'appeal'], 'document_request': ['document', 'request', 'copies', 'forms'], 'technical_support': ['error', 'issue', 'problem', 'technical'], 'general_info': ['information', 'contact', 'hours', 'location'], 'resume_info': ['skills', 'resume', 'cv', 'proficiencies', 'abilities', 'expertise', 'competencies', 'qualifications', 'experience', 'work', 'education', 'background', 'certifications', 'projects', 'programming', 'languages', 'achievements', 'awards', 'contact', 'career', 'summary', 'tools', 'technologies', 'roles', 'responsibilities', 'soft skills', 'applicant', 'candidate', 'developer', 'engineer', 'profile', 'professional', 'employment', 'history', 'management', 'software', 'admin', 'implementation', 'tracking', 'project', 'system', 'solution', 'platform', 'application', 'team', 'lead', 'player', 'restocking', 'inventory', 'tournament', 'manual', 'implemented']}\n",
      "Intent examples: ['general_info', 'resume_info', 'case_status', 'court_details', 'party_information', 'hearing_information', 'document_request']\n",
      "Text sample: 'Java Full Stack Developer 2023 - Present Devzen Software Solutions Developed secure JWT-based authentication verification APIs using Spring Boot Spring Security Built batch processing workflows with Spring Batch for large-scale data management Designed REST APIs for real-time inventory tracking alerts optimizing stock management Integrated Elasticsearch for efficient search retrieval in high-volume applications Developed dynamic responsive UI components using Angular TypeScript for product manag'\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import sentence_transformers\n",
    "import transformers\n",
    "import spacy\n",
    "\n",
    "# Print environment and config for debugging\n",
    "try:\n",
    "    print('spaCy version:', spacy.__version__)\n",
    "except Exception:\n",
    "    print('spaCy not available')\n",
    "try:\n",
    "    print('SentenceTransformers version:', sentence_transformers.__version__)\n",
    "except Exception:\n",
    "    print('SentenceTransformers not available')\n",
    "try:\n",
    "    print('Transformers version:', transformers.__version__)\n",
    "except Exception:\n",
    "    print('Transformers not available')\n",
    "try:\n",
    "    print('spaCy model:', nlp.meta['name'] if nlp else 'None')\n",
    "except Exception:\n",
    "    print('spaCy model: None')\n",
    "try:\n",
    "    print('Intent keywords:', intent_keywords)\n",
    "except Exception:\n",
    "    print('Intent keywords: not loaded')\n",
    "try:\n",
    "    print('Intent examples:', list(intent_examples.keys()))\n",
    "except Exception:\n",
    "    print('Intent examples: not loaded')\n",
    "if 'text' in globals():\n",
    "    print('Text sample:', repr(text[:500]))\n",
    "else:\n",
    "    print('No text loaded yet.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60d5d43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\new folder (5)\\new-search-models\\search_env\\lib\\site-packages (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "c:\\New folder (5)\\new-search-models\\search_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\New folder (5)\\new-search-models\\search_env\\lib\\site-packages\\thinc\\shims\\pytorch.py:261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(filelike, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv\n",
    "\n",
    "import sys\n",
    "import os, json, yaml\n",
    "\n",
    "project_root = 'c:\\\\New folder (5)\\\\new-search-models'\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "# Load config.yaml using the correct path\n",
    "config_path = os.path.join(project_root,  'config.yaml')\n",
    "if os.path.exists(config_path):\n",
    "\twith open(config_path, 'r') as f:\n",
    "\t\tconfig = yaml.safe_load(f)\n",
    "else:\n",
    "\tprint(f\"‚ö†Ô∏è config.yaml not found at {config_path}\")\n",
    "\tconfig = {}\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(os.path.join(project_root, 'config', '.env'))\n",
    "from scripts.entity_utils import normalize_entity, get_spacy_nlp\n",
    "from scripts.search_pipeline import get_openai_embedding\n",
    "from scripts.intent_utils import get_intent\n",
    "# from scripts.metadata import extract_metadata  # <-- You must have your full extraction code in this file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68df9c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Sample Text Loaded\n"
     ]
    }
   ],
   "source": [
    "with open(r'C:\\New folder (5)\\new-search-models\\data\\chunks\\Jimson_Ratnam_JavaFullStackDeveloper_2+years_chunk1.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "print('‚úÖ Sample Text Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "29006c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entities': ['devzen',\n",
       "  'spring boot spring security',\n",
       "  'spring batch',\n",
       "  'elasticsearch',\n",
       "  'angular typescript',\n",
       "  'spring boot angular',\n",
       "  'aw ec2 s3 rd iam',\n",
       "  'spring boot',\n",
       "  'angular',\n",
       "  '25',\n",
       "  '50',\n",
       "  'portal',\n",
       "  '20',\n",
       "  'six month of launch',\n",
       "  '40',\n",
       "  '90'],\n",
       " 'keywords': ['java',\n",
       "  'stack',\n",
       "  'developer',\n",
       "  'devzen',\n",
       "  'software',\n",
       "  'solution',\n",
       "  'jwt',\n",
       "  'authentication',\n",
       "  'verification',\n",
       "  'apis',\n",
       "  'spring',\n",
       "  'boot',\n",
       "  'spring',\n",
       "  'security',\n",
       "  'batch',\n",
       "  'processing',\n",
       "  'workflow',\n",
       "  'spring',\n",
       "  'batch',\n",
       "  'scale',\n",
       "  'datum',\n",
       "  'management',\n",
       "  'rest',\n",
       "  'apis',\n",
       "  'time',\n",
       "  'inventory',\n",
       "  'tracking',\n",
       "  'alert',\n",
       "  'stock',\n",
       "  'management',\n",
       "  'elasticsearch',\n",
       "  'search',\n",
       "  'retrieval',\n",
       "  'volume',\n",
       "  'application',\n",
       "  'component',\n",
       "  'angular',\n",
       "  'typescript',\n",
       "  'product',\n",
       "  'management',\n",
       "  'payment',\n",
       "  'gateway',\n",
       "  'integration',\n",
       "  'spring',\n",
       "  'boot',\n",
       "  'angular',\n",
       "  'transaction',\n",
       "  'dashboard',\n",
       "  'key',\n",
       "  'insight',\n",
       "  'tournament',\n",
       "  'inventory',\n",
       "  'management',\n",
       "  'application',\n",
       "  'docker',\n",
       "  'consistency',\n",
       "  'development',\n",
       "  'production',\n",
       "  'environment',\n",
       "  'cicd',\n",
       "  'pipeline',\n",
       "  'build',\n",
       "  'test',\n",
       "  'deployment',\n",
       "  'process',\n",
       "  'github',\n",
       "  'action',\n",
       "  'workflow',\n",
       "  'deployment',\n",
       "  'efficiency',\n",
       "  'application',\n",
       "  'aws',\n",
       "  'ec2',\n",
       "  'rds',\n",
       "  'iam',\n",
       "  'cloud',\n",
       "  'hosting',\n",
       "  'management',\n",
       "  'apis',\n",
       "  'spring',\n",
       "  'boot',\n",
       "  'document',\n",
       "  'management',\n",
       "  'user',\n",
       "  'authentication',\n",
       "  'security',\n",
       "  'efficiency',\n",
       "  'angular',\n",
       "  'user',\n",
       "  'experience',\n",
       "  'document',\n",
       "  'creation',\n",
       "  'socialization',\n",
       "  'publication',\n",
       "  'key',\n",
       "  'achievement',\n",
       "  'security',\n",
       "  'user',\n",
       "  'interaction',\n",
       "  'angular',\n",
       "  'api',\n",
       "  'user',\n",
       "  'authentication',\n",
       "  'verification',\n",
       "  'process',\n",
       "  'document',\n",
       "  'management',\n",
       "  'search',\n",
       "  'security',\n",
       "  'access',\n",
       "  'functionality',\n",
       "  'improvement',\n",
       "  'user',\n",
       "  'incident',\n",
       "  'satisfaction',\n",
       "  'score',\n",
       "  'document',\n",
       "  'management',\n",
       "  'improve',\n",
       "  'tournament',\n",
       "  'efficiency',\n",
       "  'feature',\n",
       "  'document',\n",
       "  'creation',\n",
       "  'end',\n",
       "  'end',\n",
       "  'platform',\n",
       "  'socialization',\n",
       "  'publication',\n",
       "  'portal',\n",
       "  'user',\n",
       "  'player',\n",
       "  'registration',\n",
       "  'tournament',\n",
       "  'scheduling',\n",
       "  'base',\n",
       "  'month',\n",
       "  'launch',\n",
       "  'administration',\n",
       "  'time',\n",
       "  'tournament',\n",
       "  'management',\n",
       "  'process',\n",
       "  'admin',\n",
       "  'control',\n",
       "  'time',\n",
       "  'scoring',\n",
       "  'score',\n",
       "  'update',\n",
       "  'player',\n",
       "  'admin',\n",
       "  'dashboard',\n",
       "  'spectator',\n",
       "  'score',\n",
       "  'update',\n",
       "  'delay',\n",
       "  'tournament',\n",
       "  'oversight'],\n",
       " 'intent': 'resume_info',\n",
       " 'intent_confidence': 0.28846153846153844}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = extract_metadata(text)\n",
    "\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7b04ab4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extracted Metadata:\n",
      "{'entities': ['devzen',\n",
      "              'spring boot spring security',\n",
      "              'spring batch',\n",
      "              'elasticsearch',\n",
      "              'angular typescript',\n",
      "              'spring boot angular',\n",
      "              'aw ec2 s3 rd iam',\n",
      "              'spring boot',\n",
      "              'angular',\n",
      "              '25',\n",
      "              '50',\n",
      "              'portal',\n",
      "              '20',\n",
      "              'six month of launch',\n",
      "              '40',\n",
      "              '90'],\n",
      " 'intent': 'resume_info',\n",
      " 'intent_confidence': 0.28846153846153844,\n",
      " 'keywords': ['java',\n",
      "              'stack',\n",
      "              'developer',\n",
      "              'devzen',\n",
      "              'software',\n",
      "              'solution',\n",
      "              'jwt',\n",
      "              'authentication',\n",
      "              'verification',\n",
      "              'apis',\n",
      "              'spring',\n",
      "              'boot',\n",
      "              'spring',\n",
      "              'security',\n",
      "              'batch',\n",
      "              'processing',\n",
      "              'workflow',\n",
      "              'spring',\n",
      "              'batch',\n",
      "              'scale',\n",
      "              'datum',\n",
      "              'management',\n",
      "              'rest',\n",
      "              'apis',\n",
      "              'time',\n",
      "              'inventory',\n",
      "              'tracking',\n",
      "              'alert',\n",
      "              'stock',\n",
      "              'management',\n",
      "              'elasticsearch',\n",
      "              'search',\n",
      "              'retrieval',\n",
      "              'volume',\n",
      "              'application',\n",
      "              'component',\n",
      "              'angular',\n",
      "              'typescript',\n",
      "              'product',\n",
      "              'management',\n",
      "              'payment',\n",
      "              'gateway',\n",
      "              'integration',\n",
      "              'spring',\n",
      "              'boot',\n",
      "              'angular',\n",
      "              'transaction',\n",
      "              'dashboard',\n",
      "              'key',\n",
      "              'insight',\n",
      "              'tournament',\n",
      "              'inventory',\n",
      "              'management',\n",
      "              'application',\n",
      "              'docker',\n",
      "              'consistency',\n",
      "              'development',\n",
      "              'production',\n",
      "              'environment',\n",
      "              'cicd',\n",
      "              'pipeline',\n",
      "              'build',\n",
      "              'test',\n",
      "              'deployment',\n",
      "              'process',\n",
      "              'github',\n",
      "              'action',\n",
      "              'workflow',\n",
      "              'deployment',\n",
      "              'efficiency',\n",
      "              'application',\n",
      "              'aws',\n",
      "              'ec2',\n",
      "              'rds',\n",
      "              'iam',\n",
      "              'cloud',\n",
      "              'hosting',\n",
      "              'management',\n",
      "              'apis',\n",
      "              'spring',\n",
      "              'boot',\n",
      "              'document',\n",
      "              'management',\n",
      "              'user',\n",
      "              'authentication',\n",
      "              'security',\n",
      "              'efficiency',\n",
      "              'angular',\n",
      "              'user',\n",
      "              'experience',\n",
      "              'document',\n",
      "              'creation',\n",
      "              'socialization',\n",
      "              'publication',\n",
      "              'key',\n",
      "              'achievement',\n",
      "              'security',\n",
      "              'user',\n",
      "              'interaction',\n",
      "              'angular',\n",
      "              'api',\n",
      "              'user',\n",
      "              'authentication',\n",
      "              'verification',\n",
      "              'process',\n",
      "              'document',\n",
      "              'management',\n",
      "              'search',\n",
      "              'security',\n",
      "              'access',\n",
      "              'functionality',\n",
      "              'improvement',\n",
      "              'user',\n",
      "              'incident',\n",
      "              'satisfaction',\n",
      "              'score',\n",
      "              'document',\n",
      "              'management',\n",
      "              'improve',\n",
      "              'tournament',\n",
      "              'efficiency',\n",
      "              'feature',\n",
      "              'document',\n",
      "              'creation',\n",
      "              'end',\n",
      "              'end',\n",
      "              'platform',\n",
      "              'socialization',\n",
      "              'publication',\n",
      "              'portal',\n",
      "              'user',\n",
      "              'player',\n",
      "              'registration',\n",
      "              'tournament',\n",
      "              'scheduling',\n",
      "              'base',\n",
      "              'month',\n",
      "              'launch',\n",
      "              'administration',\n",
      "              'time',\n",
      "              'tournament',\n",
      "              'management',\n",
      "              'process',\n",
      "              'admin',\n",
      "              'control',\n",
      "              'time',\n",
      "              'scoring',\n",
      "              'score',\n",
      "              'update',\n",
      "              'player',\n",
      "              'admin',\n",
      "              'dashboard',\n",
      "              'spectator',\n",
      "              'score',\n",
      "              'update',\n",
      "              'delay',\n",
      "              'tournament',\n",
      "              'oversight']}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "print('‚úÖ Extracted Metadata:')\n",
    "pprint(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "278cad73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Simple Keyword Matching ==\n",
      "Intent: resume_info, Confidence: 0.29\n",
      "\n",
      "== Embedding Similarity ==\n",
      "Intent: resume_info, Confidence: 0.29\n",
      "\n",
      "== Embedding Similarity ==\n",
      "Intent: resume_info, Similarity: 0.41\n",
      "\n",
      "== Regex-based Detection ==\n",
      "Intent: resume_info, Confidence: 1.00\n",
      "\n",
      "== Majority Voting ==\n",
      "Intent: resume_info, Confidence: 1.00\n",
      "\n",
      "== Bag-of-Words Cosine Similarity ==\n",
      "Intent: resume_info, Similarity: 0.32\n",
      "Intent: resume_info, Similarity: 0.41\n",
      "\n",
      "== Regex-based Detection ==\n",
      "Intent: resume_info, Confidence: 1.00\n",
      "\n",
      "== Majority Voting ==\n",
      "Intent: resume_info, Confidence: 1.00\n",
      "\n",
      "== Bag-of-Words Cosine Similarity ==\n",
      "Intent: resume_info, Similarity: 0.32\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Test alternative intent detection techniques\n",
    "\n",
    "# 1. Simple keyword matching (baseline)\n",
    "def detect_intent_keywords(text, intent_keywords):\n",
    "    doc = nlp(text.lower())\n",
    "    tokens = [token.lemma_ for token in doc]\n",
    "    scores = {}\n",
    "    for intent, keywords in intent_keywords.items():\n",
    "        matches = sum(kw in tokens for kw in keywords)\n",
    "        scores[intent] = matches\n",
    "    best_intent = max(scores, key=scores.get)\n",
    "    confidence = scores[best_intent] / max(1, len(intent_keywords[best_intent]))\n",
    "    return best_intent, confidence\n",
    "\n",
    "# 2. Embedding similarity (SentenceTransformer, already used in get_intent)\n",
    "def detect_intent_embedding(text, intent_examples, intent_model):\n",
    "    query_emb = intent_model.encode(text, convert_to_tensor=True)\n",
    "    best_intent, best_score = None, 0\n",
    "    for intent, examples in intent_examples.items():\n",
    "        example_embs = intent_model.encode(examples, convert_to_tensor=True)\n",
    "        scores = util.pytorch_cos_sim(query_emb, example_embs)\n",
    "        max_score = scores.max().item()\n",
    "        if max_score > best_score:\n",
    "            best_score = max_score\n",
    "            best_intent = intent\n",
    "    return best_intent, best_score\n",
    "\n",
    "# 3. Regex-based intent detection (very basic)\n",
    "def detect_intent_regex(text):\n",
    "    patterns = {\n",
    "        \"resume_info\": r\"\\b(resume|cv|skills|experience|project|education|certification)\\b\",\n",
    "        \"claim_process\": r\"\\b(claim|insurance|submit|file)\\b\",\n",
    "        \"case_status\": r\"\\b(status|case|update|progress|judgement|order|appeal)\\b\",\n",
    "        \"document_request\": r\"\\b(document|request|copy|copies|form)\\b\",\n",
    "        \"technical_support\": r\"\\b(error|issue|problem|technical)\\b\",\n",
    "        \"general_info\": r\"\\b(information|contact|hours|location|summary|purpose)\\b\"\n",
    "    }\n",
    "    for intent, pattern in patterns.items():\n",
    "        if re.search(pattern, text, re.IGNORECASE):\n",
    "            return intent, 1.0\n",
    "    return \"general_info\", 0.0\n",
    "\n",
    "# Run all techniques on the loaded text\n",
    "print(\"== Simple Keyword Matching ==\")\n",
    "intent_kw, conf_kw = detect_intent_keywords(text, intent_keywords)\n",
    "print(f\"Intent: {intent_kw}, Confidence: {conf_kw:.2f}\")\n",
    "\n",
    "print(\"\\n== Embedding Similarity ==\")\n",
    "intent_emb, conf_emb = detect_intent_embedding(text, intent_examples, intent_model)\n",
    "print(f\"Intent: {intent_emb}, Similarity: {conf_emb:.2f}\")\n",
    "\n",
    "print(\"\\n== Regex-based Detection ==\")\n",
    "intent_rgx, conf_rgx = detect_intent_regex(text)\n",
    "print(f\"Intent: {intent_rgx}, Confidence: {conf_rgx:.2f}\")\n",
    "# 4. Majority voting among techniques\n",
    "\n",
    "def majority_vote(*intents):\n",
    "    count = Counter(intents)\n",
    "    best, freq = count.most_common(1)[0]\n",
    "    return best, freq / len(intents)\n",
    "\n",
    "intents = [intent_kw, intent_emb, intent_rgx]\n",
    "majority_intent, majority_conf = majority_vote(*intents)\n",
    "print(\"\\n== Majority Voting ==\")\n",
    "print(f\"Intent: {majority_intent}, Confidence: {majority_conf:.2f}\")\n",
    "\n",
    "# 5. Bag-of-words cosine similarity (very basic)\n",
    "\n",
    "def detect_intent_bow(text, intent_examples):\n",
    "    vectorizer = CountVectorizer().fit([text] + [ex for exs in intent_examples.values() for ex in exs])\n",
    "    text_vec = vectorizer.transform([text])\n",
    "    best_intent, best_score = None, 0\n",
    "    for intent, examples in intent_examples.items():\n",
    "        ex_vecs = vectorizer.transform(examples)\n",
    "        sim = cosine_similarity(text_vec, ex_vecs).max()\n",
    "        if sim > best_score:\n",
    "            best_score = sim\n",
    "            best_intent = intent\n",
    "    return best_intent, best_score\n",
    "\n",
    "intent_bow, conf_bow = detect_intent_bow(text, intent_examples)\n",
    "print(\"\\n== Bag-of-Words Cosine Similarity ==\")\n",
    "print(f\"Intent: {intent_bow}, Similarity: {conf_bow:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70297b57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9ad41c6",
   "metadata": {},
   "source": [
    "## using both regex and the Transformers NER pipeline, along with KeyBERT for keyword extraction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87e3eeab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# --- Alternative Metadata Extraction: Regex + Transformers NER ---\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\n",
    "from keybert import KeyBERT\n",
    "import re\n",
    "\n",
    "# Load NER pipeline (Roberta large NER)\n",
    "ner_pipe = pipeline(\n",
    "    \"ner\",\n",
    "    model=AutoModelForTokenClassification.from_pretrained(\"Jean-Baptiste/roberta-large-ner-english\"),\n",
    "    tokenizer=AutoTokenizer.from_pretrained(\"Jean-Baptiste/roberta-large-ner-english\"),\n",
    "    aggregation_strategy=\"simple\",\n",
    "    device=-1  # CPU\n",
    ")\n",
    "\n",
    "# KeyBERT for keyword extraction\n",
    "keyword_model = KeyBERT()\n",
    "\n",
    "def extract_metadata_alt(text):\n",
    "    # Entities using transformers NER\n",
    "    entities = set()\n",
    "    for ent in ner_pipe(text):\n",
    "        if ent['score'] > 0.8:\n",
    "            entities.add(ent['word'].strip().lower())\n",
    "    # Regex for names (simple)\n",
    "    name_matches = re.findall(r'([A-Z][a-z]+(?: [A-Z][a-z]+)+)', text)\n",
    "    for name in name_matches:\n",
    "        entities.add(name.lower())\n",
    "    # Keywords using KeyBERT\n",
    "    keywords = [kw for kw, _ in keyword_model.extract_keywords(text, top_n=10)]\n",
    "    # Intent using previous get_intent\n",
    "    detected_intent, intent_confidence, _ = get_intent(text)\n",
    "    return {\n",
    "        \"entities\": sorted(list(entities)),\n",
    "        \"keywords\": keywords,\n",
    "        \"intent\": detected_intent,\n",
    "        \"intent_confidence\": intent_confidence\n",
    "    }\n",
    "\n",
    "# alt_metadata = extract_metadata_alt(text)\n",
    "# print('‚úÖ Alternative Metadata Extraction:')\n",
    "# from pprint import pprint\n",
    "# pprint(alt_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac0ca12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c87473e",
   "metadata": {},
   "source": [
    "## üöÄ Production-Grade Intent Detection Solutions\n",
    "\n",
    "For real-world, scalable, and robust intent detection, consider these best practices:\n",
    "\n",
    "- **Hybrid Pipeline:** Combine fast rule-based/keyword/regex checks for high-precision intents with embedding-based similarity for flexible, robust matching.\n",
    "- **ML/Deep Learning Models:** Use fine-tuned transformer models (e.g., BERT, RoBERTa, DistilBERT) for intent classification if you have enough labeled data.\n",
    "- **Fallbacks:** Always provide a fallback (e.g., \"general_info\") for ambiguous or low-confidence cases.\n",
    "- **Confidence Thresholds:** Use thresholds to decide when to trust a prediction or escalate to a human/manual review.\n",
    "- **Monitoring & Logging:** Log predictions, confidence, and input for continuous improvement and error analysis.\n",
    "- **Versioning:** Version your models, configs, and intent definitions for reproducibility and safe updates.\n",
    "- **Batch & Real-Time Support:** Design your pipeline to work both in batch (offline) and real-time (API) modes.\n",
    "\n",
    "Below is a modular, production-ready intent detection pipeline you can adapt and extend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8e8fca9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-22 12:08:29,087 INFO Intent: resume_info, Confidence: 0.29, Text: Java Full Stack Developer 2023 - Present Devzen Software Solutions Developed sec...\n",
      "2025-06-22 12:08:29,277 INFO Intent: resume_info, Confidence: 0.21, Text: reducing base by 20 within six months of launch manual administration time by 40...\n",
      "2025-06-22 12:08:29,277 INFO Intent: resume_info, Confidence: 0.21, Text: reducing base by 20 within six months of launch manual administration time by 40...\n",
      "2025-06-22 12:08:29,758 INFO Intent: resume_info, Confidence: 0.25, Text: JIMSON RATNAM KAPAVARAPU Java Fullstack Developer 91 9154631932 jimsonjimmy2008g...\n",
      "2025-06-22 12:08:29,758 INFO Intent: resume_info, Confidence: 0.25, Text: JIMSON RATNAM KAPAVARAPU Java Fullstack Developer 91 9154631932 jimsonjimmy2008g...\n",
      "2025-06-22 12:08:30,121 INFO Intent: resume_info, Confidence: 0.21, Text: real-time updates for matches fixtures Implemented secure role-based access cont...\n",
      "2025-06-22 12:08:30,121 INFO Intent: resume_info, Confidence: 0.21, Text: real-time updates for matches fixtures Implemented secure role-based access cont...\n",
      "2025-06-22 12:08:30,691 INFO Intent: case_status, Confidence: 0.43, Text: CMPNo6648 of 2018 in ASSRNo19304 of 2018 IN THE HIGH COURT OF JUDICATURE AT MADR...\n",
      "2025-06-22 12:08:30,691 INFO Intent: case_status, Confidence: 0.43, Text: CMPNo6648 of 2018 in ASSRNo19304 of 2018 IN THE HIGH COURT OF JUDICATURE AT MADR...\n",
      "2025-06-22 12:08:31,156 INFO Intent: claim_process, Confidence: 0.20, Text: not open to any litigant to fix his own period of limitation for instituting pro...\n",
      "2025-06-22 12:08:31,156 INFO Intent: claim_process, Confidence: 0.20, Text: not open to any litigant to fix his own period of limitation for instituting pro...\n",
      "2025-06-22 12:08:31,609 INFO Intent: claim_process, Confidence: 0.40, Text: various decisions that sufficient cause appearing in Section 5 of the Limitation...\n",
      "2025-06-22 12:08:31,609 INFO Intent: claim_process, Confidence: 0.40, Text: various decisions that sufficient cause appearing in Section 5 of the Limitation...\n",
      "2025-06-22 12:08:32,056 INFO Intent: case_status, Confidence: 0.29, Text: prejudice to the plaintiffDecree holder who has been pursuing the money suit for...\n",
      "2025-06-22 12:08:32,056 INFO Intent: case_status, Confidence: 0.29, Text: prejudice to the plaintiffDecree holder who has been pursuing the money suit for...\n",
      "2025-06-22 12:08:32,523 INFO Intent: case_status, Confidence: 0.29, Text: of the property the petitioner therein filed the application to condone the dela...\n",
      "2025-06-22 12:08:32,523 INFO Intent: case_status, Confidence: 0.29, Text: of the property the petitioner therein filed the application to condone the dela...\n",
      "2025-06-22 12:08:33,042 INFO Intent: claim_process, Confidence: 0.40, Text: In the instant case it is clear that the PetitionerJudgment Debtor has deliberat...\n",
      "2025-06-22 12:08:33,042 INFO Intent: claim_process, Confidence: 0.40, Text: In the instant case it is clear that the PetitionerJudgment Debtor has deliberat...\n",
      "2025-06-22 12:08:33,291 INFO Intent: claim_process, Confidence: 0.40, Text: and another 2012 3 SCC 563 the Honourable Supreme Court while dismissing the app...\n",
      "2025-06-22 12:08:33,291 INFO Intent: claim_process, Confidence: 0.40, Text: and another 2012 3 SCC 563 the Honourable Supreme Court while dismissing the app...\n",
      "2025-06-22 12:08:33,732 INFO Intent: claim_process, Confidence: 0.20, Text: by the decree-holder by filing a petition before the Court below and full satisf...\n",
      "2025-06-22 12:08:33,732 INFO Intent: claim_process, Confidence: 0.20, Text: by the decree-holder by filing a petition before the Court below and full satisf...\n",
      "2025-06-22 12:08:34,196 INFO Intent: claim_process, Confidence: 0.40, Text: has filed any explanation for not applying the certified copy within the prescri...\n",
      "2025-06-22 12:08:34,196 INFO Intent: claim_process, Confidence: 0.40, Text: has filed any explanation for not applying the certified copy within the prescri...\n",
      "2025-06-22 12:08:34,661 INFO Intent: claim_process, Confidence: 0.60, Text: to be adopted to advance substantial justice we are of the view that in the fact...\n",
      "2025-06-22 12:08:34,661 INFO Intent: claim_process, Confidence: 0.60, Text: to be adopted to advance substantial justice we are of the view that in the fact...\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.13it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.13it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 55.53it/s]\n",
      "Batches:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 33.89it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 33.89it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 62.43it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 62.43it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.66it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 71.41it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 71.41it/s]\n",
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 62.44it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 62.44it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.69it/s]\n",
      "2025-06-22 12:08:35,670 INFO Intent: case_status, Confidence: 0.39, Text: of delay there can be some lapse on the part of the litigant concerned That alon...\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.69it/s]\n",
      "2025-06-22 12:08:35,670 INFO Intent: case_status, Confidence: 0.39, Text: of delay there can be some lapse on the part of the litigant concerned That alon...\n",
      "2025-06-22 12:08:36,123 INFO Intent: case_status, Confidence: 0.14, Text: Thus exercise of discretionary power is to be condoned cautiously and only in th...\n",
      "2025-06-22 12:08:36,123 INFO Intent: case_status, Confidence: 0.14, Text: Thus exercise of discretionary power is to be condoned cautiously and only in th...\n",
      "2025-06-22 12:08:36,560 INFO Intent: case_status, Confidence: 0.14, Text: showing mala fide or deliberate delay as a dilatory tactic the court should norm...\n",
      "2025-06-22 12:08:36,560 INFO Intent: case_status, Confidence: 0.14, Text: showing mala fide or deliberate delay as a dilatory tactic the court should norm...\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 43.47it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 43.47it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 50.01it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 50.01it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 40.89it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 40.89it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 62.50it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 62.50it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 58.84it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 58.84it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 55.55it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 55.55it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 55.56it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 55.56it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.66it/s]\n",
      "2025-06-22 12:08:37,250 INFO Intent: case_status, Confidence: 0.42, Text: 4 length of delay is no matter but acceptability of the explanation is the only ...\n",
      "\n",
      "2025-06-22 12:08:37,250 INFO Intent: case_status, Confidence: 0.42, Text: 4 length of delay is no matter but acceptability of the explanation is the only ...\n",
      "2025-06-22 12:08:37,694 INFO Intent: case_status, Confidence: 0.14, Text: he then was in paragraphs 14 and 17 which read as under 14 If a litigant chooses...\n",
      "2025-06-22 12:08:37,694 INFO Intent: case_status, Confidence: 0.14, Text: he then was in paragraphs 14 and 17 which read as under 14 If a litigant chooses...\n",
      "2025-06-22 12:08:38,158 INFO Intent: case_status, Confidence: 0.29, Text: On a conspectus reading of the above principles set out in the various judgments...\n",
      "2025-06-22 12:08:38,158 INFO Intent: case_status, Confidence: 0.29, Text: On a conspectus reading of the above principles set out in the various judgments...\n",
      "2025-06-22 12:08:38,261 INFO Intent: case_status, Confidence: 0.14, Text: application for condonation of delay in such cases no indulgence should be shown...\n",
      "2025-06-22 12:08:38,261 INFO Intent: case_status, Confidence: 0.14, Text: application for condonation of delay in such cases no indulgence should be shown...\n",
      "2025-06-22 12:08:38,705 INFO Intent: case_status, Confidence: 0.29, Text: Thus the Courts have taken a clear view that the intention of the parties in fil...\n",
      "2025-06-22 12:08:38,705 INFO Intent: case_status, Confidence: 0.29, Text: Thus the Courts have taken a clear view that the intention of the parties in fil...\n",
      "2025-06-22 12:08:38,943 INFO Intent: case_status, Confidence: 0.14, Text: a Judge is required to maintain whilst adjudicating any lis between the parties ...\n",
      "2025-06-22 12:08:38,943 INFO Intent: case_status, Confidence: 0.14, Text: a Judge is required to maintain whilst adjudicating any lis between the parties ...\n",
      "2025-06-22 12:08:39,396 INFO Intent: claim_process, Confidence: 0.40, Text: that the question of limitation is not merely a technical consideration Rules of...\n",
      "2025-06-22 12:08:39,396 INFO Intent: claim_process, Confidence: 0.40, Text: that the question of limitation is not merely a technical consideration Rules of...\n",
      "2025-06-22 12:08:39,856 INFO Intent: case_status, Confidence: 0.14, Text: it is for the general welfare that a period be put to litigation Rules of limita...\n",
      "2025-06-22 12:08:39,856 INFO Intent: case_status, Confidence: 0.14, Text: it is for the general welfare that a period be put to litigation Rules of limita...\n",
      "2025-06-22 12:08:40,053 INFO Intent: claim_process, Confidence: 0.20, Text: all relevant facts and it is at this stage the diligence of the party of its bon...\n",
      "2025-06-22 12:08:40,053 INFO Intent: claim_process, Confidence: 0.20, Text: all relevant facts and it is at this stage the diligence of the party of its bon...\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 62.50it/s]\n",
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 20.83it/s]\n",
      "Batches:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 10.58it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 10.58it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 52.62it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 52.62it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.66it/s]\n",
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.68it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.68it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 43.47it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 43.47it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.60it/s]\n",
      "2025-06-22 12:08:40,567 INFO Intent: case_status, Confidence: 0.44, Text: this nature for condonation of delay it is well settled that length of delay is ...\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.60it/s]\n",
      "2025-06-22 12:08:40,567 INFO Intent: case_status, Confidence: 0.44, Text: this nature for condonation of delay it is well settled that length of delay is ...\n",
      "2025-06-22 12:08:41,008 INFO Intent: case_status, Confidence: 0.29, Text: or it is not put forth as part of dilatory tactics the court must show utmost co...\n",
      "2025-06-22 12:08:41,008 INFO Intent: case_status, Confidence: 0.29, Text: or it is not put forth as part of dilatory tactics the court must show utmost co...\n",
      "2025-06-22 12:08:41,463 INFO Intent: technical_support, Confidence: 0.25, Text: and are to be applied in proper perspective to the obtaining factsituation iii S...\n",
      "2025-06-22 12:08:41,463 INFO Intent: technical_support, Confidence: 0.25, Text: and are to be applied in proper perspective to the obtaining factsituation iii S...\n",
      "2025-06-22 12:08:41,916 INFO Intent: resume_info, Confidence: 0.04, Text: of both parties and the said principle cannot be given a total go by in the name...\n",
      "2025-06-22 12:08:41,916 INFO Intent: resume_info, Confidence: 0.04, Text: of both parties and the said principle cannot be given a total go by in the name...\n",
      "2025-06-22 12:08:42,359 INFO Intent: case_status, Confidence: 0.43, Text: regard being had to the concept of judicial discretion yet a conscious effort fo...\n",
      "2025-06-22 12:08:42,359 INFO Intent: case_status, Confidence: 0.43, Text: regard being had to the concept of judicial discretion yet a conscious effort fo...\n",
      "2025-06-22 12:08:42,712 INFO Intent: case_status, Confidence: 0.29, Text: limitation has got a specific purpose and object then the power of discretion is...\n",
      "2025-06-22 12:08:42,712 INFO Intent: case_status, Confidence: 0.29, Text: limitation has got a specific purpose and object then the power of discretion is...\n",
      "2025-06-22 12:08:43,165 INFO Intent: claim_process, Confidence: 0.20, Text: no valid reason for the purpose of condoning such an enormous delay of more than...\n",
      "2025-06-22 12:08:43,165 INFO Intent: claim_process, Confidence: 0.20, Text: no valid reason for the purpose of condoning such an enormous delay of more than...\n",
      "2025-06-22 12:08:43,625 INFO Intent: claim_process, Confidence: 0.20, Text: by exercising the power of discretion by the Courts It is also settled that when...\n",
      "2025-06-22 12:08:43,625 INFO Intent: claim_process, Confidence: 0.20, Text: by exercising the power of discretion by the Courts It is also settled that when...\n",
      "2025-06-22 12:08:44,119 INFO Intent: claim_process, Confidence: 0.20, Text: for the revision petitioner has relied on the following decisions- i In Kandaswa...\n",
      "2025-06-22 12:08:44,119 INFO Intent: claim_process, Confidence: 0.20, Text: for the revision petitioner has relied on the following decisions- i In Kandaswa...\n",
      "2025-06-22 12:08:44,604 INFO Intent: case_status, Confidence: 0.14, Text: Once it is held that a party has lost his right to have the matter considered on...\n",
      "2025-06-22 12:08:44,604 INFO Intent: case_status, Confidence: 0.14, Text: Once it is held that a party has lost his right to have the matter considered on...\n",
      "2025-06-22 12:08:44,726 INFO Intent: case_status, Confidence: 0.14, Text: that the party who failed to approach the Court within the time stipulated comes...\n",
      "2025-06-22 12:08:44,726 INFO Intent: case_status, Confidence: 0.14, Text: that the party who failed to approach the Court within the time stipulated comes...\n",
      "2025-06-22 12:08:45,234 INFO Intent: case_status, Confidence: 0.14, Text: 2001 6 Supreme Court Cases 176 which was relied on by the learned counsel for th...\n",
      "2025-06-22 12:08:45,234 INFO Intent: case_status, Confidence: 0.14, Text: 2001 6 Supreme Court Cases 176 which was relied on by the learned counsel for th...\n",
      "2025-06-22 12:08:45,785 INFO Intent: claim_process, Confidence: 0.20, Text: WPNo20999 of 2003 IN THE HIGH COURT OF JUDICATURE AT MADRAS Reserved on 30102018...\n",
      "2025-06-22 12:08:45,785 INFO Intent: claim_process, Confidence: 0.20, Text: WPNo20999 of 2003 IN THE HIGH COURT OF JUDICATURE AT MADRAS Reserved on 30102018...\n",
      "2025-06-22 12:08:46,249 INFO Intent: claim_process, Confidence: 0.40, Text: The learned standing counsel would therefore submit that the limitation for the ...\n",
      "2025-06-22 12:08:46,249 INFO Intent: claim_process, Confidence: 0.40, Text: The learned standing counsel would therefore submit that the limitation for the ...\n",
      "2025-06-22 12:08:46,352 INFO Intent: case_status, Confidence: 0.29, Text: the writ jurisdiction the said allegation of violation of principles of natural ...\n",
      "2025-06-22 12:08:46,352 INFO Intent: case_status, Confidence: 0.29, Text: the writ jurisdiction the said allegation of violation of principles of natural ...\n",
      "2025-06-22 12:08:46,809 INFO Intent: claim_process, Confidence: 0.40, Text: II1 v Rakesh Sarin reported in 2014 222 Taxman 84 Madras 2014 362 ITR 619 Madras...\n",
      "2025-06-22 12:08:46,809 INFO Intent: claim_process, Confidence: 0.40, Text: II1 v Rakesh Sarin reported in 2014 222 Taxman 84 Madras 2014 362 ITR 619 Madras...\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.62it/s]\n",
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  7.81it/s]\n",
      "Batches:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 43.48it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 43.48it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 71.44it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 71.44it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 71.44it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 71.44it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 71.44it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.68it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.68it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.68it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.68it/s]\n",
      "2025-06-22 12:08:47,385 INFO Intent: party_information, Confidence: 0.47, Text: appellate remedy available under the Act to the petitioner however since the wri...\n",
      "\n",
      "2025-06-22 12:08:47,385 INFO Intent: party_information, Confidence: 0.47, Text: appellate remedy available under the Act to the petitioner however since the wri...\n",
      "2025-06-22 12:08:47,835 INFO Intent: claim_process, Confidence: 0.40, Text: premises it was concluded on the same day itself However in so far as the search...\n",
      "2025-06-22 12:08:47,835 INFO Intent: claim_process, Confidence: 0.40, Text: premises it was concluded on the same day itself However in so far as the search...\n",
      "2025-06-22 12:08:48,294 INFO Intent: claim_process, Confidence: 0.20, Text: concerned it has been specifically averred in the counter affidavit filed by the...\n",
      "2025-06-22 12:08:48,294 INFO Intent: claim_process, Confidence: 0.20, Text: concerned it has been specifically averred in the counter affidavit filed by the...\n",
      "2025-06-22 12:08:48,755 INFO Intent: case_status, Confidence: 0.29, Text: return in Form 2B on 23102001 I state that the notice us 1432 of the Income Tax ...\n",
      "2025-06-22 12:08:48,755 INFO Intent: case_status, Confidence: 0.29, Text: return in Form 2B on 23102001 I state that the notice us 1432 of the Income Tax ...\n",
      "2025-06-22 12:08:49,197 INFO Intent: claim_process, Confidence: 0.40, Text: on the basis of oral requirement is factually incorrect 36 The allegation that a...\n",
      "2025-06-22 12:08:49,197 INFO Intent: claim_process, Confidence: 0.40, Text: on the basis of oral requirement is factually incorrect 36 The allegation that a...\n",
      "2025-06-22 12:08:49,650 INFO Intent: claim_process, Confidence: 0.40, Text: claim that the last panchanama was drawn only on 12062001 and therefore the limi...\n",
      "2025-06-22 12:08:49,650 INFO Intent: claim_process, Confidence: 0.40, Text: claim that the last panchanama was drawn only on 12062001 and therefore the limi...\n",
      "2025-06-22 12:08:50,112 INFO Intent: case_status, Confidence: 0.29, Text: months of February to June each time when the panchanamas were drawn as a proof ...\n",
      "2025-06-22 12:08:50,112 INFO Intent: case_status, Confidence: 0.29, Text: months of February to June each time when the panchanamas were drawn as a proof ...\n",
      "2025-06-22 12:08:50,572 INFO Intent: case_status, Confidence: 0.14, Text: for the sake of conveniencewho was a medical practitioner and also he was doing ...\n",
      "2025-06-22 12:08:50,572 INFO Intent: case_status, Confidence: 0.14, Text: for the sake of conveniencewho was a medical practitioner and also he was doing ...\n",
      "2025-06-22 12:08:51,021 INFO Intent: case_status, Confidence: 0.29, Text: the case may be was executed in cases where a search is initiated or books of ac...\n",
      "2025-06-22 12:08:51,021 INFO Intent: case_status, Confidence: 0.29, Text: the case may be was executed in cases where a search is initiated or books of ac...\n",
      "2025-06-22 12:08:51,466 INFO Intent: case_status, Confidence: 0.29, Text: in 2011 339 ITR 210 Karnataka 2011 244 CTR 2460 httpwwwjudisnicin WPNo20999 of 2...\n",
      "2025-06-22 12:08:51,466 INFO Intent: case_status, Confidence: 0.29, Text: in 2011 339 ITR 210 Karnataka 2011 244 CTR 2460 httpwwwjudisnicin WPNo20999 of 2...\n",
      "2025-06-22 12:08:51,916 INFO Intent: case_status, Confidence: 0.14, Text: defined in Explanation 2 to section 158BE For the purpose of limitation there ca...\n",
      "2025-06-22 12:08:51,916 INFO Intent: case_status, Confidence: 0.14, Text: defined in Explanation 2 to section 158BE For the purpose of limitation there ca...\n",
      "2025-06-22 12:08:52,410 INFO Intent: case_status, Confidence: 0.29, Text: executed on different dates also Then the doubt would arise regarding which auth...\n",
      "2025-06-22 12:08:52,410 INFO Intent: case_status, Confidence: 0.29, Text: executed on different dates also Then the doubt would arise regarding which auth...\n",
      "2025-06-22 12:08:52,855 INFO Intent: case_status, Confidence: 0.14, Text: The main attribute of the panchnama is stated to be that it should record the co...\n",
      "2025-06-22 12:08:52,855 INFO Intent: case_status, Confidence: 0.14, Text: The main attribute of the panchnama is stated to be that it should record the co...\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.63it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.63it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.35it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.35it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 40.82it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 40.82it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 58.83it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 58.83it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.68it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.68it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 62.52it/s]\n",
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 58.84it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 58.84it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 58.83it/s]\n",
      "2025-06-22 12:08:53,598 INFO Intent: hearing_information, Confidence: 0.39, Text: WPNo20999 of 2003 assessment proceedings But such a panchnama would not extend t...\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 58.83it/s]\n",
      "2025-06-22 12:08:53,598 INFO Intent: hearing_information, Confidence: 0.39, Text: WPNo20999 of 2003 assessment proceedings But such a panchnama would not extend t...\n",
      "2025-06-22 12:08:54,062 INFO Intent: case_status, Confidence: 0.14, Text: issue of more than one authorisation Supposingly two authorisations are issued o...\n",
      "2025-06-22 12:08:54,062 INFO Intent: case_status, Confidence: 0.14, Text: issue of more than one authorisation Supposingly two authorisations are issued o...\n",
      "2025-06-22 12:08:54,507 INFO Intent: claim_process, Confidence: 0.20, Text: 23 1998 see 1999 235 ITR St 35 refers to this dilemma faced by the Department 12...\n",
      "2025-06-22 12:08:54,507 INFO Intent: claim_process, Confidence: 0.20, Text: 23 1998 see 1999 235 ITR St 35 refers to this dilemma faced by the Department 12...\n",
      "2025-06-22 12:08:54,987 INFO Intent: case_status, Confidence: 0.29, Text: detailed discussion as is done in the following paras The question arises as to ...\n",
      "2025-06-22 12:08:54,987 INFO Intent: case_status, Confidence: 0.29, Text: detailed discussion as is done in the following paras The question arises as to ...\n",
      "2025-06-22 12:08:55,459 INFO Intent: technical_support, Confidence: 0.25, Text: counting limitation of one or two years under section 158BE Otherwise it may lea...\n",
      "2025-06-22 12:08:55,459 INFO Intent: technical_support, Confidence: 0.25, Text: counting limitation of one or two years under section 158BE Otherwise it may lea...\n",
      "2025-06-22 12:08:55,920 INFO Intent: case_status, Confidence: 0.29, Text: May and June 2001 and last such search operations was completed on 12062001 and ...\n",
      "2025-06-22 12:08:55,920 INFO Intent: case_status, Confidence: 0.29, Text: May and June 2001 and last such search operations was completed on 12062001 and ...\n",
      "2025-06-22 12:08:56,385 INFO Intent: case_status, Confidence: 0.29, Text: more than one authorisation is issued on different dates the last panchnama draw...\n",
      "2025-06-22 12:08:56,385 INFO Intent: case_status, Confidence: 0.29, Text: more than one authorisation is issued on different dates the last panchnama draw...\n",
      "2025-06-22 12:08:56,841 INFO Intent: case_status, Confidence: 0.29, Text: account or other documents or any assets are requisitioned on or after the 1st d...\n",
      "2025-06-22 12:08:56,841 INFO Intent: case_status, Confidence: 0.29, Text: account or other documents or any assets are requisitioned on or after the 1st d...\n",
      "2025-06-22 12:08:57,306 INFO Intent: case_status, Confidence: 0.14, Text: as an authorisation to enter the premises and inspect the materials which are th...\n",
      "2025-06-22 12:08:57,306 INFO Intent: case_status, Confidence: 0.14, Text: as an authorisation to enter the premises and inspect the materials which are th...\n",
      "2025-06-22 12:08:57,798 INFO Intent: case_status, Confidence: 0.29, Text: inspect the materials which are the subject-matter of prohibitory order or restr...\n",
      "2025-06-22 12:08:57,798 INFO Intent: case_status, Confidence: 0.29, Text: inspect the materials which are the subject-matter of prohibitory order or restr...\n",
      "2025-06-22 12:08:58,320 INFO Intent: case_status, Confidence: 0.14, Text: validity of the authorisation issued for the search Similarly in circumstances n...\n",
      "2025-06-22 12:08:58,320 INFO Intent: case_status, Confidence: 0.14, Text: validity of the authorisation issued for the search Similarly in circumstances n...\n",
      "2025-06-22 12:08:58,780 INFO Intent: case_status, Confidence: 0.29, Text: to find out is there any 4160 httpwwwjudisnicin WPNo20999 of 2003 incriminating ...\n",
      "2025-06-22 12:08:58,780 INFO Intent: case_status, Confidence: 0.29, Text: to find out is there any 4160 httpwwwjudisnicin WPNo20999 of 2003 incriminating ...\n",
      "2025-06-22 12:08:59,228 INFO Intent: case_status, Confidence: 0.29, Text: These two decisions have been heavily relied upon by the learned counsel appeari...\n",
      "2025-06-22 12:08:59,228 INFO Intent: case_status, Confidence: 0.29, Text: These two decisions have been heavily relied upon by the learned counsel appeari...\n",
      "2025-06-22 12:08:59,332 INFO Intent: case_status, Confidence: 0.14, Text: the last authorisation shall be treated as the last authorisation for the purpos...\n",
      "2025-06-22 12:08:59,332 INFO Intent: case_status, Confidence: 0.14, Text: the last authorisation shall be treated as the last authorisation for the purpos...\n",
      "2025-06-22 12:08:59,796 INFO Intent: case_status, Confidence: 0.43, Text: 16 Going by the facts herein viz as to the search completed on 13122001 with dra...\n",
      "2025-06-22 12:08:59,796 INFO Intent: case_status, Confidence: 0.43, Text: 16 Going by the facts herein viz as to the search completed on 13122001 with dra...\n",
      "2025-06-22 12:09:00,268 INFO Intent: case_status, Confidence: 0.29, Text: Section 158BE of the Act is given as 2 years from the end of the 4560 httpwwwjud...\n",
      "2025-06-22 12:09:00,268 INFO Intent: case_status, Confidence: 0.29, Text: Section 158BE of the Act is given as 2 years from the end of the 4560 httpwwwjud...\n",
      "2025-06-22 12:09:00,728 INFO Intent: case_status, Confidence: 0.43, Text: been obtained under Section 158BG of the Act and there had been a violation of p...\n",
      "2025-06-22 12:09:00,728 INFO Intent: case_status, Confidence: 0.43, Text: been obtained under Section 158BG of the Act and there had been a violation of p...\n",
      "2025-06-22 12:09:01,176 INFO Intent: case_status, Confidence: 0.29, Text: pm In terms 4660 httpwwwjudisnicin WPNo20999 of 2003 of the panchanama the searc...\n",
      "2025-06-22 12:09:01,176 INFO Intent: case_status, Confidence: 0.29, Text: pm In terms 4660 httpwwwjudisnicin WPNo20999 of 2003 of the panchanama the searc...\n",
      "2025-06-22 12:09:01,633 INFO Intent: resume_info, Confidence: 0.04, Text: as the date where the last panchanama was drawn if the Revenue had any case what...\n",
      "2025-06-22 12:09:01,633 INFO Intent: resume_info, Confidence: 0.04, Text: as the date where the last panchanama was drawn if the Revenue had any case what...\n",
      "2025-06-22 12:09:02,093 INFO Intent: case_status, Confidence: 0.43, Text: in fitness of things fresh authorisation was issued by the Department on 2708200...\n",
      "2025-06-22 12:09:02,093 INFO Intent: case_status, Confidence: 0.43, Text: in fitness of things fresh authorisation was issued by the Department on 2708200...\n",
      "2025-06-22 12:09:02,546 INFO Intent: case_status, Confidence: 0.29, Text: all these three Judgments ie one Division Bench of Karnataka High Court and two ...\n",
      "2025-06-22 12:09:02,546 INFO Intent: case_status, Confidence: 0.29, Text: all these three Judgments ie one Division Bench of Karnataka High Court and two ...\n",
      "2025-06-22 12:09:03,006 INFO Intent: case_status, Confidence: 0.29, Text: insist that merely because prohibitory order was issued under that pretext the t...\n",
      "2025-06-22 12:09:03,006 INFO Intent: case_status, Confidence: 0.29, Text: insist that merely because prohibitory order was issued under that pretext the t...\n",
      "2025-06-22 12:09:03,461 INFO Intent: case_status, Confidence: 0.29, Text: or other documents maintained in the form of electronic record as defined in cla...\n",
      "2025-06-22 12:09:03,461 INFO Intent: case_status, Confidence: 0.29, Text: or other documents maintained in the form of electronic record as defined in cla...\n",
      "2025-06-22 12:09:03,924 INFO Intent: claim_process, Confidence: 0.20, Text: non completion of the search on the respective dates but not for the purpose of ...\n",
      "2025-06-22 12:09:03,924 INFO Intent: claim_process, Confidence: 0.20, Text: non completion of the search on the respective dates but not for the purpose of ...\n",
      "2025-06-22 12:09:04,382 INFO Intent: case_status, Confidence: 0.14, Text: Therefore within the said provision of iib referred to above it can only be cons...\n",
      "2025-06-22 12:09:04,382 INFO Intent: case_status, Confidence: 0.14, Text: Therefore within the said provision of iib referred to above it can only be cons...\n",
      "2025-06-22 12:09:04,473 INFO Intent: case_status, Confidence: 0.14, Text: WPNo20999 of 2003 55 If that being the position the impugned Block Assessment Or...\n",
      "2025-06-22 12:09:04,473 INFO Intent: case_status, Confidence: 0.14, Text: WPNo20999 of 2003 55 If that being the position the impugned Block Assessment Or...\n",
      "2025-06-22 12:09:04,942 INFO Intent: case_status, Confidence: 0.29, Text: that the Revenue without having access to the electronic documents should have c...\n",
      "2025-06-22 12:09:04,942 INFO Intent: case_status, Confidence: 0.29, Text: that the Revenue without having access to the electronic documents should have c...\n",
      "2025-06-22 12:09:05,414 INFO Intent: case_status, Confidence: 0.14, Text: violated the principles of natural justice 4 By raising the aforesaid grounds th...\n",
      "2025-06-22 12:09:05,414 INFO Intent: case_status, Confidence: 0.14, Text: violated the principles of natural justice 4 By raising the aforesaid grounds th...\n",
      "2025-06-22 12:09:05,782 INFO Intent: case_status, Confidence: 0.29, Text: the petitioner side in the context of Section 158BE1b to state that the impugned...\n",
      "2025-06-22 12:09:05,782 INFO Intent: case_status, Confidence: 0.29, Text: the petitioner side in the context of Section 158BE1b to state that the impugned...\n",
      "2025-06-22 12:09:06,253 INFO Intent: claim_process, Confidence: 0.20, Text: contend that once the search was over on 25012001 itself pursuant to the authori...\n",
      "2025-06-22 12:09:06,253 INFO Intent: claim_process, Confidence: 0.20, Text: contend that once the search was over on 25012001 itself pursuant to the authori...\n",
      "2025-06-22 12:09:06,831 INFO Intent: case_status, Confidence: 0.29, Text: been issued which has either not been issued or if it is issued which is beyond ...\n",
      "2025-06-22 12:09:06,831 INFO Intent: case_status, Confidence: 0.29, Text: been issued which has either not been issued or if it is issued which is beyond ...\n",
      "2025-06-22 12:09:07,333 INFO Intent: claim_process, Confidence: 0.40, Text: petitioner has relied upon various judgments among them he heavily relied upon t...\n",
      "2025-06-22 12:09:07,333 INFO Intent: claim_process, Confidence: 0.40, Text: petitioner has relied upon various judgments among them he heavily relied upon t...\n",
      "2025-06-22 12:09:07,811 INFO Intent: claim_process, Confidence: 0.20, Text: was initiated on 25012001 at No4181 Kodambakkam High Road and it continued on 30...\n",
      "2025-06-22 12:09:07,811 INFO Intent: claim_process, Confidence: 0.20, Text: was initiated on 25012001 at No4181 Kodambakkam High Road and it continued on 30...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Intent detection for all chunks:\n",
      "Jimson_Ratnam_JavaFullStackDeveloper_2+years_chunk1.txt: {'intent': 'resume_info', 'intent_confidence': 0.28846153846153844}\n",
      "Jimson_Ratnam_JavaFullStackDeveloper_2+years_chunk2.txt: {'intent': 'resume_info', 'intent_confidence': 0.21153846153846154}\n",
      "Jimson_Ratnam_JavaFullStackDeveloper_2+years_chunk3.txt: {'intent': 'resume_info', 'intent_confidence': 0.25}\n",
      "Jimson_Ratnam_JavaFullStackDeveloper_2+years_chunk4.txt: {'intent': 'resume_info', 'intent_confidence': 0.21153846153846154}\n",
      "MHC_CaseStatus_511605_chunk1.txt: {'intent': 'case_status', 'intent_confidence': 0.42857142857142855}\n",
      "MHC_CaseStatus_511605_chunk10.txt: {'intent': 'claim_process', 'intent_confidence': 0.2}\n",
      "MHC_CaseStatus_511605_chunk11.txt: {'intent': 'claim_process', 'intent_confidence': 0.4}\n",
      "MHC_CaseStatus_511605_chunk12.txt: {'intent': 'case_status', 'intent_confidence': 0.2857142857142857}\n",
      "MHC_CaseStatus_511605_chunk13.txt: {'intent': 'case_status', 'intent_confidence': 0.2857142857142857}\n",
      "MHC_CaseStatus_511605_chunk14.txt: {'intent': 'claim_process', 'intent_confidence': 0.4}\n",
      "MHC_CaseStatus_511605_chunk15.txt: {'intent': 'claim_process', 'intent_confidence': 0.4}\n",
      "MHC_CaseStatus_511605_chunk16.txt: {'intent': 'claim_process', 'intent_confidence': 0.2}\n",
      "MHC_CaseStatus_511605_chunk17.txt: {'intent': 'claim_process', 'intent_confidence': 0.4}\n",
      "MHC_CaseStatus_511605_chunk18.txt: {'intent': 'claim_process', 'intent_confidence': 0.6}\n",
      "MHC_CaseStatus_511605_chunk19.txt: {'intent': 'case_status', 'intent_confidence': 0.39488834142684937}\n",
      "MHC_CaseStatus_511605_chunk2.txt: {'intent': 'case_status', 'intent_confidence': 0.14285714285714285}\n",
      "MHC_CaseStatus_511605_chunk20.txt: {'intent': 'case_status', 'intent_confidence': 0.14285714285714285}\n",
      "MHC_CaseStatus_511605_chunk21.txt: {'intent': 'case_status', 'intent_confidence': 0.42398136854171753}\n",
      "MHC_CaseStatus_511605_chunk22.txt: {'intent': 'case_status', 'intent_confidence': 0.14285714285714285}\n",
      "MHC_CaseStatus_511605_chunk23.txt: {'intent': 'case_status', 'intent_confidence': 0.2857142857142857}\n",
      "MHC_CaseStatus_511605_chunk24.txt: {'intent': 'case_status', 'intent_confidence': 0.14285714285714285}\n",
      "MHC_CaseStatus_511605_chunk25.txt: {'intent': 'case_status', 'intent_confidence': 0.2857142857142857}\n",
      "MHC_CaseStatus_511605_chunk26.txt: {'intent': 'case_status', 'intent_confidence': 0.14285714285714285}\n",
      "MHC_CaseStatus_511605_chunk27.txt: {'intent': 'claim_process', 'intent_confidence': 0.4}\n",
      "MHC_CaseStatus_511605_chunk28.txt: {'intent': 'case_status', 'intent_confidence': 0.14285714285714285}\n",
      "MHC_CaseStatus_511605_chunk29.txt: {'intent': 'claim_process', 'intent_confidence': 0.2}\n",
      "MHC_CaseStatus_511605_chunk3.txt: {'intent': 'case_status', 'intent_confidence': 0.4418315887451172}\n",
      "MHC_CaseStatus_511605_chunk30.txt: {'intent': 'case_status', 'intent_confidence': 0.2857142857142857}\n",
      "MHC_CaseStatus_511605_chunk31.txt: {'intent': 'technical_support', 'intent_confidence': 0.25}\n",
      "MHC_CaseStatus_511605_chunk32.txt: {'intent': 'resume_info', 'intent_confidence': 0.038461538461538464}\n",
      "MHC_CaseStatus_511605_chunk33.txt: {'intent': 'case_status', 'intent_confidence': 0.42857142857142855}\n",
      "MHC_CaseStatus_511605_chunk34.txt: {'intent': 'case_status', 'intent_confidence': 0.2857142857142857}\n",
      "MHC_CaseStatus_511605_chunk4.txt: {'intent': 'claim_process', 'intent_confidence': 0.2}\n",
      "MHC_CaseStatus_511605_chunk5.txt: {'intent': 'claim_process', 'intent_confidence': 0.2}\n",
      "MHC_CaseStatus_511605_chunk6.txt: {'intent': 'claim_process', 'intent_confidence': 0.2}\n",
      "MHC_CaseStatus_511605_chunk7.txt: {'intent': 'case_status', 'intent_confidence': 0.14285714285714285}\n",
      "MHC_CaseStatus_511605_chunk8.txt: {'intent': 'case_status', 'intent_confidence': 0.14285714285714285}\n",
      "MHC_CaseStatus_511605_chunk9.txt: {'intent': 'case_status', 'intent_confidence': 0.14285714285714285}\n",
      "MHC_CaseStatus_511692_chunk1.txt: {'intent': 'claim_process', 'intent_confidence': 0.2}\n",
      "MHC_CaseStatus_511692_chunk10.txt: {'intent': 'claim_process', 'intent_confidence': 0.4}\n",
      "MHC_CaseStatus_511692_chunk11.txt: {'intent': 'case_status', 'intent_confidence': 0.2857142857142857}\n",
      "MHC_CaseStatus_511692_chunk12.txt: {'intent': 'claim_process', 'intent_confidence': 0.4}\n",
      "MHC_CaseStatus_511692_chunk13.txt: {'intent': 'party_information', 'intent_confidence': 0.46807143092155457}\n",
      "MHC_CaseStatus_511692_chunk14.txt: {'intent': 'claim_process', 'intent_confidence': 0.4}\n",
      "MHC_CaseStatus_511692_chunk15.txt: {'intent': 'claim_process', 'intent_confidence': 0.2}\n",
      "MHC_CaseStatus_511692_chunk16.txt: {'intent': 'case_status', 'intent_confidence': 0.2857142857142857}\n",
      "MHC_CaseStatus_511692_chunk17.txt: {'intent': 'claim_process', 'intent_confidence': 0.4}\n",
      "MHC_CaseStatus_511692_chunk18.txt: {'intent': 'claim_process', 'intent_confidence': 0.4}\n",
      "MHC_CaseStatus_511692_chunk19.txt: {'intent': 'case_status', 'intent_confidence': 0.2857142857142857}\n",
      "MHC_CaseStatus_511692_chunk2.txt: {'intent': 'case_status', 'intent_confidence': 0.14285714285714285}\n",
      "MHC_CaseStatus_511692_chunk20.txt: {'intent': 'case_status', 'intent_confidence': 0.2857142857142857}\n",
      "MHC_CaseStatus_511692_chunk21.txt: {'intent': 'case_status', 'intent_confidence': 0.2857142857142857}\n",
      "MHC_CaseStatus_511692_chunk22.txt: {'intent': 'case_status', 'intent_confidence': 0.14285714285714285}\n",
      "MHC_CaseStatus_511692_chunk23.txt: {'intent': 'case_status', 'intent_confidence': 0.2857142857142857}\n",
      "MHC_CaseStatus_511692_chunk24.txt: {'intent': 'case_status', 'intent_confidence': 0.14285714285714285}\n",
      "MHC_CaseStatus_511692_chunk25.txt: {'intent': 'hearing_information', 'intent_confidence': 0.38895538449287415}\n",
      "MHC_CaseStatus_511692_chunk26.txt: {'intent': 'case_status', 'intent_confidence': 0.14285714285714285}\n",
      "MHC_CaseStatus_511692_chunk27.txt: {'intent': 'claim_process', 'intent_confidence': 0.2}\n",
      "MHC_CaseStatus_511692_chunk28.txt: {'intent': 'case_status', 'intent_confidence': 0.2857142857142857}\n",
      "MHC_CaseStatus_511692_chunk29.txt: {'intent': 'technical_support', 'intent_confidence': 0.25}\n",
      "MHC_CaseStatus_511692_chunk3.txt: {'intent': 'case_status', 'intent_confidence': 0.2857142857142857}\n",
      "MHC_CaseStatus_511692_chunk30.txt: {'intent': 'case_status', 'intent_confidence': 0.2857142857142857}\n",
      "MHC_CaseStatus_511692_chunk31.txt: {'intent': 'case_status', 'intent_confidence': 0.2857142857142857}\n",
      "MHC_CaseStatus_511692_chunk32.txt: {'intent': 'case_status', 'intent_confidence': 0.14285714285714285}\n",
      "MHC_CaseStatus_511692_chunk33.txt: {'intent': 'case_status', 'intent_confidence': 0.2857142857142857}\n",
      "MHC_CaseStatus_511692_chunk34.txt: {'intent': 'case_status', 'intent_confidence': 0.14285714285714285}\n",
      "MHC_CaseStatus_511692_chunk35.txt: {'intent': 'case_status', 'intent_confidence': 0.2857142857142857}\n",
      "MHC_CaseStatus_511692_chunk36.txt: {'intent': 'case_status', 'intent_confidence': 0.2857142857142857}\n",
      "MHC_CaseStatus_511692_chunk37.txt: {'intent': 'case_status', 'intent_confidence': 0.14285714285714285}\n",
      "MHC_CaseStatus_511692_chunk38.txt: {'intent': 'case_status', 'intent_confidence': 0.42857142857142855}\n",
      "MHC_CaseStatus_511692_chunk39.txt: {'intent': 'case_status', 'intent_confidence': 0.2857142857142857}\n",
      "MHC_CaseStatus_511692_chunk4.txt: {'intent': 'case_status', 'intent_confidence': 0.42857142857142855}\n",
      "MHC_CaseStatus_511692_chunk40.txt: {'intent': 'case_status', 'intent_confidence': 0.2857142857142857}\n",
      "MHC_CaseStatus_511692_chunk41.txt: {'intent': 'resume_info', 'intent_confidence': 0.038461538461538464}\n",
      "MHC_CaseStatus_511692_chunk42.txt: {'intent': 'case_status', 'intent_confidence': 0.42857142857142855}\n",
      "MHC_CaseStatus_511692_chunk43.txt: {'intent': 'case_status', 'intent_confidence': 0.2857142857142857}\n",
      "MHC_CaseStatus_511692_chunk44.txt: {'intent': 'case_status', 'intent_confidence': 0.2857142857142857}\n",
      "MHC_CaseStatus_511692_chunk45.txt: {'intent': 'case_status', 'intent_confidence': 0.2857142857142857}\n",
      "MHC_CaseStatus_511692_chunk46.txt: {'intent': 'claim_process', 'intent_confidence': 0.2}\n",
      "MHC_CaseStatus_511692_chunk47.txt: {'intent': 'case_status', 'intent_confidence': 0.14285714285714285}\n",
      "MHC_CaseStatus_511692_chunk48.txt: {'intent': 'case_status', 'intent_confidence': 0.14285714285714285}\n",
      "MHC_CaseStatus_511692_chunk49.txt: {'intent': 'case_status', 'intent_confidence': 0.2857142857142857}\n",
      "MHC_CaseStatus_511692_chunk5.txt: {'intent': 'case_status', 'intent_confidence': 0.14285714285714285}\n",
      "MHC_CaseStatus_511692_chunk50.txt: {'intent': 'case_status', 'intent_confidence': 0.2857142857142857}\n",
      "MHC_CaseStatus_511692_chunk6.txt: {'intent': 'claim_process', 'intent_confidence': 0.2}\n",
      "MHC_CaseStatus_511692_chunk7.txt: {'intent': 'case_status', 'intent_confidence': 0.2857142857142857}\n",
      "MHC_CaseStatus_511692_chunk8.txt: {'intent': 'claim_process', 'intent_confidence': 0.4}\n",
      "MHC_CaseStatus_511692_chunk9.txt: {'intent': 'claim_process', 'intent_confidence': 0.2}\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from typing import Dict, Any\n",
    "\n",
    "# Configure logging for production\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "class ProductionIntentDetector:\n",
    "    def __init__(self, intent_keywords, intent_examples, intent_model, threshold=0.35):\n",
    "        self.intent_keywords = intent_keywords\n",
    "        self.intent_examples = intent_examples\n",
    "        self.intent_model = intent_model\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def detect(self, text: str) -> Dict[str, Any]:\n",
    "        # 1. Fast rule-based keyword/lemmatized match\n",
    "        doc = nlp(text.lower())\n",
    "        tokens = [token.lemma_ for token in doc]\n",
    "        detected_intent = None\n",
    "        max_matches = 0\n",
    "        for intent, keywords in self.intent_keywords.items():\n",
    "            matches = sum(kw in tokens for kw in keywords)\n",
    "            if matches > max_matches:\n",
    "                max_matches = matches\n",
    "                detected_intent = intent\n",
    "        intent_confidence = max_matches / max(1, len(self.intent_keywords.get(detected_intent, []))) if detected_intent else 0.0\n",
    "        # 2. Embedding similarity fallback\n",
    "        if not detected_intent or max_matches == 0:\n",
    "            query_emb = self.intent_model.encode(text, convert_to_tensor=True)\n",
    "            best_intent, best_score = None, 0\n",
    "            for intent, examples in self.intent_examples.items():\n",
    "                example_embs = self.intent_model.encode(examples, convert_to_tensor=True)\n",
    "                scores = util.pytorch_cos_sim(query_emb, example_embs)\n",
    "                max_score = scores.max().item()\n",
    "                if max_score > best_score:\n",
    "                    best_score = max_score\n",
    "                    best_intent = intent\n",
    "            if best_score > self.threshold:\n",
    "                detected_intent = best_intent\n",
    "                intent_confidence = best_score\n",
    "        # 3. Fallback to general_info\n",
    "        if not detected_intent:\n",
    "            detected_intent = \"general_info\"\n",
    "            intent_confidence = 0.0\n",
    "        # 4. Logging for monitoring\n",
    "        logging.info(f\"Intent: {detected_intent}, Confidence: {intent_confidence:.2f}, Text: {text[:80]}...\")\n",
    "        return {\n",
    "            \"intent\": detected_intent,\n",
    "            \"intent_confidence\": intent_confidence\n",
    "        }\n",
    "\n",
    "# Usage example\n",
    "prod_detector = ProductionIntentDetector(intent_keywords, intent_examples, intent_model, threshold=0.35)\n",
    "chunk_dir = os.path.join(project_root, '..', 'data', 'chunks')\n",
    "chunk_files = glob.glob(os.path.join(chunk_dir, '*.txt'))\n",
    "\n",
    "chunk_intents = {}\n",
    "for chunk_path in chunk_files:\n",
    "    with open(chunk_path, 'r', encoding='utf-8') as f:\n",
    "        chunk_text = f.read()\n",
    "    result = prod_detector.detect(chunk_text)\n",
    "    chunk_intents[os.path.basename(chunk_path)] = result\n",
    "\n",
    "print('‚úÖ Intent detection for all chunks:')\n",
    "for fname, res in chunk_intents.items():\n",
    "    print(f\"{fname}: {res}\")\n",
    "# prod_result = prod_detector.detect(text)\n",
    "# print('‚úÖ Production-Grade Intent Detection:')\n",
    "# print(prod_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d31c4abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== spaCy + SentenceTransformer Pipeline ==\n",
      "{'entities': ['devzen',\n",
      "              'spring boot spring security',\n",
      "              'spring batch',\n",
      "              'elasticsearch',\n",
      "              'angular typescript',\n",
      "              'spring boot angular',\n",
      "              'aw ec2 s3 rd iam',\n",
      "              'spring boot',\n",
      "              'angular',\n",
      "              '25',\n",
      "              '50',\n",
      "              'portal',\n",
      "              '20',\n",
      "              'six month of launch',\n",
      "              '40',\n",
      "              '90'],\n",
      " 'intent': 'resume_info',\n",
      " 'intent_confidence': 0.28846153846153844,\n",
      " 'keywords': ['java',\n",
      "              'stack',\n",
      "              'developer',\n",
      "              'devzen',\n",
      "              'software',\n",
      "              'solution',\n",
      "              'jwt',\n",
      "              'authentication',\n",
      "              'verification',\n",
      "              'apis',\n",
      "              'spring',\n",
      "              'boot',\n",
      "              'spring',\n",
      "              'security',\n",
      "              'batch',\n",
      "              'processing',\n",
      "              'workflow',\n",
      "              'spring',\n",
      "              'batch',\n",
      "              'scale',\n",
      "              'datum',\n",
      "              'management',\n",
      "              'rest',\n",
      "              'apis',\n",
      "              'time',\n",
      "              'inventory',\n",
      "              'tracking',\n",
      "              'alert',\n",
      "              'stock',\n",
      "              'management',\n",
      "              'elasticsearch',\n",
      "              'search',\n",
      "              'retrieval',\n",
      "              'volume',\n",
      "              'application',\n",
      "              'component',\n",
      "              'angular',\n",
      "              'typescript',\n",
      "              'product',\n",
      "              'management',\n",
      "              'payment',\n",
      "              'gateway',\n",
      "              'integration',\n",
      "              'spring',\n",
      "              'boot',\n",
      "              'angular',\n",
      "              'transaction',\n",
      "              'dashboard',\n",
      "              'key',\n",
      "              'insight',\n",
      "              'tournament',\n",
      "              'inventory',\n",
      "              'management',\n",
      "              'application',\n",
      "              'docker',\n",
      "              'consistency',\n",
      "              'development',\n",
      "              'production',\n",
      "              'environment',\n",
      "              'cicd',\n",
      "              'pipeline',\n",
      "              'build',\n",
      "              'test',\n",
      "              'deployment',\n",
      "              'process',\n",
      "              'github',\n",
      "              'action',\n",
      "              'workflow',\n",
      "              'deployment',\n",
      "              'efficiency',\n",
      "              'application',\n",
      "              'aws',\n",
      "              'ec2',\n",
      "              'rds',\n",
      "              'iam',\n",
      "              'cloud',\n",
      "              'hosting',\n",
      "              'management',\n",
      "              'apis',\n",
      "              'spring',\n",
      "              'boot',\n",
      "              'document',\n",
      "              'management',\n",
      "              'user',\n",
      "              'authentication',\n",
      "              'security',\n",
      "              'efficiency',\n",
      "              'angular',\n",
      "              'user',\n",
      "              'experience',\n",
      "              'document',\n",
      "              'creation',\n",
      "              'socialization',\n",
      "              'publication',\n",
      "              'key',\n",
      "              'achievement',\n",
      "              'security',\n",
      "              'user',\n",
      "              'interaction',\n",
      "              'angular',\n",
      "              'api',\n",
      "              'user',\n",
      "              'authentication',\n",
      "              'verification',\n",
      "              'process',\n",
      "              'document',\n",
      "              'management',\n",
      "              'search',\n",
      "              'security',\n",
      "              'access',\n",
      "              'functionality',\n",
      "              'improvement',\n",
      "              'user',\n",
      "              'incident',\n",
      "              'satisfaction',\n",
      "              'score',\n",
      "              'document',\n",
      "              'management',\n",
      "              'improve',\n",
      "              'tournament',\n",
      "              'efficiency',\n",
      "              'feature',\n",
      "              'document',\n",
      "              'creation',\n",
      "              'end',\n",
      "              'end',\n",
      "              'platform',\n",
      "              'socialization',\n",
      "              'publication',\n",
      "              'portal',\n",
      "              'user',\n",
      "              'player',\n",
      "              'registration',\n",
      "              'tournament',\n",
      "              'scheduling',\n",
      "              'base',\n",
      "              'month',\n",
      "              'launch',\n",
      "              'administration',\n",
      "              'time',\n",
      "              'tournament',\n",
      "              'management',\n",
      "              'process',\n",
      "              'admin',\n",
      "              'control',\n",
      "              'time',\n",
      "              'scoring',\n",
      "              'score',\n",
      "              'update',\n",
      "              'player',\n",
      "              'admin',\n",
      "              'dashboard',\n",
      "              'spectator',\n",
      "              'score',\n",
      "              'update',\n",
      "              'delay',\n",
      "              'tournament',\n",
      "              'oversight']}\n",
      "\n",
      "== Transformers NER + KeyBERT Pipeline ==\n",
      "{'entities': ['devzen',\n",
      "              'spring boot spring security',\n",
      "              'spring batch',\n",
      "              'elasticsearch',\n",
      "              'angular typescript',\n",
      "              'spring boot angular',\n",
      "              'aw ec2 s3 rd iam',\n",
      "              'spring boot',\n",
      "              'angular',\n",
      "              '25',\n",
      "              '50',\n",
      "              'portal',\n",
      "              '20',\n",
      "              'six month of launch',\n",
      "              '40',\n",
      "              '90'],\n",
      " 'intent': 'resume_info',\n",
      " 'intent_confidence': 0.28846153846153844,\n",
      " 'keywords': ['java',\n",
      "              'stack',\n",
      "              'developer',\n",
      "              'devzen',\n",
      "              'software',\n",
      "              'solution',\n",
      "              'jwt',\n",
      "              'authentication',\n",
      "              'verification',\n",
      "              'apis',\n",
      "              'spring',\n",
      "              'boot',\n",
      "              'spring',\n",
      "              'security',\n",
      "              'batch',\n",
      "              'processing',\n",
      "              'workflow',\n",
      "              'spring',\n",
      "              'batch',\n",
      "              'scale',\n",
      "              'datum',\n",
      "              'management',\n",
      "              'rest',\n",
      "              'apis',\n",
      "              'time',\n",
      "              'inventory',\n",
      "              'tracking',\n",
      "              'alert',\n",
      "              'stock',\n",
      "              'management',\n",
      "              'elasticsearch',\n",
      "              'search',\n",
      "              'retrieval',\n",
      "              'volume',\n",
      "              'application',\n",
      "              'component',\n",
      "              'angular',\n",
      "              'typescript',\n",
      "              'product',\n",
      "              'management',\n",
      "              'payment',\n",
      "              'gateway',\n",
      "              'integration',\n",
      "              'spring',\n",
      "              'boot',\n",
      "              'angular',\n",
      "              'transaction',\n",
      "              'dashboard',\n",
      "              'key',\n",
      "              'insight',\n",
      "              'tournament',\n",
      "              'inventory',\n",
      "              'management',\n",
      "              'application',\n",
      "              'docker',\n",
      "              'consistency',\n",
      "              'development',\n",
      "              'production',\n",
      "              'environment',\n",
      "              'cicd',\n",
      "              'pipeline',\n",
      "              'build',\n",
      "              'test',\n",
      "              'deployment',\n",
      "              'process',\n",
      "              'github',\n",
      "              'action',\n",
      "              'workflow',\n",
      "              'deployment',\n",
      "              'efficiency',\n",
      "              'application',\n",
      "              'aws',\n",
      "              'ec2',\n",
      "              'rds',\n",
      "              'iam',\n",
      "              'cloud',\n",
      "              'hosting',\n",
      "              'management',\n",
      "              'apis',\n",
      "              'spring',\n",
      "              'boot',\n",
      "              'document',\n",
      "              'management',\n",
      "              'user',\n",
      "              'authentication',\n",
      "              'security',\n",
      "              'efficiency',\n",
      "              'angular',\n",
      "              'user',\n",
      "              'experience',\n",
      "              'document',\n",
      "              'creation',\n",
      "              'socialization',\n",
      "              'publication',\n",
      "              'key',\n",
      "              'achievement',\n",
      "              'security',\n",
      "              'user',\n",
      "              'interaction',\n",
      "              'angular',\n",
      "              'api',\n",
      "              'user',\n",
      "              'authentication',\n",
      "              'verification',\n",
      "              'process',\n",
      "              'document',\n",
      "              'management',\n",
      "              'search',\n",
      "              'security',\n",
      "              'access',\n",
      "              'functionality',\n",
      "              'improvement',\n",
      "              'user',\n",
      "              'incident',\n",
      "              'satisfaction',\n",
      "              'score',\n",
      "              'document',\n",
      "              'management',\n",
      "              'improve',\n",
      "              'tournament',\n",
      "              'efficiency',\n",
      "              'feature',\n",
      "              'document',\n",
      "              'creation',\n",
      "              'end',\n",
      "              'end',\n",
      "              'platform',\n",
      "              'socialization',\n",
      "              'publication',\n",
      "              'portal',\n",
      "              'user',\n",
      "              'player',\n",
      "              'registration',\n",
      "              'tournament',\n",
      "              'scheduling',\n",
      "              'base',\n",
      "              'month',\n",
      "              'launch',\n",
      "              'administration',\n",
      "              'time',\n",
      "              'tournament',\n",
      "              'management',\n",
      "              'process',\n",
      "              'admin',\n",
      "              'control',\n",
      "              'time',\n",
      "              'scoring',\n",
      "              'score',\n",
      "              'update',\n",
      "              'player',\n",
      "              'admin',\n",
      "              'dashboard',\n",
      "              'spectator',\n",
      "              'score',\n",
      "              'update',\n",
      "              'delay',\n",
      "              'tournament',\n",
      "              'oversight']}\n",
      "\n",
      "== Transformers NER + KeyBERT Pipeline ==\n",
      "{'entities': ['angular',\n",
      "              'angular type',\n",
      "              'angular typescript',\n",
      "              'aws ec2 s3 rds iam',\n",
      "              'devzen software solutions',\n",
      "              'docker',\n",
      "              'elasticsearch',\n",
      "              'enhanced document management improved tournament efficiency '\n",
      "              'implemented',\n",
      "              'github actions',\n",
      "              'hub actions',\n",
      "              'integrated elasticsearch',\n",
      "              'java full stack developer',\n",
      "              'java full stack developer 20',\n",
      "              'jwt-based',\n",
      "              'present devzen software solutions developed',\n",
      "              'robust admin control real',\n",
      "              'spring batch',\n",
      "              'spring boot',\n",
      "              'spring boot angular',\n",
      "              'spring boot spring security',\n",
      "              'spring boot spring security built',\n",
      "              'strengthened system security improved user interaction '\n",
      "              'leveraged angular',\n",
      "              'time scoring implemented',\n",
      "              'utilized git'],\n",
      " 'intent': 'resume_info',\n",
      " 'intent_confidence': 0.28846153846153844,\n",
      " 'keywords': ['apis',\n",
      "              'authentication',\n",
      "              'api',\n",
      "              'aws',\n",
      "              'jwt',\n",
      "              'cloud',\n",
      "              'workflows',\n",
      "              'angular',\n",
      "              'secure',\n",
      "              'java']}\n",
      "\n",
      "== Comparison ==\n",
      "Entities overlap: {'angular typescript', 'spring boot angular', 'spring boot', 'angular', 'elasticsearch', 'spring batch', 'spring boot spring security'}\n",
      "Keywords overlap: {'angular', 'apis', 'java', 'aws', 'cloud', 'authentication', 'jwt', 'api'}\n",
      "Intent (spaCy pipeline): resume_info | Confidence: 0.28846153846153844\n",
      "Intent (Transformers+KeyBERT): resume_info | Confidence: 0.28846153846153844\n",
      "{'entities': ['angular',\n",
      "              'angular type',\n",
      "              'angular typescript',\n",
      "              'aws ec2 s3 rds iam',\n",
      "              'devzen software solutions',\n",
      "              'docker',\n",
      "              'elasticsearch',\n",
      "              'enhanced document management improved tournament efficiency '\n",
      "              'implemented',\n",
      "              'github actions',\n",
      "              'hub actions',\n",
      "              'integrated elasticsearch',\n",
      "              'java full stack developer',\n",
      "              'java full stack developer 20',\n",
      "              'jwt-based',\n",
      "              'present devzen software solutions developed',\n",
      "              'robust admin control real',\n",
      "              'spring batch',\n",
      "              'spring boot',\n",
      "              'spring boot angular',\n",
      "              'spring boot spring security',\n",
      "              'spring boot spring security built',\n",
      "              'strengthened system security improved user interaction '\n",
      "              'leveraged angular',\n",
      "              'time scoring implemented',\n",
      "              'utilized git'],\n",
      " 'intent': 'resume_info',\n",
      " 'intent_confidence': 0.28846153846153844,\n",
      " 'keywords': ['apis',\n",
      "              'authentication',\n",
      "              'api',\n",
      "              'aws',\n",
      "              'jwt',\n",
      "              'cloud',\n",
      "              'workflows',\n",
      "              'angular',\n",
      "              'secure',\n",
      "              'java']}\n",
      "\n",
      "== Comparison ==\n",
      "Entities overlap: {'angular typescript', 'spring boot angular', 'spring boot', 'angular', 'elasticsearch', 'spring batch', 'spring boot spring security'}\n",
      "Keywords overlap: {'angular', 'apis', 'java', 'aws', 'cloud', 'authentication', 'jwt', 'api'}\n",
      "Intent (spaCy pipeline): resume_info | Confidence: 0.28846153846153844\n",
      "Intent (Transformers+KeyBERT): resume_info | Confidence: 0.28846153846153844\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Test and compare all metadata extraction techniques side by side\n",
    "\n",
    "print(\"== spaCy + SentenceTransformer Pipeline ==\")\n",
    "spacy_metadata = extract_metadata(text)\n",
    "pprint(spacy_metadata)\n",
    "\n",
    "print(\"\\n== Transformers NER + KeyBERT Pipeline ==\")\n",
    "alt_metadata = extract_metadata_alt(text)\n",
    "pprint(alt_metadata)\n",
    "\n",
    "print(\"\\n== Comparison ==\")\n",
    "print(\"Entities overlap:\", set(spacy_metadata['entities']) & set(alt_metadata['entities']))\n",
    "print(\"Keywords overlap:\", set(spacy_metadata['keywords']) & set(alt_metadata['keywords']))\n",
    "print(\"Intent (spaCy pipeline):\", spacy_metadata['intent'], \"| Confidence:\", spacy_metadata['intent_confidence'])\n",
    "print(\"Intent (Transformers+KeyBERT):\", alt_metadata['intent'], \"| Confidence:\", alt_metadata['intent_confidence'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "785763ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Zero-Shot Classification ==\n",
      "Intent: case_status, Confidence: 0.18\n",
      "\n",
      "== Ensemble Intent Detection ==\n",
      "Intent: resume_info, Confidence: 0.75\n",
      "All results: [('resume_info', 0.28846153846153844), ('resume_info', 0.4149797260761261), ('resume_info', 1.0), ('case_status', 0.17770253121852875)]\n",
      "\n",
      "== Ensemble Intent Detection ==\n",
      "Intent: resume_info, Confidence: 0.75\n",
      "All results: [('resume_info', 0.28846153846153844), ('resume_info', 0.4149797260761261), ('resume_info', 1.0), ('case_status', 0.17770253121852875)]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from collections import Counter\n",
    "\n",
    "# Advanced Intent Detection Techniques\n",
    "\n",
    "# 1. Zero-shot classification with HuggingFace Transformers (e.g., facebook/bart-large-mnli)\n",
    "\n",
    "zero_shot_classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "def detect_intent_zero_shot(text, candidate_labels):\n",
    "    result = zero_shot_classifier(text, candidate_labels)\n",
    "    best_intent = result['labels'][0]\n",
    "    confidence = result['scores'][0]\n",
    "    return best_intent, confidence\n",
    "\n",
    "candidate_labels = list(intent_examples.keys())\n",
    "intent_zs, conf_zs = detect_intent_zero_shot(text, candidate_labels)\n",
    "print(\"== Zero-Shot Classification ==\")\n",
    "print(f\"Intent: {intent_zs}, Confidence: {conf_zs:.2f}\")\n",
    "\n",
    "# 2. Fine-tuned intent classification model (if you have labeled data)\n",
    "# (Placeholder: You would train a classifier using your labeled dataset, e.g., using sklearn, Keras, or HuggingFace Trainer.)\n",
    "\n",
    "# 3. Ensemble: Combine multiple techniques (majority vote, weighted average, etc.)\n",
    "# Ensure detect_intent_keywords is defined (use from cell 7 if available)\n",
    "if 'detect_intent_keywords' not in globals():\n",
    "    def detect_intent_keywords(text, intent_keywords):\n",
    "        doc = nlp(text.lower())\n",
    "        tokens = [token.lemma_ for token in doc]\n",
    "        scores = {}\n",
    "        for intent, keywords in intent_keywords.items():\n",
    "            matches = sum(kw in tokens for kw in keywords)\n",
    "            scores[intent] = matches\n",
    "        best_intent = max(scores, key=scores.get)\n",
    "        confidence = scores[best_intent] / max(1, len(intent_keywords[best_intent]))\n",
    "        return best_intent, confidence\n",
    "\n",
    "def ensemble_intent_detection(text):\n",
    "    results = []\n",
    "    # Keyword\n",
    "    kw_intent, kw_conf = detect_intent_keywords(text, intent_keywords)\n",
    "    results.append((kw_intent, kw_conf))\n",
    "    # Embedding\n",
    "    emb_intent, emb_conf = detect_intent_embedding(text, intent_examples, intent_model)\n",
    "    results.append((emb_intent, emb_conf))\n",
    "    # Regex\n",
    "    rgx_intent, rgx_conf = detect_intent_regex(text)\n",
    "    results.append((rgx_intent, rgx_conf))\n",
    "    # Zero-shot\n",
    "    zs_intent, zs_conf = detect_intent_zero_shot(text, candidate_labels)\n",
    "    results.append((zs_intent, zs_conf))\n",
    "    # Majority vote\n",
    "    intents = [intent for intent, _ in results]\n",
    "    best, freq = Counter(intents).most_common(1)[0]\n",
    "    return best, freq / len(results), results\n",
    "\n",
    "ensemble_intent, ensemble_conf, all_results = ensemble_intent_detection(text)\n",
    "print(\"\\n== Ensemble Intent Detection ==\")\n",
    "print(f\"Intent: {ensemble_intent}, Confidence: {ensemble_conf:.2f}\")\n",
    "print(\"All results:\", all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35270645",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d6e53fb",
   "metadata": {},
   "source": [
    "## Fine-Tuned Transformer Classifier for Intent Detection\n",
    "\n",
    "This section demonstrates how to train and use a transformer (DistilBERT) for intent classification using Hugging Face Transformers. You need a labeled dataset (text, intent) for this. The example below uses realistic intent examples from your domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "15517ba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How do I file a claim?</td>\n",
       "      <td>claim_process</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the process for submitting an insuranc...</td>\n",
       "      <td>claim_process</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I want to submit a new claim for my car accident.</td>\n",
       "      <td>claim_process</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Guide me through the claim submission steps.</td>\n",
       "      <td>claim_process</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Where do I upload my claim documents?</td>\n",
       "      <td>claim_process</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>the petitioner side in the context of Section ...</td>\n",
       "      <td>case_status</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>contend that once the search was over on 25012...</td>\n",
       "      <td>claim_process</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>been issued which has either not been issued o...</td>\n",
       "      <td>case_status</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>petitioner has relied upon various judgments a...</td>\n",
       "      <td>claim_process</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>was initiated on 25012001 at No4181 Kodambakka...</td>\n",
       "      <td>claim_process</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>148 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text          label\n",
       "0                               How do I file a claim?  claim_process\n",
       "1    What is the process for submitting an insuranc...  claim_process\n",
       "2    I want to submit a new claim for my car accident.  claim_process\n",
       "3         Guide me through the claim submission steps.  claim_process\n",
       "4                Where do I upload my claim documents?  claim_process\n",
       "..                                                 ...            ...\n",
       "143  the petitioner side in the context of Section ...    case_status\n",
       "144  contend that once the search was over on 25012...  claim_process\n",
       "145  been issued which has either not been issued o...    case_status\n",
       "146  petitioner has relied upon various judgments a...  claim_process\n",
       "147  was initiated on 25012001 at No4181 Kodambakka...  claim_process\n",
       "\n",
       "[148 rows x 2 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example labeled data for intent fine-tuning (expand with more real samples for best results)\n",
    "data = [\n",
    "    {\"text\": \"How do I file a claim?\", \"label\": \"claim_process\"},\n",
    "    {\"text\": \"What is the process for submitting an insurance claim?\", \"label\": \"claim_process\"},\n",
    "    {\"text\": \"What is the current status of the case?\", \"label\": \"case_status\"},\n",
    "    {\"text\": \"Show me the progress of case number 511605.\", \"label\": \"case_status\"},\n",
    "    {\"text\": \"Can I get a copy of the case order?\", \"label\": \"document_request\"},\n",
    "    {\"text\": \"How do I request the judgment document?\", \"label\": \"document_request\"},\n",
    "    {\"text\": \"What skills are listed in the resume?\", \"label\": \"resume_info\"},\n",
    "    {\"text\": \"List the programming languages known by the applicant.\", \"label\": \"resume_info\"},\n",
    "    {\"text\": \"Who is the presiding judge for this case?\", \"label\": \"court_details\"},\n",
    "    {\"text\": \"Who are the parties involved in this case?\", \"label\": \"party_information\"},\n",
    "    {\"text\": \"When was the last hearing held?\", \"label\": \"hearing_information\"},\n",
    "    {\"text\": \"I have a technical issue with the system.\", \"label\": \"technical_support\"},\n",
    "    {\"text\": \"Give me a summary of the file.\", \"label\": \"general_info\"},\n",
    "    {\"text\": \"Tell me about this document.\", \"label\": \"general_info\"},\n",
    "]\n",
    "df = pd.DataFrame(expanded_data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d9d14a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 148/148 [00:00<00:00, 5921.84 examples/s]\n",
      "\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\jimmy\\AppData\\Local\\Temp\\ipykernel_18852\\3636785226.py:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "C:\\Users\\jimmy\\AppData\\Local\\Temp\\ipykernel_18852\\3636785226.py:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='90' max='90' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [90/90 00:12, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.515600</td>\n",
       "      <td>1.640425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.380800</td>\n",
       "      <td>1.555576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.111900</td>\n",
       "      <td>1.467606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=90, training_loss=1.2983389218648276, metrics={'train_runtime': 12.4043, 'train_samples_per_second': 28.539, 'train_steps_per_second': 7.256, 'total_flos': 5862309599232.0, 'train_loss': 1.2983389218648276, 'epoch': 3.0})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# 1. Prepare label mappings\n",
    "df['label_id'] = df['label'].astype('category').cat.codes\n",
    "label2id = {label: i for i, label in enumerate(df['label'].astype('category').cat.categories)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "# 2. Convert to Hugging Face Dataset\n",
    "# Rename 'label_id' to 'labels' for Trainer compatibility\n",
    "df_for_hf = df.rename(columns={'label_id': 'labels'})\n",
    "# Remove the 'label' column (string) to avoid Trainer confusion\n",
    "df_for_hf = df_for_hf.drop(columns=['label'])\n",
    "dataset = Dataset.from_pandas(df_for_hf)\n",
    "\n",
    "# 3. Tokenize\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "def preprocess(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=64)\n",
    "dataset = dataset.map(preprocess, batched=True)\n",
    "\n",
    "# 4. Train/Test split\n",
    "split = dataset.train_test_split(test_size=0.2)\n",
    "train_ds, test_ds = split[\"train\"], split[\"test\"]\n",
    "\n",
    "# 5. Model and Trainer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, num_labels=len(label2id), id2label=id2label, label2id=label2id)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./intent_model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# 6. Train (this will take a few minutes on CPU, much faster on GPU)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "55357a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jimson_Ratnam_JavaFullStackDeveloper_2+years_chunk1.txt: resume_info\n",
      "Jimson_Ratnam_JavaFullStackDeveloper_2+years_chunk2.txt: resume_info\n",
      "Jimson_Ratnam_JavaFullStackDeveloper_2+years_chunk3.txt: resume_info\n",
      "Jimson_Ratnam_JavaFullStackDeveloper_2+years_chunk4.txt: resume_info\n",
      "MHC_CaseStatus_511605_chunk1.txt: case_status\n",
      "MHC_CaseStatus_511605_chunk10.txt: case_status\n",
      "MHC_CaseStatus_511605_chunk11.txt: case_status\n",
      "MHC_CaseStatus_511605_chunk12.txt: case_status\n",
      "MHC_CaseStatus_511605_chunk13.txt: case_status\n",
      "MHC_CaseStatus_511605_chunk14.txt: case_status\n",
      "MHC_CaseStatus_511605_chunk15.txt: case_status\n",
      "MHC_CaseStatus_511605_chunk16.txt: case_status\n",
      "MHC_CaseStatus_511605_chunk17.txt: case_status\n",
      "MHC_CaseStatus_511605_chunk18.txt: case_status\n",
      "MHC_CaseStatus_511605_chunk19.txt: case_status\n",
      "MHC_CaseStatus_511605_chunk2.txt: case_status\n",
      "MHC_CaseStatus_511605_chunk20.txt: case_status\n",
      "MHC_CaseStatus_511605_chunk21.txt: case_status\n",
      "MHC_CaseStatus_511605_chunk22.txt: case_status\n",
      "MHC_CaseStatus_511605_chunk23.txt: case_status\n",
      "MHC_CaseStatus_511605_chunk24.txt: case_status\n",
      "MHC_CaseStatus_511605_chunk25.txt: case_status\n",
      "MHC_CaseStatus_511605_chunk26.txt: case_status\n",
      "MHC_CaseStatus_511605_chunk27.txt: case_status\n",
      "MHC_CaseStatus_511605_chunk28.txt: case_status\n",
      "MHC_CaseStatus_511605_chunk29.txt: case_status\n",
      "MHC_CaseStatus_511605_chunk3.txt: case_status\n",
      "MHC_CaseStatus_511605_chunk30.txt: case_status\n",
      "MHC_CaseStatus_511605_chunk31.txt: case_status\n",
      "MHC_CaseStatus_511605_chunk32.txt: case_status\n",
      "MHC_CaseStatus_511605_chunk33.txt: case_status\n",
      "MHC_CaseStatus_511605_chunk34.txt: case_status\n",
      "MHC_CaseStatus_511605_chunk4.txt: case_status\n",
      "MHC_CaseStatus_511605_chunk5.txt: case_status\n",
      "MHC_CaseStatus_511605_chunk6.txt: case_status\n",
      "MHC_CaseStatus_511605_chunk7.txt: case_status\n",
      "MHC_CaseStatus_511605_chunk8.txt: case_status\n",
      "MHC_CaseStatus_511605_chunk9.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk1.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk10.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk11.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk12.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk13.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk14.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk15.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk16.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk17.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk18.txt: case_status\n",
      "MHC_CaseStatus_511605_chunk29.txt: case_status\n",
      "MHC_CaseStatus_511605_chunk3.txt: case_status\n",
      "MHC_CaseStatus_511605_chunk30.txt: case_status\n",
      "MHC_CaseStatus_511605_chunk31.txt: case_status\n",
      "MHC_CaseStatus_511605_chunk32.txt: case_status\n",
      "MHC_CaseStatus_511605_chunk33.txt: case_status\n",
      "MHC_CaseStatus_511605_chunk34.txt: case_status\n",
      "MHC_CaseStatus_511605_chunk4.txt: case_status\n",
      "MHC_CaseStatus_511605_chunk5.txt: case_status\n",
      "MHC_CaseStatus_511605_chunk6.txt: case_status\n",
      "MHC_CaseStatus_511605_chunk7.txt: case_status\n",
      "MHC_CaseStatus_511605_chunk8.txt: case_status\n",
      "MHC_CaseStatus_511605_chunk9.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk1.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk10.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk11.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk12.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk13.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk14.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk15.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk16.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk17.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk18.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk19.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk2.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk20.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk21.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk22.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk23.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk24.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk25.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk26.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk27.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk28.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk29.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk3.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk30.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk31.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk32.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk33.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk34.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk35.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk36.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk37.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk38.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk39.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk19.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk2.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk20.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk21.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk22.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk23.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk24.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk25.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk26.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk27.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk28.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk29.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk3.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk30.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk31.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk32.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk33.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk34.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk35.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk36.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk37.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk38.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk39.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk4.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk40.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk41.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk42.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk43.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk44.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk45.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk46.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk47.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk48.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk49.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk5.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk50.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk6.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk7.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk8.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk9.txt: claim_process\n",
      "MHC_CaseStatus_511692_chunk4.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk40.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk41.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk42.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk43.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk44.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk45.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk46.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk47.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk48.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk49.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk5.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk50.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk6.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk7.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk8.txt: case_status\n",
      "MHC_CaseStatus_511692_chunk9.txt: claim_process\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# 7. Inference: Predict intent for new text\n",
    "def predict_intent(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=64)\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    outputs = model(**inputs)\n",
    "    pred_id = outputs.logits.argmax(dim=1).item()\n",
    "    return id2label[pred_id]\n",
    "\n",
    "chunk_dir = os.path.join(project_root, '..', 'data', 'chunks')\n",
    "chunk_files = glob.glob(os.path.join(chunk_dir, '*.txt'))\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# print(predict_intent(open(r'C:\\New folder (5)\\new-search-models\\data\\chunks\\MHC_CaseStatus_511605_chunk1.txt', 'r', encoding='utf-8').read()))\n",
    "for chunk_path in chunk_files:\n",
    "    with open(chunk_path, 'r', encoding='utf-8') as f:\n",
    "        chunk_text = f.read()\n",
    "    print(f\"{os.path.basename(chunk_path)}: {predict_intent(chunk_text)}\")\n",
    "# print(predict_intent(\"List the skills in this resume.\"))\n",
    "# print(predict_intent(\"Who is the presiding judge?\"))\n",
    "# print(predict_intent(\"I have a technical issue with the system.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5e2705",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387d3c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 88 labeled examples from correct_intents.txt.\n"
     ]
    }
   ],
   "source": [
    "# üì• Build labeled training data from correct_intents.txt\n",
    "import re\n",
    "\n",
    "correct_intents_path = r'C:\\New folder (5)\\new-search-models\\correct_intents.txt'\n",
    "chunk_dir = os.path.join(project_root, '..', 'data', 'chunks')\n",
    "\n",
    "labeled_from_file = []\n",
    "with open(correct_intents_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        m = re.match(r'([^:]+):.*?\\'intent\\': \\'([^\\']+)\\'', line)\n",
    "        if m:\n",
    "            fname, intent = m.group(1).strip(), m.group(2).strip()\n",
    "            chunk_path = os.path.join(chunk_dir, fname)\n",
    "            if os.path.exists(chunk_path):\n",
    "                with open(chunk_path, 'r', encoding='utf-8') as cf:\n",
    "                    chunk_text = cf.read()\n",
    "                labeled_from_file.append({\"text\": chunk_text, \"label\": intent})\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Chunk file not found: {chunk_path}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Could not parse line: {line.strip()}\")\n",
    "\n",
    "print(f\"Loaded {len(labeled_from_file)} labeled examples from correct_intents.txt.\")\n",
    "# Add these to your expanded_data list before retraining:\n",
    "# expanded_data.extend(labeled_from_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f25a1314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expanded_data now has 148 examples (including those from correct_intents.txt).\n"
     ]
    }
   ],
   "source": [
    "# ‚ûï Add labeled examples from correct_intents.txt to your training data\n",
    "if 'expanded_data' in globals() and 'labeled_from_file' in globals():\n",
    "    expanded_data.extend(labeled_from_file)\n",
    "    print(f\"expanded_data now has {len(expanded_data)} examples (including those from correct_intents.txt).\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Make sure both expanded_data and labeled_from_file are defined before running this cell.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66792358",
   "metadata": {},
   "outputs": [],
   "source": [
    "## üõ†Ô∏è Iterative Improvement: Add Misclassified Examples\n",
    "\n",
    "# If you notice misclassified chunks, copy their text and true intent below. Add them to your training data to help the model learn from its mistakes. Retrain and re-evaluate for better accuracy.\n",
    "# Example: Add misclassified examples to your training data\n",
    "# Replace the text and label with your real misclassified cases\n",
    "misclassified_examples = [\n",
    "    {\"text\": \"<Paste misclassified chunk text here>\", \"label\": \"<correct_intent>\"},\n",
    "    # Example:\n",
    "    # {\"text\": \"The case was closed on 2023-05-01.\", \"label\": \"case_status\"},\n",
    "    # {\"text\": \"Please send me a copy of the final judgment.\", \"label\": \"document_request\"},\n",
    "]\n",
    "\n",
    "# Add these to your expanded_data list before retraining:\n",
    "# expanded_data.extend(misclassified_examples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9b3279",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Expand Your Labeled Dataset for Better Classifier Performance\n",
    "\n",
    "Your current labeled dataset is too small and unbalanced, which causes the model to predict the same intent for most inputs. \n",
    "\n",
    "- Add at least 10‚Äì20 diverse, realistic examples for each intent category.\n",
    "- Include edge cases and ambiguous queries.\n",
    "- The more varied and representative your data, the better your classifier will perform.\n",
    "\n",
    "After expanding, retrain the model and re-run the evaluation cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "18794fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.58\n",
      "\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "      case_status       1.00      1.00      1.00         2\n",
      "    claim_process       0.50      0.50      0.50         2\n",
      " document_request       0.50      1.00      0.67         3\n",
      "     general_info       0.00      0.00      0.00         3\n",
      "      resume_info       0.50      1.00      0.67         1\n",
      "technical_support       0.00      0.00      0.00         1\n",
      "\n",
      "         accuracy                           0.58        12\n",
      "        macro avg       0.42      0.58      0.47        12\n",
      "     weighted avg       0.42      0.58      0.47        12\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2 0 0 0 0 0]\n",
      " [0 1 1 0 0 0]\n",
      " [0 0 3 0 0 0]\n",
      " [0 1 1 0 1 0]\n",
      " [0 0 0 0 1 0]\n",
      " [0 0 1 0 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\New folder (5)\\new-search-models\\search_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\New folder (5)\\new-search-models\\search_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\New folder (5)\\new-search-models\\search_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 8. Evaluate classifier performance on the test set\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Get true and predicted labels for the test set\n",
    "true_labels = [id2label[i] for i in test_ds['labels']]\n",
    "pred_labels = []\n",
    "for text in test_ds['text']:\n",
    "    pred = predict_intent(text)\n",
    "    pred_labels.append(pred)\n",
    "\n",
    "# Accuracy\n",
    "acc = accuracy_score(true_labels, pred_labels)\n",
    "print(f\"Test Accuracy: {acc:.2f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, pred_labels))\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(true_labels, pred_labels, labels=list(label2id.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2b8b2f",
   "metadata": {},
   "source": [
    "## üè∑Ô∏è Expand Labeled Data for Better Intent Classification\n",
    "\n",
    "To improve classifier performance, add more diverse and realistic examples for each intent. This helps the model generalize and reduces bias toward majority classes. Below is an expanded dataset template you can use and modify for your domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "726005e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## üè∑Ô∏è Expand Labeled Data for Better Intent Classification\n",
    "\n",
    "# To improve classifier performance, add more diverse and realistic examples for each intent. This helps the model generalize and reduces bias toward majority classes. Below is an expanded dataset template you can use and modify for your domain.\n",
    "# Template: Expanded labeled data for intent fine-tuning\n",
    "# Copy, edit, and expand this list with your real examples\n",
    "expanded_data = [\n",
    "    # claim_process\n",
    "    {\"text\": \"How do I file a claim?\", \"label\": \"claim_process\"},\n",
    "    {\"text\": \"What is the process for submitting an insurance claim?\", \"label\": \"claim_process\"},\n",
    "    {\"text\": \"I want to submit a new claim for my car accident.\", \"label\": \"claim_process\"},\n",
    "    {\"text\": \"Guide me through the claim submission steps.\", \"label\": \"claim_process\"},\n",
    "    {\"text\": \"Where do I upload my claim documents?\", \"label\": \"claim_process\"},\n",
    "    {\"text\": \"How long does it take to process a claim?\", \"label\": \"claim_process\"},\n",
    "    {\"text\": \"Can I check the status of my insurance claim?\", \"label\": \"claim_process\"},\n",
    "    {\"text\": \"What documents are needed to file a claim?\", \"label\": \"claim_process\"},\n",
    "    {\"text\": \"Is there a deadline for submitting claims?\", \"label\": \"claim_process\"},\n",
    "    {\"text\": \"Can I cancel a claim after submitting?\", \"label\": \"claim_process\"},\n",
    "    # case_status\n",
    "    {\"text\": \"What is the current status of the case?\", \"label\": \"case_status\"},\n",
    "    {\"text\": \"Show me the progress of case number 511605.\", \"label\": \"case_status\"},\n",
    "    {\"text\": \"Has a judgment been issued in my case?\", \"label\": \"case_status\"},\n",
    "    {\"text\": \"Is my case still pending?\", \"label\": \"case_status\"},\n",
    "    {\"text\": \"When is the next hearing for my case?\", \"label\": \"case_status\"},\n",
    "    {\"text\": \"What was the outcome of the last court session?\", \"label\": \"case_status\"},\n",
    "    {\"text\": \"Who is the presiding judge for this case?\", \"label\": \"case_status\"},\n",
    "    {\"text\": \"Has an appeal been filed?\", \"label\": \"case_status\"},\n",
    "    {\"text\": \"Is there an order available for my case?\", \"label\": \"case_status\"},\n",
    "    {\"text\": \"What is the next step in my case?\", \"label\": \"case_status\"},\n",
    "    # document_request\n",
    "    {\"text\": \"Can I get a copy of the case order?\", \"label\": \"document_request\"},\n",
    "    {\"text\": \"How do I request the judgment document?\", \"label\": \"document_request\"},\n",
    "    {\"text\": \"I need certified copies of my case documents.\", \"label\": \"document_request\"},\n",
    "    {\"text\": \"Where can I download the court forms?\", \"label\": \"document_request\"},\n",
    "    {\"text\": \"Request a copy of the final order.\", \"label\": \"document_request\"},\n",
    "    {\"text\": \"How do I obtain previous hearing transcripts?\", \"label\": \"document_request\"},\n",
    "    {\"text\": \"Can I get a digital copy of my case file?\", \"label\": \"document_request\"},\n",
    "    {\"text\": \"What is the fee for document requests?\", \"label\": \"document_request\"},\n",
    "    {\"text\": \"How long does it take to receive requested documents?\", \"label\": \"document_request\"},\n",
    "    {\"text\": \"Is there a limit to the number of documents I can request?\", \"label\": \"document_request\"},\n",
    "    # resume_info\n",
    "    {\"text\": \"What skills are listed in the resume?\", \"label\": \"resume_info\"},\n",
    "    {\"text\": \"List the programming languages known by the applicant.\", \"label\": \"resume_info\"},\n",
    "    {\"text\": \"Show me the candidate's work experience.\", \"label\": \"resume_info\"},\n",
    "    {\"text\": \"What certifications does the applicant have?\", \"label\": \"resume_info\"},\n",
    "    {\"text\": \"Summarize the professional experience section.\", \"label\": \"resume_info\"},\n",
    "    {\"text\": \"List the tools and technologies used by the candidate.\", \"label\": \"resume_info\"},\n",
    "    {\"text\": \"What is the educational background of the applicant?\", \"label\": \"resume_info\"},\n",
    "    {\"text\": \"What are the achievements or awards?\", \"label\": \"resume_info\"},\n",
    "    {\"text\": \"Show me the contact information in the resume.\", \"label\": \"resume_info\"},\n",
    "    {\"text\": \"What is the career objective or summary?\", \"label\": \"resume_info\"},\n",
    "    # technical_support\n",
    "    {\"text\": \"I have a technical issue with the system.\", \"label\": \"technical_support\"},\n",
    "    {\"text\": \"There is a problem with the website.\", \"label\": \"technical_support\"},\n",
    "    {\"text\": \"I can't log in to my account.\", \"label\": \"technical_support\"},\n",
    "    {\"text\": \"The upload button is not working.\", \"label\": \"technical_support\"},\n",
    "    {\"text\": \"How do I reset my password?\", \"label\": \"technical_support\"},\n",
    "    {\"text\": \"The page is loading very slowly.\", \"label\": \"technical_support\"},\n",
    "    {\"text\": \"I received an error message while submitting my form.\", \"label\": \"technical_support\"},\n",
    "    {\"text\": \"The system crashed during my session.\", \"label\": \"technical_support\"},\n",
    "    {\"text\": \"How do I contact technical support?\", \"label\": \"technical_support\"},\n",
    "    {\"text\": \"The app keeps freezing.\", \"label\": \"technical_support\"},\n",
    "    # general_info\n",
    "    {\"text\": \"Give me a summary of the file.\", \"label\": \"general_info\"},\n",
    "    {\"text\": \"Tell me about this document.\", \"label\": \"general_info\"},\n",
    "    {\"text\": \"What is the purpose of this document?\", \"label\": \"general_info\"},\n",
    "    {\"text\": \"Provide general information about the case.\", \"label\": \"general_info\"},\n",
    "    {\"text\": \"What are the office hours?\", \"label\": \"general_info\"},\n",
    "    {\"text\": \"How do I contact the support team?\", \"label\": \"general_info\"},\n",
    "    {\"text\": \"Where is the office located?\", \"label\": \"general_info\"},\n",
    "    {\"text\": \"What services are offered?\", \"label\": \"general_info\"},\n",
    "    {\"text\": \"How do I register for an account?\", \"label\": \"general_info\"},\n",
    "    {\"text\": \"What is the refund policy?\", \"label\": \"general_info\"},\n",
    "]\n",
    "\n",
    "# You can now use expanded_data instead of the old 'data' list for training your classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1761ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "search_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
