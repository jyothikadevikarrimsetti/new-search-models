1 Vol0123456789 Scientific Reports 2024 1410140 httpsdoiorg101038s41598-024-60779-z wwwnaturecomscientificreportsA study of extractive summarization of long documents incorporating local topic and hierarchical information Ting Wang 1 Chuan Yang 1 Maoyang Zou 2 Jiaying Liang 1 Dong Xiang 1 Wenjie Yang 1 Hongyang Wang 1 Jia Li 1 In recent years the transformer-based language models have achieved remarkable success in the field of extractive text summarization However there are still some limitations in this kind of research First the transformer language model usually regards the text as a linear sequence ignoring the inherent hierarchical structure information of the text Second for long text data traditional extractive models often focus on global topic information which poses challenges in how they capturing and integrating local contextual information within topic segments To address these issues we propose a long text extractive summarization model that employs a local topic information extraction module and a text hierarchical extraction module to capture the local topic information and documents hierarchical structure information of the original text Our approach enhances the ability to determine whether a sentence belongs to the summary In this experiment ROUGE score is used as the experimental evaluation index and evaluates the model on three large public datasets Through experimental validation the model demonstrates superior performance in terms of ROUGE- 1 ROUGE-2 and ROUGE-L scores compared to current mainstream summarization models affirming the effectiveness of incorporating local topic information and document hierarchical structure into the model Text summarization is an arduous task in the field of natural language processing NLP1 wherein the goal is to generate a concise and logically connected summary of a given document This process involves extracting crucial information and reduce the length of the document while preserving the essential meaning23 Text sum- marization can effectively reduce the information burden of users enable users to quickly obtain information from redundant information greatly reduce manpower and material resources It plays an important role in various domains including information retrieval title generation and other related fields Based on the methodology employed text summarization tasks can be categorized into two types extrac- tive summarization4 and abstractive summarization5 The abstractive summarization method utilizes neural network-based approaches such as the Sequence-2-Sequence Seq2Seq architecture6 also known as encoder- decoder architecture The principle of an encoder-decoder is similar to the way human think or write summaries The encoder first encodes the full text and then the decoder generates new sentences word by word to form a document summary This method generates less redundant summary information but might face challenges in maintaining fluency and grammatical correctness In addition the generation of new words or phrases may produce summaries that are inconsistent with the original statement7 These issues can be mitigated by directly selecting sentences from the source text and assembling them into summaries ie the extractive summarization The extractive method treats summarization as a classification problem where important sentences are directly selected from the source text to construct a summary Summaries generated through this approach often exhibit a good performance in fluency and grammar For the extractive summarization task the core challenge lies in learning comprehensive sentence context information and modeling inter-sentence relationships through the encoder thereby enabling sentence classifiers to extract more valuable sentences Traditional extractive methods usually employ graph-based methods or clustering-based methods for unsupervised summarization89 These approaches construct the correlation between sentences using cosine similarity and then use sorting methods to OPEN 1School of Computer Science Chengdu University of Information Technology Chengdu 610225 Sichuan Province China 2College of Blockchain Industry Chengdu University of Information Technology Chengdu 610225 Sichuan Province China email zoumycuiteducn 2 Vol1234567890 Scientific Reports 2024 1410140 httpsdoiorg101038s41598-024-60779-z wwwnaturecomscientificreportscalculate the importance of sentences With the rapid development of deep learning many extractive summari- zation methods use Recurrent Neural Network RNN to capture the relationship between sentences1011 How - ever RNN-based methods are difficult to deal with long-distance dependencies especially for long document summaries In recent years transformer12 language model which has been pre-trained by large-scale corpus has achieved excellent results when fine-tuned for downstream tasks and have found widespread application in the field of text summarization Liu et al 13 proposed the BERTSUM model by improving the BERT embedding layer They applied the BERT model for the first time in the text summarization and achieved state-of-the-art SOTA performance on CNNDailyMail dataset Zhang et al 14 designed a hierarchical transformer to capture long-range inter-sentence relationships However this method did not yield significant performance gains for summarization tasks and faced challenges such as slow training speed and potential overfitting At the same time some researchers introduced the neural topic model NTM15 and graph neural network GNN16 into the task of text summarization to capture global semantic information and further guide the generation of abstracts Cui et al 17 use NTM to capture the theme features of documents and GNN to represent documents as graph structures thus obtaining the relationship between sentences However for long document summarization tasks the above methods have two shortcomings The first one is that they fail to recognize the explicit hierarchical structures and section headings inherent within the long document When manually summarizing text we tend to focus on the main sections For example in the context of scientific papers more attention may be given to sections like Methodology Experimental and Conclu - sion but Background or Related Work may not receive as much emphasis In addition sentences within a section have stronger relationships compared to those outside the section Understanding the logical relation - ship between sentences and the hierarchical structure within the document helps the model better identify the important sentences However the traditional transformer-based text summarization methods often regard the text as a sequential structure and struggle with longer documents The second shortcoming is that the longer the document the more topics it may discuss because each section presents different topic information In summary the aforementioned methods focus on the overall topic information of the entire document that is the global information neglecting the local topic information of individual sections In order to address these issues this paper proposes a long-document extractive summarization model that integrates local topic information and document hierarchy information into current topic segment The main contributions of this paper can be summarized as follows 1 Introduction of an innovative long-document extractive summarization model This model consists of a text encoder a module for extracting local topic information and a module for embedding hierarchical structure information of the document The information is integrated into the sentence representation of the document enhancing the quality of the generated summaries 2 This paper utilizes LSTM-Minus18 to obtain distributed representations of local information and combines it with text summarization tasks Instead of employing a fixed three-segment approach for text paragraph- ing the paper adopts a dynamic method based on the number of sentences to determine paragraph length thereby calculating the starting and ending positions of each paragraph in the text Paragraph segments are divided based on these positions and their topic information is computed 3 Experimental results conducted on the PubMed dataset reveal excellent performance of the proposed method when compared to several baseline models Related Work Extractive summarization method With the rapid development of neural networks significant achievements have been made in extractive sum- marization tasks At present the extractive methods are mainly regarded as sentence sorting task or binary sequence labeling tasks In the sentence sorting paradigm models are required to assign scores to each sentence in the text and place higher-scored sentences at the front of the summary list while lower-scored sentences are placed towards the back This process yields an ordered list of sentences and the top few sentences are selected as the summary Narayan et al 19 proposed a topic-aware convolutional neural network model This model first extracts features from the documents using convolutional neural networks and then weights the features accord- ing to the topic Finally a selection-based sorting method is employed to choose the most relevant sentences as the summary Experiments results on multiple datasets show that this approach can generate concise summaries that still preserve valuable information Li et al20 proposed a method for evaluating sentence importance in multi-document summarization using variational autoencoder Different from the traditional method based on feature engineering this method directly learns the abstract semantic representation directly from the original data KL divergence is introduced to constrain the generated sentence representations to be close to the prior distribution thereby improving the generalization ability of the model Regarding the second paradigm which considers extractive text summarization as a sequence labeling task this approach involves extracting and encoding features for each sentence or paragraph The encoded features are then input into a decoder for labeling prediction to determine which sentences should be selected for the summary The sequence labeling method has been widely applied in extractive text summarization and has achieved good results Nalapati et al4 proposed the SummaRuNNer model for text summarization which is a sequence model based on RNN This model generates document summarization by learning the importance of each sentence within the document It has demonstrated good summarization performance on multiple text datasets Zhang et al21 introduced a latent variable extractive model which treats sentences as latent variables and infers summaries using sentences with activated variables 3 Vol0123456789 Scientific Reports 2024 1410140 httpsdoiorg101038s41598-024-60779-z wwwnaturecomscientificreportsHowever most of the methods mentioned above rely on RNN for extractive summarization RNN-based methods face challenges in handling long-distance dependencies at the sentence level and may omit on language or structural information due to the input format of the original document In order to address these issues researchers have started utilizing transformer-based pre-training language model as encoders and representing documents through more intuitive graph structures They have also incorporated NTM to extract topic features from the documents further guiding the models to produce high-quality summaries Jia et al22 proposed a method called deep differential amplifier for extractive summarization which enhances the features of sum- mary sentences by contrast to non-summary sentences using differential amplifiers Shi et al23 proposed a star architecture-based extractive summarization method where sentences in documents are modeled as satellite nodes and a virtual central node is introduced to learn the inter-sentence relationships using the star structure This approach achieved promising results on three public datasets Ma et al24 embedded the topic features extracted by NTM into BERT to generate a vector representation with topic features thus improving the quality of summaries Although the aforementioned methods have succeeded in modeling inter-sentence relationship and extract- ing global semantics there is still a problem with extractive text summarization methods based on transformer pre-training language models The length of the input document in text summarization is longer compared to general natural language processing task and using just a transformer-based encoder is insufficient for effectively handling long texts and often leads to high computational costs To better understand the original document researchers have proposed various improvements Xie et al25 first preprocessed the documents by dividing them into blocks with the same size encoded each block with block encoding They merged the block encoding results with NTM to generate global topic features Finally they established a comparison graph between topic features and sentence features to filter summary sentences This method has achieved good results in both long docu - ments and short news documents with particular advantages in handling the former Beltagy et al26 introduced the Longformer model specifically designed for processing long documents By replacing the self-attention mechanism of the transformer with a sliding window self-attention mechanism the time complexity is reduced to linear level enabling the model to handle long documents easily Although the Longformer performs well in handling long documents it fails to model local semantic information and document hierarchy structure which affects its performance Therefore this paper uses the Longformer as the encoder and incorporates local contextual information of the current topic segment and hierarchical structure information of the document This allow our model to prioritize local topic information and overall structural information when dealing with long scientific papers LSTM-Minus Wang et al27 proposed the LSTM-Minus method for the first time and applied it to dependency parsing and achieved good results The LSTM-Minus method is a novel approach for learning embedding of text segments utilizing subtraction between LSTM hidden vectors to learn the distributed representation of sentence segments Initially a sentence is divided into three segments prefix infix and suffix and the segment from the word wi to the word wj is represented by the hidden vector hjhi This allows the model to effectively learn segment embedding from both external and internal information thus enhancing its ability to obtain sentence-level information In the same year Cross et al28 extended the unidirectional LSTM-Minus to the bidirectional using it as sentence span representation and achieving impressive performance in component syntactic analysis tasks Build upon this idea we applied this method to the field of text summarization to extract the contextual information from local topic segments Method To address the limitations of the existing extractive text summarization methods this paper proposes a long document extractive summarization model that integrates local contextual information and document-level hierarchical information from the current topic segment The model is inspired by the long document extrac - tive model proposed by Ruan et al29 which incorporates hierarchical structure information The final model of this paper is obtained by incorporating local topic information Experiments results show that the inclusion of local topic information further deepens the models understanding of long texts The task of long text extractive summarization is defined as follow Given an original document Dsent 1 sent n D contains n sentences where each sentence denoted as the sent i represents the i-th sentence of the original document The purpose of the extractive text summarization model is to select m sentences capturing the central idea of the original text as summaries where m is the desired number of summary sentences mn This task is typically treated as a sentence classification problem For each sentence sent i there is a corresponding label yi0 1 where a label of 1 means that the sentence belongs to the summary while 0 indicates that it does not The proposed model as shown in Fig 1 comprises three main modules a pre-trained language model based encoder a local topic information extraction module referred to as the Topic Segment Representation module in the Fig 1 and a text hierarchical information embedding module Because this work deals with long text cor - pus the encoder used is based on the Longformer an improvement over the transformer pre-training language model which allows for better encoding of long documents Once the contextual representation of the document is obtained through the encoder it is passed to the local topic information extraction module which extracts the topic information of the sentence segment it belongs to The specific structure of this module is shown in Fig 2 Then the local topic information representation is fused with the text contextual representation resulting in a fusion of the local topic information and the textual context The text hierarchical structure information embedding module embeds the hierarchical structure information of the text into the fused representation of the local topic information and textual context By using a two-layer stacked transformer this module learns 4 Vol1234567890 Scientific Reports 2024 1410140 httpsdoiorg101038s41598-024-60779-z wwwnaturecomscientificreports Figure 1 Overall structure diagram of the model Figure 2 Local topic information extraction module 5 Vol0123456789 Scientific Reports 2024 1410140 httpsdoiorg101038s41598-024-60779-z wwwnaturecomscientificreportsthe hierarchical structure information at both the sentence and document levels enabling the model to gain a deeper understanding of the text context Finally the confidence score of each sentence is calculated through Sigmoid layer for each sentence to determine whether it should be included in the summary Text hierarchical information Sentence hierarchical information Due to scientific papers consisting of multiple sections with each section containing several paragraphs that describe different topics this paper uses paragraphs as the unit for hierarchical division of the article The sen - tence-level hierarchical structure information includes the linear position of the paragraph to which the sentence belongs and the linear position representation of the sentence within the paragraph The positions of paragraphs and sentences are represented by numerical serial numbers corresponding to them For an original document Dsent 1 sent n the hierarchical structure information of the i-th sentence sent i is expressed as a two- dimensional vector parenleftbig sssgparenrightbig which indicates the position of the sentence at this level as shown in Formula 1 where ss represents the linear position of the paragraph containing the sentence relative to the entire article and gs represents the linear position of the sentence within its respective paragraph All sentences within the same paragraph share the same value in the first dimension of the vsent vector indicating a higher correlation among sentences within the same paragraph And the gs vector further indicates the linear relationship among sentences within the paragraph Section title information Compared with short news articles scientific papers often have section titles The content within each section is usually highly relevant to the corresponding section title as the section title serves as a concise summary of the content of the section In this study when encoding sentences the section titles are incorporated as additional hierarchical information into the sentence encoding However for scientific papers there are many similar sec- tion titles with the same meaning For instance Method and Methodology have similar meanings and can be grouped together under the Method category Therefore for the PubMed dataset used in this paper eight section title categories are defined29 including introduction background case Method result discus - sion conclusion and additional information If the section title of a section does not fall into any of the eight predefined categories the original section title itself is directly used Encoder Document encoding The purpose of document encoding is to encode the sentences of the input document into a vector representation with a fixed length Previous methods for extractive text summarization tasks often employed RNN and BERT30 as encoders BERT is a bidirectional transformer encoder that is pre-trained on large-scale corpus and has achieved excellent performance on various natural language processing tasks However for long text data BERT cannot process the entire document which will lead to information loss Therefore in this paper we uses the Longformer pre-training language model as the text encoder Longformer improves the self-attention mechanism of the traditional transformer into the sliding window self-attention which makes it easy to handle documents with thousands of characters In the traditional transformer self-attention mechanism the calculation is performed by linearly transforming the input word embedding matrix to generate a Query matrix Query Q a Key matrix Key K and a Value matrix Value V of dimension d The specific calculation process is shown in Formula 2 where QKVRLd and d represents the dimension of a word vector matrix while dk represents the dimen- sion of the K matrix Hence the spatial complexity of the traditional transformer self-attention mechanism is Oparenleftbig L2parenrightbig the spatial complexity of Longformers sliding windows self-attention mechanism is OL scaling linearly with the input sequence length L As a result Longformer has more advantages in encoding long texts As shown in Fig 1 in order to obtain the representation of each sentence we inserts BOS beginning of sentence and EOS end of sentence tags at the beginning and end of each sentence respectively The model embedding layer includes Token Embeddings TE Segment Embeddings SE and Position Embeddings PE These features are sum to obtained the embedded representation of each word Subsequently the context of the input sequence is learned by using the pre-trained Longformer The entire procedure is illustrated in Eqs 3 and 4 where wij represents the j-th word of the i-th sentence which is obtained by Formula 3 wi0 and wi represent the BOS and EOS tags of the i-th sentence respectively and hij represents the hidden state of the corresponding word After Longformer encoding we use the BOS tag as the context representation of each sentence that isHsparenleftbig h10hN0parenrightbig 1 vsent issgs 2 Attention softmaxparenleftbiggQKT dkparenrightbigg V 3 wijTESEPE 4braceleftbig h10h11hN0hNbracerightbig Longformerparenleftbig w10w11wN0wNparenrightbig 6 Vol1234567890 Scientific Reports 2024 1410140 httpsdoiorg101038s41598-024-60779-z wwwnaturecomscientificreportsLocal topic information extraction In order to capture the local context information of the text segment to which the sentence belongs this study employs the LSTM-Minus method to learn text segment embeddings Its detailed structure is shown in Fig 2 The input of the local topic information extraction module is the contextual representation of each sentence obtained by the encoder LSTM can learn and utilize the previous information through its own gating structure and store it in the memory cell Therefore this study utilizes a Bi-LSTM to encode the sentence context representations to get the hidden vector representation of each sentence Subsequently the local contextual information of the topic segment to which the sentence belongs is represented by the subtraction between the hidden vectors at the beginning and end of each topic segment For the i-th topic segment ti the specific expression method can be found in Eqs 5 6 7 where fi and bi respectively represent the forward and backward propagating topic segments while start i and end i respectively represent the starting position and ending position of the topic segment represents the vector concatenation symbol In the case of the second topic segment t2 in Fig 2 the local topic information can be expressed as bracketleftbig f5f2b3b6bracketrightbig where f5 and f2 represent the forward propagating hidden state of the fifth sentence and the second sentence respectively and b3 and b6 represent the backward propagating hidden state of the third sentence and the sixth sentence respectively To prevent index out of range this study introduces zero vectors at the beginning and end of both forward and backward propagation respectively After calculating the local contextual information of the topic segment to which the sentence belongs it is concatenated with the sentence context encoding of the document to further enrich the sentence contextual representation Text hierarchical information encoding Currently there are two mainstream linear position encoding methods The first one used in transformer12 involves generating fixed values using sinecosine functions The second one used in BERT30 involves generat- ing random values that are trainable The position encoding method in transformer can only mark the position of a character without considering its contextual information On the contrary BERTs position encoding is achieved by randomly initializing an embedding vector with the dimension of bracketleftbig seqlength widthbracketrightbig Therein the first dimension represents the sequence length and the second dimension represents the vector length cor - responding to each character They are trained along with the entire extractive model allowing it to not only mark the character position but also learn the function of this position Therefore this study uses BERT posi - tion encoding method to encode vsent vector The hierarchical structure vector parenleftbig sssgparenrightbig of the i-th sentence can be expressed as Formula 8 where PE represents the position encoding method of BERT and d represents the vector dimension of the sen - tence while represents the vector concatenation symbol In order to encode the section title information STE of a sentence this study uses the same pre-trained encoder for document encoding By inputting the extracted section titles into the pre-trained encoder the hid - den states corresponding to each character are obtained and then summed up This approach allows for better integration of semantic information from each position within the section title in order to provide a more comprehensively representation of the section title information Training and Infer After obtaining the output sentence vector from the text hierarchical structure information embedding module a two-layer stacked transformer is applied to learn the hierarchical information at the sentence and document levels Subsequently these vectors are input into a sigmoid function to predict whether a sentence belongs to the summary In the training stage this model uses binary cross entropy as the loss function aiming to minimize the binary cross entropy loss function to optimize the model See Eqs 10 and 11 for details where indicates sigmoid function Wh represents learnable parameter matrix HSi represents the sentence vector representation that incorporates local topic information and hierarchical structure information bh represents bias In Formula 10 loss i represents the loss when judging whether each sentence belongs to summary hatwideyi repre - sents the predicted probability value of the current sentence and yi represents the true label value of the sentence5 fihf end ihf start i1 6 bihb start ihb end i1 7 tiparenleftbig fibiparenrightbig 8 SHE iPEparenleftbigg ssd 2parenrightbigg PEparenleftbigg gsd 2parenrightbigg 9 hatwideyiWhHSibh 10 Loss loss 1 loss n 11 loss iparenleftbighatwideyiyiparenrightbig bracketleftbig yilogparenleftbighatwideyiparenrightbig parenleftbig 1yiparenrightbig logparenleftbig 1hatwideyiparenrightbigbracketrightbig 7 Vol0123456789 Scientific Reports 2024 1410140 httpsdoiorg101038s41598-024-60779-z wwwnaturecomscientificreportsExperiment Dataset In order to verify the generalization ability of the model in this article experiments were conducted on three datasets namely the short news text dataset CNNDaily Mail and two long text datasets PubMed and ArXiv The CNNDaily Mail dataset comprises 310000 news articles The PubMed dataset is generated based on the PubMed literature database which provides a search engine for biomedical literature The other long text dataset is ArXiv which contains papers from various domains Table 1 provides a detailed comparison of the document count average text length and average summary length of the CNNDaily Mail dataset PubMed dataset and ArXiv dataset As shown in Table 2 following Cohan et al31 and See et al11 the training testing and validation set sizes for the PubMed dataset ArXiv dataset and CNNDM dataset are presented respectively Evaluation metrics In this paper ROUGE32 Recall-Oriented Understudy for Gisting Evaluation score is used to evaluate text summarization models including ROUGE-L ROUGE-N ROUGE-W and ROUGE-S ROUGE-L is calculated using the longest common subsequence and measures the similarity between the generated summary and the reference summary ROUGE-N where N can be 1 2 3 or 4 is an evaluation method based on n-gram recall rate The fundamental idea is to calculate the co-occurrence information score between the model-generated summary and the manually generated reference summary to assess the similarity between them In this paper ROUGE-1 R-1 ROUGE-2 R-2 and ROUGE-L R-L are used as evaluation metrics and the specific calcula- tion processes of ROUGE-N and ROUGE-L are shown in Formulas 12 13 14 15 where Count matchparenleftbig gramnparenrightbig indicates the number of n-grams that simultaneously appear in both the generated summary by an article model and the reference summary Countparenleftbig gramnparenrightbig donates the number of n-grams in the reference summary LCSparenleftbig RefSum ModelSumparenrightbig represents the longest common subsequence of the reference summary and model-generation summary m represents the length of reference summary while n represents the length of model-generation summary is a hyper-parameter used in the evaluation metrics Experimental setup This model was built using the Pytorch deep learning framework and trained on a RTX4090 GPU with 24GB of memory The training process employed gradient accumulation every two steps In the experiment the Long - former-base-4096 model was chosen as the encoder Similar to BERT it consists of 12 layers of transformer 12 ROUGE Nsummationtext SRefSumsummationtext gram nSCount matchparenleftbig gram nparenrightbig summationtext SRefSum summationtext gram nSCountparenleftbig gram nparenrightbig 13 RlcsLCSparenleftbig RefSum ModelSumparenrightbig m 14 PlcsLCSparenleftbig RefSum ModelSumparenrightbig n 15 ROUGE Lparenleftbig 12parenrightbig RclsPcls Rcls2Pcls Table 1 Comparison of lengths of three datasetsDatasets Number of documents Average document length words Average abstract length words CNN 92579 656 43 DailyMail 219506 693 52 PubMed 133215 3016 203 ArXiv 215913 5825 272 Table 2 Division of three datasetsDataset Train Test Val CNNDM 287227 11490 13368 PubMed 119924 6633 6658 ArXiv 203037 6440 6436 8 Vol1234567890 Scientific Reports 2024 1410140 httpsdoiorg101038s41598-024-60779-z wwwnaturecomscientificreportsencoding with a hidden size of 768 We adopt the same training strategy as Liu et al13 including a warm-up phase for 10000 steps followed by training for 50000 steps Specific parameters can be found in Table 3 Ethical and informed consent The data utilized in our study are exclusively sourced from publicly available and published datasets There is no conflict with others data and all data sources have been used in accordance with their respective terms of use and copyright policies Results and analysis To validate the effectiveness of the proposed model in the field of extractive summarization for long texts experi- ments were conducted on publicly available datasets including PubMed ArXiv and CNNDM A systematic comparison was performed between the proposed model and recently proposed extractive summarization models as well as abstractive summarization models BERTSUM13 This model was the first to introduce BERT to text summarization It adds a CLS token as a sentinel to represent the sentence During extractive summarization it predicts scores for each sentence to determine whether they belong to the summary Sent-CLF and Sent-PTR33 Both of them use a hierarchical bidirectional LSTM with word and sentence-level representation as encoders The difference lies in how they determine if a sentence is part of the summary Sent-PTR uses sentence pointers while Sent-CLF predicts sentence score Longformer-Ext26 This model improves upon transformer-based encoders by introducing a revised attention mechanism that combines local and global attention patterns It stacks 12 layers of enhanced transformers as the encoder and predicts scores to select summary sentences Reformer-Ext34 This model replaces the attention mechanism of the original transformer with a hash-based attention mechanism and incorporates reversible computation These modifications allows it to handle long text summarization effectively ExtSum-LG RdLoss35 and ExtSum-LG MMR35 Both models utilize ExtSum-LG as the base model The former adds redundant loss items to the original loss function to minimize redundant sentences during the sentence scoring stage resulting in summaries with less redundancy The latter recalculates the sentence importance scores using the obtained sentence confidence scores and selects sentences with lower redundancy as the summary PEGASUS36 and T537 The former adopts an unsupervised approach for pre-training focusing on the task of text summarization The latter on the other hand serves as a Transformer-based general text transforma- tion model TextRank38 Mihalcea et al drawing upon the PageRank algorithm proposed a methodology utilizing words phrases and sentences as nodes with their relationships represented as edges to construct a graph This approach facilitates the exploration of relationships among various vertices and edges in the context of their study Topic-GraphSum39 Integrating pre-trained language models with topic modeling for the purpose of abstract generation Pointer-Generator Coverage11 Utilizing a pointer generator to directly copy words from the source text while retaining the capability to generate new words Additionally employing a coverage mechanism to control the repetition of content in the summary HIBERT14 Adopting a Transformer-based bidirectional encoder for document encoding and leveraging unlabeled data for pre-training HSSAS40 By introducing a hierarchical self-attention mechanism to encode sentences and documents the extraction of sentences as summaries is facilitated Analysis of comparative experimental results on the PubMed dataset According to the experimental results in Table 4 the first two models refer to the unsupervised LEAD model and the greedily constructed ORACLE method Given that the PubMed dataset consists of lengthy texts this study employed the LEAD-7 method extracting the initial 7 sentences as summaries however the results were not satisfactory This observation suggests that the initial sentences in the PubMed dataset do not contain as Table 3 Hyperparameter settingHyperparameter Hyperparametric meaning Superparameter setting value Optimizer optimizer Adam Adamepsilon Adam Fuzzy shadow 1e-8 s Beat1and Beta2 values 090999 Batch size Bath size 600 Learning rate Learning rate 2e-3 Extlayer Stack Transformer layers 2 Extdropout Stacked Transformer dropout 01 9 Vol0123456789 Scientific Reports 2024 1410140 httpsdoiorg101038s41598-024-60779-z wwwnaturecomscientificreportsmuch information as those in the CNNDailyMail dataset The ORACLE summaries are generated using a greedy strategy selecting sentences that maximize the ROUGE scores and are often considered the upper limit of model performance on this dataset Through comparative experiments with recent extractive and abstractive summarization models the proposed long-text extractive summarization model which combines local topic information and hierarchical structure information achieves higher R-1 R-2 and R-L scores on the PubMed dataset than other models This substantiates the effectiveness of our model in comparison to existing approaches Specifically the model in this article is better than the PEGASUS model in R1 and R2 scores but the RL is slightly lower Compared to the BigBird PEGASUS model R2 and RL are slightly lower than this model but R1 is slightly higher Since this article uses the PEGASUS-Large version the model parameters are 540 M while the model parameters of this article are only 193 M It can be seen that the model of this article shows strong performance Since the T5 model is not specifically trained for text summarization tasks it shows poor results Compared to the BERTSUM model our proposed model shows improvements of 54 501 and 521 in R-1 R-2 and R-L scores respectively This indicates that our extractive summarization model incorporating Longformer as the text encoder effectively addresses the challenges posed by the length limitations of BERT pre-trained language models Additionally when compared to the Longformer-Ext extractive model using Longformer as the encoder our model achieves improvements of 274 315 and 235 in R-1 R-2 and R-L scores respectively This suggests that our proposed approach combining local context and hierarchical structure information can effectively enhance the performance of long-text extractive summarization models Analysis of comparative experimental results on ArXiv dataset Based on the experimental results presented in Table 5 it is evident that due to the longer length of text in the ArXiv dataset compared to the PubMed dataset employing the LEAD-10 method to extract the initial 10 sen - tences as summaries still yields unsatisfactory results The second section of the table compares the proposed model with generative summarization while the third section compares it with recent extractive summarization models Our model demonstrates excellent performance indicating that the ArXiv dataset exhibits a notice- able hierarchical structure The introduced hierarchical structure information extraction module in our model proves beneficial in aiding the models understanding of the source text thereby enhancing the quality of the generated summaries Given the significantly longer average length of documents in the ArXiv dataset compared to PubMed the PEGASUS models performance on this dataset is slightly lower than its performance on the PubMed dataset Analysis of comparative experimental results on CNNDM dataset According to Table 6 the unsupervised LEDA-3 performs better on the CNNDM dataset compared to the PubMed and ArXiv datasets This is attributed to the nature of CNNDM as a news-oriented short-text dataset where the first 3 sentences often encapsulate the majority of the textual meaning Moreover in terms of the Rouge-1 Rouge-2 and Rouge-L evaluation metrics our proposed model exhibits improvements of 417 333 and 462 respectively over the TextRank model on the CNNDM dataset This suggests that our model outper - forms TextRank by considering hierarchical structure information and paragraph-level topic information while TextRank focuses solely on the similarity between words and sentences Analyzing the second and third sections of the table our model competes favorably with PEGASUS and surpasses all other comparative models The introduced local topic information extraction module and hierarchical structure information in our model offer Table 4 Comparison of experimental results on PubMed dataset Significant values are in boldDataset PubMed Metrics ModelsR-1 R-2 R-L LEAD-7 3795 1333 3410 ORACLE 5815 3416 5169 Abstractive PEGASUS2020 4549 1990 4242 BigBird PEGASUA2020 4632 2065 4233 T52020 937 370 849 Extractive BERTSUMEXT2019 4109 1551 3685 Sent-CLF2020 4501 1991 4116 Sent-PTR2020 4330 1792 3947 Reformer-Ext2020 4232 1591 3826 Longformer-Ext2020 4375 1737 3971 ExtSum-LG RdLoss2021 4530 2042 4095 ExtSum-LG MMR2021 4539 2037 4099 Our Model 4649 2052 4206 10 Vol1234567890 Scientific Reports 2024 1410140 httpsdoiorg101038s41598-024-60779-z wwwnaturecomscientificreportsa more comprehensive representation of the topic ideas in news text complementing the Transformer encoder and thereby enhancing the quality of the generated summaries Ablation experiment In order to validate the effectiveness of incorporating local topic information and text hierarchical structure information in long text extractive summarization tasks we conducted ablation experiments Our baseline model shares the same structure as BERTSUM but uses Longformer-base-4096 as the encoder However this version can only handle the maximum length of 4096 tokens To overcome this limitation we replicated the Token Position EmbeddingsTPE of the original Longformer multiple times until reaching our desired length and subsequently trained additional TPEs with the entire summarization model Specifically Local Information L-Inf stands for the local topic information while article hierarchical embeddings AHE encompass the embedding of chapter titles and sentence-level structure within the article The results of the ablation experiments are shown in Table 7 Compared to the baseline model adding article hierarchical information improved the models scores by 484 436 and 457 for R-1 R-2 and R-L respectively This indicates that incorporating the hierarchical structure information of texts enables the model to better identify important sentences for long texts By adding local topic information to the baseline model we observed score improvements of 438 386 and 415 for R-1 R-2 and R-L respectively This suggests that in long text data different chapters represent different topics and incorporating local topic information allows the model to comprehend the content of the article more deeply resulting in high-quality summaries When simultaneously integrating article hierarchical structure information and local topic information into the baseline model the model leverages both as auxiliary information during summary generation This leads Table 5 Comparison of experimental results on ArXiv dataset Significant values are in boldDataset ArXiv Metrics models R-1 R-2 R-L LEAD-10 3737 1085 3317 ORACLE 5388 2305 4490 Abstractive Topic-GraphSum2021 4403 1852 3241 PEGASUS2020 4470 1727 2580 Extractive BERTSUMEXT2019 4124 1301 3610 Sent-CLF2020 3401 871 3041 Sent-PTR2020 4232 1563 3806 Reformer-Ext2020 4326 1468 3810 ExtSum-LG RdLoss2021 4401 1779 3909 ExtSum-LG MMR2021 4387 1750 3897 Our Model 4584 1903 4036 Table 6 Comparison of experimental results on CNNDM dataset Significant values are in boldDataset CNNDM Metrics models R-1 R-2 R-L LEAD-3 4024 1770 3645 ORACLE 5622 3374 5219 Abstractive Pointer-generator coverage2017 3953 1728 3638 BERTSUMABS2019 4172 1939 3876 PEGASUS2020 4417 2147 4111 Extractive TextRank2004 4020 1756 3644 BERTSUMEXT2019 4325 2024 3963 HIBERT-base2019 4231 1987 3878 HIBERT-large2019 4237 1995 3883 HSSAS2018 4230 1780 3760 Reformer-Ext2020 3885 1646 3516 Longformer-Ext2020 4300 2020 3930 Our Model 4437 2089 4106 11 Vol0123456789 Scientific Reports 2024 1410140 httpsdoiorg101038s41598-024-60779-z wwwnaturecomscientificreportsto an improvement in summary quality Comparing the results of Baseline AHE and Baseline L-Inf to Baseline All we observe an increase in all three metrics indicating that each proposed module is necessary and contributes to the overall enhancement of the models performance Sentence position analysis This study analyzed the location distribution of extract summary sentences in the source documents using dif- ferent models on the PubMed testset The results are shown in Fig 3 where the X axis represents the sentence number and the Y axis represents the occurrence proportion number of occurrencestotal number of occur - rences We examine the distribution of the top 30 sentences extracted by our proposed model blue the Oracle method green and the baseline model pink across all documents in the PubMed testset According to Fig 3 it can be found that the summary distribution generated by the Oracle model is uniform The baseline model lacks the perception ability of the overall document structure information and local topic information leading to a bias towards extracting the first 10 sentences while ignoring the subsequent ones In contrast our proposed model overcomes the limitation that the baseline model only pays attention to the initial sentences Additionally the distribution of summary sentences generated by our model is close to that of Oracle This indicates that by explicitly incorporating local topic information and article hierarchical structure information we proposed the model gains a deeper understanding of the content in PubMed documents and successful learns the internal structure at a more meaningful level effectively reducing its overreliance on the linear position of sentencesTable 7 Ablation experiment on the PubMed dataset Significant values are in boldDataset PubMed Metrics models R-1 R-2 R-L Baseline 4111 1564 3697 Baseline AHE 4595 2000 4154 Baseline L-Inf 4549 1950 4112 Baseline All Our Model 4649 2052 4206 Figure 3 Distribution of summary sentences 12 Vol1234567890 Scientific Reports 2024 1410140 httpsdoiorg101038s41598-024-60779-z wwwnaturecomscientificreportsConclusion This paper mainly focuses on the impact of multiple topic information and the inherent hierarchical structure of long texts on the quality of model-generated summaries To address the challenges posed by the abundance of topics and complex text hierarchy in generating summaries this paper proposes a long text extractive sum - marization model that combines local topic information and text hierarchical structure information Through comparative experiments conducted on the PubMed dataset the results demonstrates superior performance in long text extractive summarization tasks compared to other models Ablation experiments also confirm the necessity of each module proposed in this paper Moreover we recognize certain limitations in the models extraction of hierarchical information from text For instance when processing news short texts like those in CNNDaily Mail which lack clear hierarchical structures the models enhancement effects are not significant Therefore in our future work we plan to pay more attention to the topic information in the text We aim to compare it with real summaries construct comparative graph and guide the model to choose sentences for summarization that are similar to those in real summaries Data availability The PubMed dataset in this article is from open source links Researchers in this field have integrated them our PubMed dataset is available at https github com QianR uan histr uct relea ses tag data and models Received 10 October 2023 Accepted 26 April 2024 References 1 Rane N Govilkar S Recent trends in deep learning based abstractive text summarization Int J Recent Technol Eng 8 31083115 https doi org 10 35940 ijrte C4996 098319 2019 2 Allahyari M Pouriyeh S Assefi M et al Text summarization techniques a brief survey http arxiv org abs quant- ph 1707 02268 2017 3 Gambhir M Gupta V Recent automatic text summarization techniques a surveyJ Artif Intell Rev 471 166 2017 4 Nallapati R Zhai F Zhou B Summarunner A recurrent neural network based sequence model for extractive summarization of documents Proc of the AAAI conference on artificial intelligence 311 2017 5 Song K Wang B Feng Z et al Controlling the amount of verbatim copying in abstractive summarization Proc of the AAAI Conference on Artificial Intelligence 3405 89028909 2020 6 Sutskever I Vinyals O Le Q V Sequence to sequence learning with neural networks Adv Neural Inform Process Syst 27 2014 7 Cao Z et al Faithful to the original Fact aware neural abstractive summarization Proc AAAI Conference Artif Intell https doi org 10 1609 aaai v32i1 11912 2018 8 Erkan G Radev D R Lexrank Graph-based lexical centrality as salience in text summarization J Artif Intell Res 22 457479 2004 9 Wan X Y ang J Multi-document summarization using cluster-based link analysis Proc of the 31st annual international ACM SIGIR conference on Research and development in information retrieval 299306 2008 10 Zhou Q Y ang N Wei F et al Neural document summarization by jointly learning to score and select sentences http arxiv org abs quant- ph 1807 02305 2018 11 See A Liu P J Manning C D Get to the point Summarization with pointer-generator networks http arxiv org abs quant- ph 1704 04368 2017 12 Vaswani A Shazeer N Parmar N et al Attention is all you need Adv Neural Inform Process Syst 30 2017 13 Liu Y Lapata M Text summarization with pretrained encoders http arxiv org abs quant- ph 1908 08345 2019 14 Zhang X Wei F Zhou M HIBERT Document level pre-training of hierarchical bidirectional transformers for document sum - marization http arxiv org abs quant- ph 1905 06566 2019 15 Grootendorst M BERTopic Neural topic modeling with a class-based TF-IDF procedure http arxiv org abs quant- ph 2203 05794 2022 16 Liu Y Titov I Lapata M Single document summarization as tree induction In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics eds Liu Y et al Association for Computational Linguistics 2019 17 Cui P Hu L Liu Y Enhancing extractive text summarization with topic-aware graph neural networks http arxiv org abs quant- ph 2010 06253 2020 18 Wang W Chang B Graph-based dependency parsing with bidirectional LSTMCProceedings of the 54th Annual Meeting of the Association for Computational Linguistics Volume 1 Long Papers 23062315 2016 19 Narayan S Cohen S B Lapata M Dont give me the details just the summary topic-aware convolutional neural networks for extreme summarization http arxiv org abs quant- ph 1808 08745 2018 20 Li P et al Salience estimation via variational auto-encoders for multi-document summarization Proc AAAI Conference Artif Intell https doi org 10 1609 aaai v31i1 11007 2017 21 Zhang X Lapata M Wei F et al Neural latent extractive document summarization http arxiv org abs quant- ph 1808 07187 2018 22 Jia R Cao Y Fang F Zhou Y Fang Z Liu Y Wang S Deep Differential Amplifier for Extractive Summarization In Proc of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing Volume 1 Long Papers Online Association for Computational Linguistics 366 376 2021 23 Shi K et al StarSum A star architecture based model for extractive summarization IEEEACM Trans Audio Speech Language Process 30 30203031 2022 24 Ma T et al T-bertsum Topic-aware text summarization based on bert IEEE Trans Comput Soc Syst 93 879890 2021 25 Xie Q Huang J Saha T et al GRETEL Graph Contrastive Topic Enhanced Language Model for Long Document Extractive SummarizationJ arXiv preprint arXiv 2208 09982 2022 26 Beltagy I Peters M E Cohan A Longformer The long-document transformerJ arXiv preprint arXiv 2004 05150 2020 27 Wenhui W Chang B Graph-based Dependency Parsing with Bidirectional LSTM In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics Volume 1 Long Papers pages 23062315 Berlin Germany Association for Computational Linguistics 2016 28 James Cross and Liang Huang Span-Based Constituency Parsing with a Structure-Label System and Provably Optimal Dynamic Oracles In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing pages 111 Austin Texas Association for Computational Linguistics 2016 13 Vol0123456789 Scientific Reports 2024 1410140 httpsdoiorg101038s41598-024-60779-z wwwnaturecomscientificreports 29 Ruan Q Ostendorff M Rehm G Histruct Improving extractive text summarization with hierarchical structure information http arxiv org abs quant- ph 2203 09629 2022 30 Jacob Devlin Ming-Wei Chang Kenton Lee and Kristina Toutanova 2019 BERT Pre-training of Deep Bidirectional Transform- ers for Language Understanding In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies Volume 1 Long and Short Papers pages 41714186 Minneapolis Minnesota Association for Computational Linguistics 31 Cohan A Dernoncourt F Kim DS Bui T Kim S Chang W Goharian N A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies Volume 2 Short Papers pages 615621 New Orleans Louisiana Association for Computational Linguistics 2018 32 Lin C Y Rouge A package for automatic evaluation of summariesCText summarization branches out 7481 2004 33 Jonathan Pilault Raymond Li Sandeep Subramanian and Chris Pal 2020 On Extractive and Abstractive Neural Document Summarization with Transformer Language Models In Proc of the 2020 Conference on Empirical Methods in Natural Language Processing EMNLP pages 93089319 Online Association for Computational Linguistics 2020 34 Kitaev N Kaiser Levskaya A Reformer The efficient transformer http arxiv org abs quant- ph 2001 04451 2020 35 Xiao W Carenini G Systematically exploring redundancy reduction in summarizing long documents http arxiv org abs quant- ph 2012 00052 2020 36 Zhang J Zhao Y Saleh M et al Pegasus Pre-training with extracted gap-sentences for abstractive summarizationCInternational Conference on Machine Learning PMLR 1132811339 2020 37 Raffel C et al Exploring the limits of transfer learning with a unified text-to-text transformer J Mach Learn Res 211 54855551 2020 38 Mihalcea R Tarau P Textrank Bringing order into textCProc of the 2004 conference on empirical methods in natural language processing 404411 2004 39 Cui P Hu L Topic-guided abstractive multi-document summarization http arxiv org abs quant- ph 2110 11207 2021 40 Al-Sabahi K Zuping Z Nadher M A hierarchical structured self-attentive model for extractive document summarization HSSAS IEEE Access 6 2420524212 2018 Acknowledgements Thanks for the key research and development projects of Sichuan Science and Technology Department project 2021GFW130 2022YFG0375 2023YFG0099 2023YFG0261 and Natural Science Foundation of Sichuan Prov- ince of China 2023NSFSC0482 Author contributions Ting Wang experimental design PubMed data analysis results interpretation manuscript drafting manuscript revision Chuan Y ang PubMed data analysis manuscript drafting results interpretation grammatical error correction Jiaying Liang Dong Xiang Wenjie Y ang Hongyang Wang and Jia Li data acquisition and PubMed data analysis Maoyang Zou conception and design of the study result interpretation manuscript revision All authors read and approved the final manuscript Competing interests The authors declare no competing interests Additional information Correspondence and requests for materials should be addressed to MZ Reprints and permissions information is available at wwwnaturecomreprints Publishers note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations Open Access This article is licensed under a Creative Commons Attribution 40 International License which permits use sharing adaptation distribution and reproduction in any medium or format as long as you give appropriate credit to the original authors and the source provide a link to the Creative Commons licence and indicate if changes were made The images or other third party material in this article are included in the articles Creative Commons licence unless indicated otherwise in a credit line to the material If material is not included in the articles Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use you will need to obtain permission directly from the copyright holder To view a copy of this licence visit http creat iveco mmons org licen ses by4 0 The Authors 2024