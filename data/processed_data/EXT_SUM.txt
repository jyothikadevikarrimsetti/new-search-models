wwwnaturecomscientificreports scientific reports Q Check for updates OPEN A study of extractive summarization of long documents incorporating local topic and hierarchical information Ting Wangl ChuanYangl Maoyang Zouv Jiaying Liangl Dong Xiangl WenjieYangl Hongyang Wang1 Jia Li1 In recent years the transformer-based language models have achieved remarkable success in the eld of extractive text summarization However there are still some limitations in this kind of research First the transformer language model usually regards the text as a linear sequence ignoring the inherent hierarchical structure information ofthe text Second for long text data traditional extractive models often focus on global topic information which poses challenges in how they capturing and integrating local contextual information within topic segments To address these issues we propose a long text extractive summarization model that employs a local topic information extraction module and a text hierarchical extraction module to capture the local topic information and documents hierarchical structure information of the original text Our approach enhances the ability to determine whether a sentence belongs to the summary In this experiment ROUGE score is used as the experimental evaluation index and evaluates the model on three large public datasets Through experimental validation the model demonstrates superior performance in terms of ROUGE- 1 ROUGE-2 and ROUGE-L scores compared to current mainstream summarization models afrming the effectiveness of incorporating local topic information and document hierarchical structure into the model Text summarization is an arduous task in the field of natural language processing NLP wherein the goal is to generate a concise and logically connected summary ofa given document This process involves extracting crucial information and reduce the length of the document while preserving the essential meaning Text sum marization can effectively reduce the information burden of users enable users to quickly obtain information from redundant information greatly reduce manpower and material resources It plays an important role in various domains including information retrieval title generation and other related elds Based on the methodology employed text summarization tasks can be categorized into two types extrac- tive summarization4 and abstractive summarization lhe abstractive summarization method utilizes neural networkebased approaches such as the Sequence727equence SquSeq architecture also known as encoder decoder architecture The principle of an encoderidecoder is similar to the way human think or write summaries Tie encoder rst encodes the full text and then the decoder generates new sentences word by word to form a document summary This method generates less redundant summary information but might face challenges in maintaining uency and grammatical correctness In addition the generation of new words or phrases may produce summaries that are inconsistent with the original statement7 These issues can be mitigated by directly selecting sentences from the source text and assembling them into summaries ie the extractive summarization T e extractive method treats summarization as a classication problem where important sentences are directly selected from the source text to construct a summary Summaries generated through this approach often exhibit a good performance in uency and grammar For the extractive summarization task the core challenge lies in learning comprehensive sentence context information and modeling inter-sentence relationships through the encoder thereby enabling sentence classiers to extract more valuable sentences Traditional extractive methods usually employ graph-based methods or clustering-based methods for unsupervised summarization These approaches construct the correlation between sentences using cosine similarity and then use sorting methods to 1School ofComputer Science Chengdu Universtty of InformationTechnology Chengdu 610225 Sichuan Province China 2College of Blockcharn Industry Chengdu Universtty of Information Technology Chengdu 610225 Sichuan Province China email zoumycuiteducn Scientic Reports 202131430140 httpszldotorg101038541598024-60779-2 natureportfolio wwwnaturecomscientificreports calculate the importance ofsentences With the rapid development of deep learning many extractive summarie zation methods use Recurrent Neural Network RNN to capture the relationship between sentencesm How- ever RNN-based methods are difcult to deal with long-distance dependencies especially for long document summaries In recent years transformerl2 language model which has been preetrained by largeescalc corpus has achieved excellent results when fineetuned for downstream tasks and have found widespread application in the eld oftext summarization Liu et al 3 proposed the BERTSUM model by improving the BERT embedding layer They applied the BERT model for the rst time in the text summarization and achieved stateeofetheeart SOTA performance on CNNDailyMail dataset Zhang et al M designed a hierarchical transformer to capture long-range inter-sentence relationships However this method did not yield signicant performance gains for summarization tasks and faced challenges such as slow training speed and potential overtting At the same time some researchers introduced the neural topic model NTM15 and graph neural network GNN into the task of text summarization to capture global semantic information and further guide the generation of abstracts Cui et al 7 use NTM to capture the theme features of documents and GNN to represent documents as graph structures thus obtaining the relationship between sentences However for long document summarization tasks the above methods have two shortcomings The rst one is that they fail to recognize the explicit hierarchical structures and section headings inherent within the long document When manually summarizing text we tend to focus on the main sections For example in the context ofscientic papers more attention may be given to sections like Methodology Experimental and Conclue sion but Background or Related Work may not receive as much emphasis In addition sentences within a section have stronger relationships compared to those outside the section Understanding the logical relation ship between sentences and the hierarchical structure within the document helps the model better identify the important sentences However the traditional transformerebased text summarization methods often regard the text as a sequential structure and struggle with longer documents The second shortcoming is that the longer the document the more topics it may discuss because each section presents different topic information In summary the aforementioned methods focus on the overall topic information of the entire document that is the global information neglecting the local topic information of individual sections In order to address these issues this paper proposes a longrdocument extractive summarization model that integrates local topic information and document hierarchy information into current topic segment The main contributions ofthis paper can be summarized as follows I Introduction of an innovative long-document extractive summarization model This model consists ofa text encoder a module for extracting local topic information and a module for embedding hierarchical structure information of the document The information is integrated into the sentence representation of the document enhancing the quality of the generated summaries 2 This paper utilizes LSTMeMinus18 to obtain distributed representations oflocal information and combines it with text summarization tasks Instead of employing a xed threersegment approach for text paragraph ing the paper adopts a dynamic method based on the number of sentences to determine paragraph length thereby calculating the starting and ending positions of each paragraph in the text Paragraph segments are divided based on these poSitions and their topic information is computed 3 Experimental results conducted on the PubMed dataset reveal excellent performance of the proposed method when compared to several baseline models Related Work Extractive summarization method With the rapid development of neural networks signicant achievements have been made in extractive sum- marization tasks At present the extractive methods are mainly regarded as sentence sorting task or binary sequence labeling tasks In the sentence sorting paradigm models are required to assign scores to each sentence in the text and place higher-scored sentences at the front of the summary list while lower-scored sentences are placed towards the back This process yields an ordered list of sentences and the top few sentences are selected as the summary Narayan et al 19 proposed a topicraware convolutional neural network model This model rst extracts features from the documents using convolutional neural networks and then weights the features accord ing to the topic Finally a selectionebased sorting method is employed to choose the most relevant sentences as the summary Experiments results on multiple datasets show that this approach can generate concise summaries that still preserve valuable information Li ct al20 proposed a method for evaluating sentence importance in multiedocument summarization using variational autoencoder Different from the traditional method based on feature engineering this method directly learns the abstract semantic representation directly from the original data KL divergence is introduced to constrain the generated sentence representations to be close to the prior distribution thereby improving the generalization ability of the model Regarding the second paradigm which considers extractive text summarization as a sequence labeling task this approach involves extracting and encoding features for each sentence or paragraph The encoded features are then input into a decoder for labeling prediction to determine which sentences should be selected for the summary The sequence labeling method has been widely applied in extractive text summarization and has achieved good results Nalapati et al4 proposed the SummaRuNNer model for text summarization which is a sequence model based on RNN This model generates document summarization by learning the importance of each sentence within the document It has demonstrated good summarization performance on multiple text datasets Zhang et al21 introduced a latent variable extractive model which treats sentences as latent variables and infers summaries using sentences with activated variables Scientic Reports 2024141014o httpsdoiorg101038541598-024-60779-2 natureportfolio wwwnaturecomscientificreports However most of the methods mentioned above rely on RNN for extractive summarization RNNrbased methods face challenges in handling long-distance dependencies at the sentence level and may omit on language or structural information due to the input format of the original document In order to address these issues researchers have started utilizing transformerebased preetraining language model as encoders and representing documents through more intuitive graph structures They have also incorporated NTM to extract topic features from the documents further guiding the models to produce highequality summaries Iia et al22 proposed a method called deep differential amplier for extractive summarization which enhances the features of sume mary sentences by contrast to nonesummary sentences using differential ampliers Shi et al23 proposed a star architecture-based extractive summarization method where sentences in documents are modeled as satellite nodes and a virtual central node is introduced to learn the inter-sentence relationships using the star structure This approach achieved promising results on three public datasets Ma et a1 embedded the topic features extracted by NTM into BERT to generate a vector representation with topic features thus improving the quality of summaries Although the aforementioned methods have succeeded in modeling interesentence relationship and extract ing global semantics there is still a problem with extractive text summarization methods based on transformer pre-training language models The length of the input document in text summarization is longer compared to general natural language processing task and using just a transformerebased encoder is insufcient for effectively handling long texts and often leads to high computational costs To better understand the original document researchers have proposed various improvements Xie et a125 rst preprocessed the documents by dividing them into blocks with the same size encoded each block with block encoding They merged the block encoding results With NTM to generate global topic features Finally they established a comparison graph between topic features and sentence features to filter summary sentences This method has achieved good results in both long docu- ments and short news documents with particular advantages in handling the former Beltagy et a126 introduced the Longformer model specically deSigned for processing long documents By replacing the self-attention mechanism of the transformer with a sliding window self-attention mechanism the time complexity is reduced to linear level enabling the model to handle long documents easily Although the Longformer performs well in handling long documents it fails to model local semantic information and document hierarchy structure which aifects its performance Therefore this paper uses the Longformer as the encoder and incorporates local contextual information of the current topic segment and hierarchical structure information of the document This allow our model to prioritize local topic information and overall structural information when dealing with long scientic papers LSTM-Minus Wang et al27 proposed the LSTMeMinus method for the rst time and applied it to dependency parsing and achieved good results The LSTMrMinus method is a novel approach for learning embedding of text segments utilizing subtraction between LSTM hidden vectors to learn the distributed representation of sentence segments Initially a sentence is divided into three segments prex inx and suffix and the segment from the word w to the word Wj is represented by the hidden vector h h This allows the model to effectively learn segment embedding from both external and internal information thus enhancing its ability to obtain sentence-level information In the same year Cross et al2X extended the unidirectional LSTMeMinus to the bidirectional using it as sentence span representation and achieving impressive performance in component syntactic analysis tasks Build upon this idea we applied this method to the eld of text summarization to extract the contextual information from local topic segments Method To address the limitations of the existing extractive text summarization methods this paper proposes a long document extractive summarization model that integrates local contextual information and documentelevel hierarchical information from the current topic segment The model is inspired by the long document extrac- tive model proposed by Ruan et al which incorporates hierarchical structure information The nal model of this paper is obtained by incorporating local topic information Experiments results show that the inclusion of local topic information further deepens the models understanding oflong texts The task oflong text extractive summarization is dened as follow Given an original document D senth sentn D contains n sentences where each sentence denoted as the sen t represents the irth sentence of the original document The purpose of the extractive text summarization model is to select in sentences capturing the central idea of the original text as summaries where m is the desired number of summary sentences m V1 This task is typically treated as a sentence classication problem For each sentence sent there is a corresponding label Vr E 0 1 where a label ofI means that the sentence belongs to the summary while 0 indicates that it does not The proposed model as shown in Fig 1 comprises three main modules a preetrained language model based encoder a local topic information extraction module referred to as the Topic Segment Representation module in the Fig 1 and a text hierarchical information embedding module Because this work deals with long text core pus the encoder used is based on the Longformer an improvement over the transformer preetraining language model which allows for better encoding of long documents Once the contextual representation of the document is obtained through the encoder it is passed to the local topic information extraction module which extracts the topic information of the sentence segment it belongs to The specic structure ofthis module is shown in Fig 2 Then the local topic information representation is fused with the text contextual representation resulting in a fusion of the local topic information and the textual context The text hierarchical structure information embedding module embeds the hierarchical structure information of the text into the fused representation of the local topic information and textual context By using a two-layer stacked transformer this module learns Scientic Reports 202131410140 httpsdoiorg101038541598-024-60779-2 natureportfolio wwwnaturecomlscientificreports Binary classication Hierarchical information - embedding Encoder sigmoid layers Hierarchical Contextual Sentence Embeddings Two Transformer layers - I - l I Section Title Embeddings 1142 STE3 - -- Sentence Hierarchical Position Embeddings iSHEZ i SHEI i Sentence Representations I - t semmemsmon Contextual Embeddings - - I - u - I - - - - - I i t r t t t PositionEmbeddings as E ME tErl ms 55 5 ma rag mm mm 512 7 7 7 771-7 4- 7 77 Segment Embeddings 3L EA EA L - - - - EA Ll EA E 7 Token Embeddings 5305 Erent lEone lElFDSli Eugen Esem l Etwo ElEOS Elms Brent l Educ Ems Inpucvocument I-III-II-I- Figure 1 Overall structure diagram of the model fs - 2 b - be f7 f5hsl H i l r sectionz section3 I I I l l I 1 l b1 u b Is b6 In i T l l i T l sent1 sent2 sent sent4 sents sents sent7 I Document Re - resentation Figure 2 Local topic information extraction module Scientic Reports 202131410140 httpsldoiorg1010381541598-024-60779-1 natureportfolio wwwnaturecomscientificreports the hierarchical structure information at both the sentence and document levels enabling the model to gain a deeper understanding of the text context Finally the condence score of each sentence is calculated through Sigmoid layer for each sentence to determine whether it should be included in the summary Text hierarchical information Sentence hierarchical information Due to scientic papers consisting of multiple sections with each section containing several paragraphs that describe different topics this paper uses paragraphs as the unit for hierarchical division of the article The sen tenceelevel hierarchical structure information includes the linear position of the paragraph to which the sentence belongs and the linear position representation of the sentence within the paragraph The positions of paragraphs and sentences are represented by numerical serial numbers corresponding to them For an original document D senth sentn the hierarchical structure information of the i-th sentence sent is expressed as a two- dimensional vector 5 5X which indicates the position of the sentence at this level as shown in Formula 1 vsem sygs 1 where 5 represents the linear position of the paragraph containing the sentence relative to the entire article and g5 represents the linear position of the sentence within its respective paragraph All sentences within the same paragraph share the same value in the rst dimension ofthe vsent vector indicating a higher correlation among sentences within the same paragraph And the g vector further indicates the linear relationship among sentences within the paragraph Section title information Compared with short news articles scientic papers often have section titles The content within each section is usually highly relevant to the corresponding section title as the section title serves as a concise summary of the content of the section In this study when encoding sentences the section titles are incorporated as additional hierarchical information into the sentence encoding However for seientic papers there are many similar sec- tion titles with the same meaning For instance Method and Methodology have similar meanings and can be grouped together under the Method category Therefore for the PubMed dataset used in this paper eight section title categories are dened including introduction background case Method result discus- sion conclusion and additional information If the section title of a section does not fall into any of the eight predened categories the original section title itself is directly used Encoder Document encoding The purpose of document encoding is to encode the sentences of the input document into a vector representation with a xed length Previous methods for extractive text summarization tasks often employed RNN and BERT30 as encoders BERT is a bidirectional transformer encoder that is preetrained on largeescale corpus and has achieved excellent performance on various natural language processing tasks However for long text data BERT cannot process the entire document which will lead to information loss Therefore in this paper we uses the Longformer pre-training language model as the text encoder Longformer improves the self-attention mechanism of the traditional transformer into the sliding window sclfeattention which makes it easy to handle documents with thousands of characters In the traditional transformer selfeattention mechanism the calculation is performed by linearly transforming the input word embedding matrix to generate a Query matrix Query Q a Key matrix Key K and a Value matrix Value V ofdimension d The specic calculation process is shown in Formula 2 QKT Attention softmax J7 V 2 where Q K V E Rlxi and d represents the dimension ofa word vector matrix while dk represents the dimene sion of the K matrix Hence the spatial complexity of the traditional transformer self-attention mechanism is 0 L2 the spatial complexity of Longformers sliding windows self-attention mechanism is 0L scaling linearly with the input sequence length L As a result Longformer has more advantages in encoding long texts As shown in Fig 1 in order to obtain the representation of each sentence we inserts B03 beginning of sentence and E05 end of sentence tags at the beginning and end of each sentence respectively The model embedding layer includes Token Embeddings TE Segment Embeddings SE and Position Embeddings PE These features are sum to obtained the embedded representation of each word Subsequently the context of the input sequence is learned by using the pre-trained Longformer The entire procedure is illustrated in Eqs 3 and 4 WW TESEPE 3 hm hm - - limo - - him LongformerWio Wu - - - Wmo - - Wm 4 where whj represents the jeth word ofthe ieth sentence which is obtained by Formula 311m and w represent the B05 and EOS tags ofthe ieth sentence respectively and hm represents the hidden state ofthe corresponding word After Longformer encoding we use the B05 tag as the context representation of each sentence that isH h1ghNo Scientic Reports 2024141014o httpsdoiorg101038541598-024-60779-2 natureportfolio